
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Concept &#8212; Machine Learning Chronicles</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=796348d33e8b1d947c94" rel="stylesheet">
<link href="../../../_static/styles/bootstrap.css?digest=796348d33e8b1d947c94" rel="stylesheet">
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=796348d33e8b1d947c94" rel="stylesheet">

  
  <link href="../../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=796348d33e8b1d947c94" rel="stylesheet">
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2">
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2">
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" href="../../../_static/styles/sphinx-book-theme.css?digest=4ec06e9971c5264fbd345897d5258098f11cc577" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=796348d33e8b1d947c94">
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=796348d33e8b1d947c94">

    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?digest=8bf782fb4ee92b3d3646425e50f299c4e1fd152d"></script>
    <script src="../../../_static/tabs.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"tex": {"macros": {"defeq": "\\overset{\\text{def}}{=}", "defa": "\\overset{\\text{(a)}}{=}", "defb": "\\overset{\\text{(b)}}{=}", "defc": "\\overset{\\text{(c)}}{=}", "defd": "\\overset{\\text{(d)}}{=}", "st": "\\mid", "mod": "\\mid", "S": "\\Omega", "s": "\\omega", "e": "\\exp", "P": "\\mathbb{P}", "R": "\\mathbb{R}", "expectation": "\\mathbb{E}", "v": "\\mathbf{v}", "a": "\\mathbf{a}", "b": "\\mathbf{b}", "c": "\\mathbf{c}", "u": "\\mathbf{u}", "w": "\\mathbf{w}", "x": "\\mathbf{x}", "y": "\\mathbf{y}", "z": "\\mathbf{z}", "0": "\\mathbf{0}", "1": "\\mathbf{1}", "A": "\\mathbf{A}", "B": "\\mathbf{B}", "C": "\\mathbf{C}", "E": "\\mathcal{F}", "eventA": "\\mathcal{A}", "lset": "\\left\\{", "rset": "\\right\\}", "lsq": "\\left[", "rsq": "\\right]", "lpar": "\\left(", "rpar": "\\right)", "lcurl": "\\left\\{", "rcurl": "\\right\\}", "pmf": "p_X", "pdf": "f_X", "pdftwo": "f_{X,Y}", "pdfjoint": "f_{\\mathbf{X}}", "pmfjointxy": "p_{X, Y}", "pdfjointxy": "f_{X, Y}", "cdf": "F_X", "pspace": "(\\Omega, \\mathcal{F}, \\mathbb{P})", "var": "\\operatorname{Var}", "std": "\\operatorname{Std}", "bern": "\\operatorname{Bernoulli}", "binomial": "\\operatorname{Binomial}", "geometric": "\\operatorname{Geometric}", "poisson": "\\operatorname{Poisson}", "uniform": "\\operatorname{Uniform}", "normal": "\\operatorname{Normal}", "gaussian": "\\operatorname{Gaussian}", "gaussiansymbol": "\\mathcal{N}", "exponential": "\\operatorname{Exponential}", "iid": "\\textbf{i.i.d.}", "and": "\\text{and}"}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'machine_learning/linear_models/logistic_regression/concept';</script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Implementation" href="implementation.html" />
    <link rel="prev" title="Logistic Regression" href="intro.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="docsearch:language" content="en">
  </head>
  
  
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="180" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>

  
  <input type="checkbox" class="sidebar-toggle" name="__primary" id="__primary">
  <label class="overlay overlay-primary" for="__primary"></label>

  
  <input type="checkbox" class="sidebar-toggle" name="__secondary" id="__secondary">
  <label class="overlay overlay-secondary" for="__secondary"></label>

  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
      
<form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false">
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
    </div>
  </div>

  
  <nav class="bd-header navbar navbar-expand-lg bd-navbar" id="navbar-main"><div class="bd-header__inner bd-page-width">
  <label class="sidebar-toggle primary-toggle" for="__primary">
      <span class="fa-solid fa-bars"></span>
  </label>
  <div id="navbar-start">
    
    
  


<a class="navbar-brand logo" href="../../../intro.html">

  
  
  
  
  
  
  

  
    <img src="../../../_static/logo.png" class="logo__image only-light" alt="Logo image">
    <img src="../../../_static/logo.png" class="logo__image only-dark" alt="Logo image">
  
  
</a>
    
  </div>

  
  <div class="col-lg-9 navbar-header-items">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <nav class="navbar-nav">
    <p class="sidebar-header-items__title" role="heading" aria-level="1" aria-label="Site Navigation">
        Site Navigation
    </p>
    <ul id="navbar-main-elements" class="navbar-nav">
        
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../notations/mathematical_notations.html">
                        Mathematical Notations
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../notations/machine_learning_notations.html">
                        Machine Learning Notations
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../notations/deep_learning_notations.html">
                        Deep Learning Notations
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../probability_theory/01_mathematical_preliminaries/intro.html">
                        Chapter 1. Mathematical Preliminaries
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../probability_theory/02_probability/intro.html">
                        Chapter 2. Probability
                      </a>
                    </li>
                
            <div class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                    More
                </button>
                <div class="dropdown-menu">
                    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../probability_theory/03_discrete_random_variables/intro.html">
                        Chapter 3. Discrete Random Variables
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../probability_theory/04_continuous_random_variables/intro.html">
                        Chapter 4. Continuous Random Variables
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../probability_theory/05_joint_distributions/intro.html">
                        Chapter 5. Joint Distributions
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../probability_theory/06_sample_statistics/intro.html">
                        Chapter 6. Sample Statistics
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../optimization/gradient_descent/intro.html">
                        Gradient Descent
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../fundamentals/intro.html">
                        Fundamentals
                      </a>
                    </li>
                

                    <li class="nav-item current active">
                      <a class="nav-link nav-internal" href="../intro.html">
                        Linear Models
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../generative/intro.html">
                        Generative
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../model_selection_and_evaluation/intro.html">
                        Model Selection and Evaluation
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../trees/intro.html">
                        Trees, Forests, Bagging and Boosting
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../decomposition/intro.html">
                        Dimensionality Reduction
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../neighbours/intro.html">
                        Neighbourhood
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../mixtures/intro.html">
                        Mixture Models
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../clustering/intro.html">
                        Clustering
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../deep_learning/natural_language_processing/intro.html">
                        Natural Language Processing (NLP)
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../references_resources_roadmap/bibliography.html">
                        Bibliography
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../references_resources_roadmap/resources.html">
                        Resources
                      </a>
                    </li>
                
                </div>
            </div>
            
    </ul>
</nav>
      </div>
      
    </div>

    <div id="navbar-end">
      
        <div class="navbar-end-item navbar-persistent--container">
          
<button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-toggle="tooltip">
  <i class="fa-solid fa-magnifying-glass"></i>
</button>
        </div>
      
      
      <div class="navbar-end-item">
        <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
</button>
      </div>
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
      </ul>
      </div>
      
    </div>
  </div>


  
  
    <div class="navbar-persistent--mobile">
<button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-toggle="tooltip">
  <i class="fa-solid fa-magnifying-glass"></i>
</button>
    </div>
  

  
  <label class="sidebar-toggle secondary-toggle" for="__secondary">
      <span class="fa-solid fa-outdent"></span>
  </label>
  

</div>
  </nav>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        
  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
      
      <div class="navbar-center-item">
        <nav class="navbar-nav">
    <p class="sidebar-header-items__title" role="heading" aria-level="1" aria-label="Site Navigation">
        Site Navigation
    </p>
    <ul id="navbar-main-elements" class="navbar-nav">
        
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../notations/mathematical_notations.html">
                        Mathematical Notations
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../notations/machine_learning_notations.html">
                        Machine Learning Notations
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../notations/deep_learning_notations.html">
                        Deep Learning Notations
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../probability_theory/01_mathematical_preliminaries/intro.html">
                        Chapter 1. Mathematical Preliminaries
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../probability_theory/02_probability/intro.html">
                        Chapter 2. Probability
                      </a>
                    </li>
                
            <div class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                    More
                </button>
                <div class="dropdown-menu">
                    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../probability_theory/03_discrete_random_variables/intro.html">
                        Chapter 3. Discrete Random Variables
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../probability_theory/04_continuous_random_variables/intro.html">
                        Chapter 4. Continuous Random Variables
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../probability_theory/05_joint_distributions/intro.html">
                        Chapter 5. Joint Distributions
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../probability_theory/06_sample_statistics/intro.html">
                        Chapter 6. Sample Statistics
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../optimization/gradient_descent/intro.html">
                        Gradient Descent
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../fundamentals/intro.html">
                        Fundamentals
                      </a>
                    </li>
                

                    <li class="nav-item current active">
                      <a class="nav-link nav-internal" href="../intro.html">
                        Linear Models
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../generative/intro.html">
                        Generative
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../model_selection_and_evaluation/intro.html">
                        Model Selection and Evaluation
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../trees/intro.html">
                        Trees, Forests, Bagging and Boosting
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../decomposition/intro.html">
                        Dimensionality Reduction
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../neighbours/intro.html">
                        Neighbourhood
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../mixtures/intro.html">
                        Mixture Models
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../clustering/intro.html">
                        Clustering
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../deep_learning/natural_language_processing/intro.html">
                        Natural Language Processing (NLP)
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../references_resources_roadmap/bibliography.html">
                        Bibliography
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../references_resources_roadmap/resources.html">
                        Resources
                      </a>
                    </li>
                
                </div>
            </div>
            
    </ul>
</nav>
      </div>
      
      </div>
    

    
    
    <div class="sidebar-header-items__end">
      
      <div class="navbar-end-item">
        <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
</button>
      </div>
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
      </ul>
      </div>
      
    </div>
    
  </div>

  
  <div class="sidebar-start-items sidebar-primary__section">
    <div class="sidebar-start-items__item">
  


<a class="navbar-brand logo" href="../../../intro.html">

  
  
  
  
  
  
  

  
    <img src="../../../_static/logo.png" class="logo__image only-light" alt="Logo image">
    <img src="../../../_static/logo.png" class="logo__image only-dark" alt="Logo image">
  
  
</a>
    </div>
    <div class="sidebar-start-items__item">
<form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false">
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
    </div>
    <div class="sidebar-start-items__item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../../intro.html">
                    Introduction
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Notations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../notations/mathematical_notations.html">Mathematical Notations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notations/machine_learning_notations.html">Machine Learning Notations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notations/deep_learning_notations.html">Deep Learning Notations</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Probability Theory</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../probability_theory/01_mathematical_preliminaries/intro.html">Chapter 1. Mathematical Preliminaries</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/01_mathematical_preliminaries/01_combinatorics.html">Permutations and Combinations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/01_mathematical_preliminaries/02_calculus.html">Calculus</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/01_mathematical_preliminaries/contours.html">Contour Maps</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/01_mathematical_preliminaries/exercises.html">Exercises</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../probability_theory/02_probability/intro.html">Chapter 2. Probability</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/02_probability/0202_probability_space.html">Probability Space</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/02_probability/0203_probability_axioms.html">Probability Axioms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/02_probability/0204_conditional_probability.html">Conditional Probability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/02_probability/0205_independence.html">Independence</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/02_probability/0206_bayes_theorem.html">Bayeâ€™s Theorem and the Law of Total Probability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/02_probability/summary.html">Summary</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../probability_theory/03_discrete_random_variables/intro.html">Chapter 3. Discrete Random Variables</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/03_discrete_random_variables/0301_random_variables.html">Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/03_discrete_random_variables/0302_discrete_random_variables.html">Discrete Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/03_discrete_random_variables/0303_probability_mass_function.html">Probability Mass Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/03_discrete_random_variables/0304_cumulative_distribution_function.html">Cumulative Distribution Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/03_discrete_random_variables/0305_expectation.html">Expectation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/03_discrete_random_variables/0306_moments_and_variance.html">Moments and Variance</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../probability_theory/03_discrete_random_variables/uniform/intro.html">Discrete Uniform Distribution</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/03_discrete_random_variables/uniform/0307_discrete_uniform_distribution_concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/03_discrete_random_variables/uniform/0307_discrete_uniform_distribution_application.html">Application</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../probability_theory/03_discrete_random_variables/bernoulli/intro.html">Bernoulli Distribution</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/03_discrete_random_variables/bernoulli/0308_bernoulli_distribution_concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/03_discrete_random_variables/bernoulli/0308_bernoulli_distribution_application.html">Application</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/03_discrete_random_variables/iid.html">Independent and Identically Distributed (IID)</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../probability_theory/03_discrete_random_variables/binomial/intro.html">Binomial Distribution</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/03_discrete_random_variables/binomial/0309_binomial_distribution_concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/03_discrete_random_variables/binomial/0309_binomial_distribution_implementation.html">Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/03_discrete_random_variables/binomial/0309_binomial_distribution_application.html">Real World Examples</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../probability_theory/03_discrete_random_variables/geometric/intro.html">Geometric Distribution</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/03_discrete_random_variables/geometric/0310_geometric_distribution_concept.html">Concept</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../probability_theory/03_discrete_random_variables/poisson/intro.html">Poisson Distribution</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/03_discrete_random_variables/poisson/0311_poisson_distribution_concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/03_discrete_random_variables/poisson/0311_poisson_distribution_implementation.html">Implementation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/03_discrete_random_variables/summary.html">Important</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/03_discrete_random_variables/exercises.html">Exercises</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../probability_theory/04_continuous_random_variables/intro.html">Chapter 4. Continuous Random Variables</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/04_continuous_random_variables/from_discrete_to_continuous.html">From Discrete to Continuous</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/04_continuous_random_variables/0401_continuous_random_variables.html">Continuous Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/04_continuous_random_variables/0402_probability_density_function.html">Probability Density Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/04_continuous_random_variables/0403_expectation.html">Expectation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/04_continuous_random_variables/0404_moments_and_variance.html">Moments and Variance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/04_continuous_random_variables/0405_cumulative_distribution_function.html">Cumulative Distribution Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/04_continuous_random_variables/0406_mean_median_mode.html">Mean, Median and Mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/04_continuous_random_variables/0407_continuous_uniform_distribution.html">Continuous Uniform Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/04_continuous_random_variables/0408_exponential_distribution.html">Exponential Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/04_continuous_random_variables/0409_gaussian_distribution.html">Gaussian Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/04_continuous_random_variables/0410_skewness_and_kurtosis.html">Skewness and Kurtosis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/04_continuous_random_variables/0411_convolve_and_sum_of_random_variables.html">Convolution and Sum of Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/04_continuous_random_variables/0412_functions_of_random_variables.html">Functions of Random Variables</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../probability_theory/05_joint_distributions/intro.html">Chapter 5. Joint Distributions</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/05_joint_distributions/from_single_variable_to_joint_distributions.html">From Single Variable to Joint Distributions</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../probability_theory/05_joint_distributions/0501_joint_pmf_pdf/intro.html">Joint PMF and PDF</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/05_joint_distributions/0501_joint_pmf_pdf/concept.html">Concept</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../probability_theory/05_joint_distributions/0502_joint_expectation_and_correlation/intro.html">Joint Expectation and Correlation</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/05_joint_distributions/0502_joint_expectation_and_correlation/concept.html">Concept</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../probability_theory/05_joint_distributions/0503_conditional_pmf_pdf/intro.html">Conditional PMF and PDF</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/05_joint_distributions/0503_conditional_pmf_pdf/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/05_joint_distributions/0503_conditional_pmf_pdf/application.html">Application</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../probability_theory/05_joint_distributions/0504_conditional_expectation_variance/intro.html">Conditional Expectation and Variance</a><input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-14"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/05_joint_distributions/0504_conditional_expectation_variance/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/05_joint_distributions/0504_conditional_expectation_variance/exercises.html">Exercises</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../probability_theory/05_joint_distributions/0505_sum_of_random_variables/intro.html">Sum of Random Variables</a><input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-15"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/05_joint_distributions/0505_sum_of_random_variables/concept.html">Concept</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../probability_theory/05_joint_distributions/0506_random_vectors/intro.html">Random Vectors</a><input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-16"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/05_joint_distributions/0506_random_vectors/concept.html">Concept</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../probability_theory/05_joint_distributions/0507_multivariate_gaussian/intro.html">Multivariate Gaussian Distribution</a><input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-17"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/05_joint_distributions/0507_multivariate_gaussian/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/05_joint_distributions/0507_multivariate_gaussian/application_transformation.html">Application: Plots and Transformations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/05_joint_distributions/0507_multivariate_gaussian/psd.html">Covariance Matrix is Positive Semi-Definite</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/05_joint_distributions/0507_multivariate_gaussian/eigendecomposition.html">Eigendecomposition and Covariance Matrix</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/05_joint_distributions/0507_multivariate_gaussian/geometry_of_multivariate_gaussian.html">The Geometry of Multivariate Gaussians</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../probability_theory/06_sample_statistics/intro.html">Chapter 6. Sample Statistics</a><input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-18"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../probability_theory/06_sample_statistics/0601_moment_generating_and_characteristic_functions/intro.html">Moment Generating and Characteristic Functions</a><input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-19"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/06_sample_statistics/0601_moment_generating_and_characteristic_functions/moment_generating_function.html">Moment Generating Function</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/06_sample_statistics/0601_moment_generating_and_characteristic_functions/moment_generating_function_application_sum_of_rv.html">Application: Moment Generating Function and the Sum of Random Variables</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/06_sample_statistics/0601_moment_generating_and_characteristic_functions/characteristic_function.html">Characteristic Function</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../probability_theory/06_sample_statistics/0602_probability_inequalities/intro.html">Probability Inequalities</a><input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-20"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/06_sample_statistics/0602_probability_inequalities/concept.html">Probability Inequalities</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/06_sample_statistics/0602_probability_inequalities/application.html">Application: Learning Theory</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../probability_theory/06_sample_statistics/0603_law_of_large_numbers/intro.html">Law of Large Numbers</a><input class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-21"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/06_sample_statistics/0603_law_of_large_numbers/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/06_sample_statistics/0603_law_of_large_numbers/convergence.html">Convergence of Sample Average</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/06_sample_statistics/0603_law_of_large_numbers/application.html">Application: Learning Theory</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Optimization</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../optimization/gradient_descent/intro.html">Gradient Descent</a><input class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-22"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../optimization/gradient_descent/concept.html">Gradient Descent Concept</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../optimization/gradient_descent/implementation.html">Gradient Descent Construction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../optimization/gradient_descent/application.html">Application: Gradient Descent</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Machine Learning</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../fundamentals/intro.html">Fundamentals</a><input class="toctree-checkbox" id="toctree-checkbox-23" name="toctree-checkbox-23" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-23"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../fundamentals/criterions/intro.html">Loss</a><input class="toctree-checkbox" id="toctree-checkbox-24" name="toctree-checkbox-24" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-24"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../fundamentals/criterions/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../fundamentals/criterions/cross_entropy_loss.html">Cross Entropy Loss</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../fundamentals/criterions/focal_loss.html">Focal Loss</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../fundamentals/empirical_risk_minimization/intro.html">Empirical Risk Minimization</a><input class="toctree-checkbox" id="toctree-checkbox-25" name="toctree-checkbox-25" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-25"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../fundamentals/empirical_risk_minimization/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../fundamentals/empirical_risk_minimization/bayes_optimal_classifier.html">Bayes Optimal Classifier</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../fundamentals/learning_theory/intro.html">Is the Learning Problem Solvable?</a><input class="toctree-checkbox" id="toctree-checkbox-26" name="toctree-checkbox-26" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-26"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../fundamentals/learning_theory/concept.html">Concept</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../fundamentals/bias_and_variance/intro.html">Bias and Variance Tradeoff</a><input class="toctree-checkbox" id="toctree-checkbox-27" name="toctree-checkbox-27" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-27"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../fundamentals/bias_and_variance/concept.html">Bias-Variance Tradeoff Concept</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../fundamentals/decision_boundary/intro.html">Decision Boundary</a><input class="toctree-checkbox" id="toctree-checkbox-28" name="toctree-checkbox-28" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-28"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../fundamentals/decision_boundary/concept.html">Concept</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../fundamentals/voronoi_region/intro.html">Voronoi Region</a><input class="toctree-checkbox" id="toctree-checkbox-29" name="toctree-checkbox-29" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-29"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../fundamentals/voronoi_region/concept.html">Concept</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../intro.html">Linear Models</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-30" name="toctree-checkbox-30" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-30"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2 has-children"><a class="reference internal" href="../linear_regression/intro.html">Linear Regression</a><input class="toctree-checkbox" id="toctree-checkbox-31" name="toctree-checkbox-31" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-31"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../linear_regression/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../linear_regression/implementation.html">Implementation</a></li>
</ul>
</li>
<li class="toctree-l2 current active has-children"><a class="reference internal" href="intro.html">Logistic Regression</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-32" name="toctree-checkbox-32" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-32"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l3 current active"><a class="current reference internal" href="#">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="implementation.html">Implementation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../generalized_linear_models/intro.html">Generalized Linear Models</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../generative/intro.html">Generative</a><input class="toctree-checkbox" id="toctree-checkbox-33" name="toctree-checkbox-33" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-33"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../generative/naive_bayes/intro.html">Naive Bayes</a><input class="toctree-checkbox" id="toctree-checkbox-34" name="toctree-checkbox-34" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-34"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../generative/naive_bayes/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../generative/naive_bayes/implementation.html">Naives Bayes Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../generative/naive_bayes/example_penguins.html">Naive Bayes Application: Penguins</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../generative/naive_bayes/application_mnist.html">Naive Bayes Application (MNIST)</a></li>

</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../model_selection_and_evaluation/intro.html">Model Selection and Evaluation</a><input class="toctree-checkbox" id="toctree-checkbox-35" name="toctree-checkbox-35" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-35"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../model_selection_and_evaluation/metrics/intro.html">Metrics and Scoring Rules</a><input class="toctree-checkbox" id="toctree-checkbox-36" name="toctree-checkbox-36" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-36"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../model_selection_and_evaluation/metrics/classification/intro.html">Classification Metrics</a><input class="toctree-checkbox" id="toctree-checkbox-37" name="toctree-checkbox-37" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-37"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../model_selection_and_evaluation/metrics/classification/accuracy.html">Accuracy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../model_selection_and_evaluation/metrics/classification/precision_recall_f1.html">Precision, Recall and F1 Score</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../model_selection_and_evaluation/metrics/classification/brier_score.html">Brier Score</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../model_selection_and_evaluation/metrics/regression/intro.html">Regression Metrics</a><input class="toctree-checkbox" id="toctree-checkbox-38" name="toctree-checkbox-38" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-38"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../model_selection_and_evaluation/metrics/regression/mae.html">Mean Absolute Error</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../model_selection_and_evaluation/metrics/regression/rmse.html">(Root) Mean Squared Error</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../model_selection_and_evaluation/metrics/regression/mape.html">Mean Absolute Percentage Error</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../trees/intro.html">Trees, Forests, Bagging and Boosting</a><input class="toctree-checkbox" id="toctree-checkbox-39" name="toctree-checkbox-39" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-39"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../trees/decision_trees/intro.html">Decision Trees</a><input class="toctree-checkbox" id="toctree-checkbox-40" name="toctree-checkbox-40" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-40"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../trees/decision_trees/concept.html">Braindump</a></li>





</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../trees/ensemble_learning/intro.html">Ensemble Learning</a><input class="toctree-checkbox" id="toctree-checkbox-41" name="toctree-checkbox-41" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-41"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../trees/ensemble_learning/bagging/intro.html">Bagging</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../trees/ensemble_learning/random_forests/intro.html">Random Forests</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../trees/ensemble_learning/boosting/intro.html">Boosting</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../decomposition/intro.html">Dimensionality Reduction</a><input class="toctree-checkbox" id="toctree-checkbox-42" name="toctree-checkbox-42" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-42"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../decomposition/pca/intro.html">Principal Component Analysis</a><input class="toctree-checkbox" id="toctree-checkbox-43" name="toctree-checkbox-43" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-43"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../decomposition/pca/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../decomposition/pca/implementation.html">PCA</a></li>

<li class="toctree-l3"><a class="reference internal" href="../../decomposition/pca/eigenface.html">Eigenface</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../neighbours/intro.html">Neighbourhood</a><input class="toctree-checkbox" id="toctree-checkbox-44" name="toctree-checkbox-44" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-44"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../neighbours/k_nearest_neighbours/intro.html">K-Nearest Neighbours</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../mixtures/intro.html">Mixture Models</a><input class="toctree-checkbox" id="toctree-checkbox-45" name="toctree-checkbox-45" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-45"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../mixtures/gmm/intro.html">Gaussian Mixture Models</a><input class="toctree-checkbox" id="toctree-checkbox-46" name="toctree-checkbox-46" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-46"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../mixtures/gmm/concept.html">Concept</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../clustering/intro.html">Clustering</a><input class="toctree-checkbox" id="toctree-checkbox-47" name="toctree-checkbox-47" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-47"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../clustering/kmeans/intro.html">K-Means</a><input class="toctree-checkbox" id="toctree-checkbox-48" name="toctree-checkbox-48" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-48"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../clustering/kmeans/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../clustering/kmeans/implementation.html">Implementation: K-Means (Lloyd)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../clustering/kmeans/image_segmentation.html">Application: Image Compression and Segmentation</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Deep Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../deep_learning/natural_language_processing/intro.html">Natural Language Processing (NLP)</a><input class="toctree-checkbox" id="toctree-checkbox-49" name="toctree-checkbox-49" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-49"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../deep_learning/natural_language_processing/vector_semantics_and_embeddings/intro.html">Vector Semantics and Embeddings</a><input class="toctree-checkbox" id="toctree-checkbox-50" name="toctree-checkbox-50" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-50"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../deep_learning/natural_language_processing/vector_semantics_and_embeddings/words_and_vectors/intro.html">Words and Vectors</a><input class="toctree-checkbox" id="toctree-checkbox-51" name="toctree-checkbox-51" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-51"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../deep_learning/natural_language_processing/vector_semantics_and_embeddings/words_and_vectors/concept.html">Concept</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../deep_learning/natural_language_processing/vector_semantics_and_embeddings/cosine_similarity/intro.html">Cosine Similarity and Notion of Closeness</a><input class="toctree-checkbox" id="toctree-checkbox-52" name="toctree-checkbox-52" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-52"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../deep_learning/natural_language_processing/vector_semantics_and_embeddings/cosine_similarity/concept.html">Concept</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../deep_learning/natural_language_processing/vector_semantics_and_embeddings/cosine_similarity/implementation.html">Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../deep_learning/natural_language_processing/vector_semantics_and_embeddings/cosine_similarity/word_similarity.html">Application: Word Similarity</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../deep_learning/natural_language_processing/vector_semantics_and_embeddings/tf_idf/intro.html">Term Frequency-Inverse Document Frequency (TF-IDF)</a><input class="toctree-checkbox" id="toctree-checkbox-53" name="toctree-checkbox-53" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-53"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../deep_learning/natural_language_processing/vector_semantics_and_embeddings/tf_idf/concept.html">Concept</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../deep_learning/natural_language_processing/vector_semantics_and_embeddings/tf_idf/implementation.html">Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../deep_learning/natural_language_processing/vector_semantics_and_embeddings/tf_idf/application.html">Movie Recommender System</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">References, Resources and Roadmap</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../references_resources_roadmap/bibliography.html">Bibliography</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../references_resources_roadmap/resources.html">Resources</a></li>
</ul>

    </div>
</nav>
    </div>
  </div>
  

  
  <div class="sidebar-end-items sidebar-primary__section">
    <div class="sidebar-end-items__item">
    </div>
  </div>

  
  <div id="rtd-footer-container"></div>

      </div>
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

        <div class="bd-content">
          <div class="bd-article-container">
            
            <div class="bd-header-article">
                



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        <label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" data-toggle="tooltip" data-placement="right" title="Toggle primary sidebar">
            <span class="fa-solid fa-bars"></span>
        </label>
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="btn btn-sm"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<div class="dropdown dropdown-repository-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      <li><a href="https://github.com/gao-hongnan/gaohn-galaxy" target="_blank"
   class="btn btn-sm dropdown-item"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">repository</span>
</a>
</a>
      
      <li><a href="https://github.com/gao-hongnan/gaohn-galaxy/issues/new?title=Issue%20on%20page%20%2Fmachine_learning/linear_models/logistic_regression/concept.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm dropdown-item"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">open issue</span>
</a>
</a>
      
  </ul>
</div>



<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      <li><a href="../../../_sources/machine_learning/linear_models/logistic_regression/concept.md" target="_blank"
   class="btn btn-sm dropdown-item"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</a>
      
      <li>
<button onclick="printPdf(this)"
  class="btn btn-sm dropdown-item"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</a>
      
  </ul>
</div>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary" data-toggle="tooltip" data-placement="left" title="Toggle secondary sidebar">
            <span class="fa-solid fa-list"></span>
        </label>
    </div>
</div>
            </div>
            
            

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Concept</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#old-notes">
   Old Notes
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#logistic-regression">
   Logistic Regression
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>

            <article class="bd-article" role="main">
              
  <section class="tex2jax_ignore mathjax_ignore" id="concept">
<h1>Concept<a class="headerlink" href="#concept" title="Permalink to this heading">#</a></h1>
<p>One key think to note is Logistic Regression variants use <strong>conditional maximum likelihood estimation</strong> while naive bayes uses <strong>joint maximum likelihood estimation</strong>. The difference is the latter maximizes the joint distribution which find parameters that maximize the joint distribution
<span class="math notranslate nohighlight">\(\mathbb{P}_{\mathcal{D}}\left[\mathbf{X}, Y ; \boldsymbol{\theta}\right]\)</span>, while the former find parameters that maximizes the conditional
distribution <span class="math notranslate nohighlight">\(\mathbb{P}_{\mathcal{D}}\left[Y | \mathbf{X} ; \boldsymbol{\theta}\right]\)</span>.</p>
<section id="old-notes">
<h2>Old Notes<a class="headerlink" href="#old-notes" title="Permalink to this heading">#</a></h2>
<p><strong>TO REFACTOR</strong></p>
<ul class="simple">
<li><p>The choice of sigmoid not only satisfies the problems we faced, but it is also mathematically convenient. In reality, we can have other choices other than sigmoid.</p></li>
</ul>
<ol class="arabic">
<li><p>We have stated that linear regression is not appropriate in the case of a qualitative response. Why not? Suppose that we are trying to predict the medical condition of a patient in the emergency room on the basis of her symptoms. In this simplified example, there are three possible diagnoses: stroke, drug overdose, and epileptic seizure. We could consider encoding these values as a quantitative response variable, <span class="math notranslate nohighlight">\(Y\)</span>, as follows:
$<span class="math notranslate nohighlight">\(
 \begin{equation}
     Y=\begin{cases}
     1, &amp; \text{if stroke}\\
     2, &amp; \text{if drug overdose}\\
     3, &amp; \text{if epileptic seizure}\\
     \end{cases}
 \end{equation}
 \)</span>$</p>
<p>Using this coding, least squares could be used to fit a linear regression model to predict <span class="math notranslate nohighlight">\(Y\)</span> on the basis of a set of predictors <span class="math notranslate nohighlight">\(X_{1}\)</span>, â€¦ , <span class="math notranslate nohighlight">\(X_{p}\)</span>. Unfortunately, this coding implies an ordering on the outcomes, putting drug overdose in between stroke and epileptic seizure, and insisting that the difference between stroke and drug overdose is the same as the difference between drug overdose and epileptic seizure. In practice there is no particular reason that this needs to be the case. For instance, one could choose an
equally reasonable coding,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{equation}
        Y=\begin{cases}
        1, &amp; \text{if epileptic seizure}\\
        2, &amp; \text{if stroke}\\
        3, &amp; \text{if drug overdose}\\
        \end{cases}
    \end{equation}
    \end{split}\]</div>
<p>which would imply a totally different relationship among the three conditions. Each of these codings would produce fundamentally different linear models that would ultimately lead to different sets of predictions on test observations.</p>
<p>If the response variableâ€™s values did take on a natural ordering, such as mild, moderate, and severe, and we felt the gap between mild and moderate was similar to the gap between moderate and severe, then a 1, 2, 3 coding would be reasonable. Unfortunately, in general there is no natural way to convert a qualitative response variable with more than two levels into a quantitative response that is ready for linear regression.</p>
<p>For a binary (two level) qualitative response, the situation is better. For instance, perhaps there are only two possibilities for the patientâ€™s medical condition: stroke and drug overdose. We could then potentially use the dummy variable approach to code the response as follows:
$<span class="math notranslate nohighlight">\(
 \begin{equation}
     Y=\begin{cases}
     0, &amp; \text{if stroke}\\
     1, &amp; \text{if drug overdose}\\
     \end{cases}
 \end{equation}
 \)</span>$</p>
<p>We could then fit a linear regression to this binary response, and predict drug overdose if <span class="math notranslate nohighlight">\(\hat{Y}&gt;0.5\)</span> and stroke otherwise. In the binary case it is not hard to show that even if we flip the above coding, linear regression will produce the same final predictions.</p>
<p>For a binary response with a <span class="math notranslate nohighlight">\(0-1\)</span> coding as above, regression by least squares does make sense; it can be shown that the <span class="math notranslate nohighlight">\(X\hat{\beta}\)</span> obtained using linear regression is in fact an estimate of <span class="math notranslate nohighlight">\(\text{Pr}(\text{Drug Overdoes}~|~X)\)</span> in this special case. However, if we use linear regression, some of our estimates might be outside the <span class="math notranslate nohighlight">\([0\)</span>, 1<span class="math notranslate nohighlight">\(]\)</span> interval , making them hard to interpret as probabilities!</p>
<p>However, the dummy variable approach cannot be easily extended to accommodate qualitative responses with more than <strong>two levels!!!</strong> For these reasons, it is preferable to use a classification method that is truly suited for qualitative response values, such as the ones presented next.</p>
</li>
<li><p>Now we recall back in linear regression, we have independent input/variables <span class="math notranslate nohighlight">\(X\)</span> and we seek a response/output variable <span class="math notranslate nohighlight">\(Y\)</span>. However in logistic regression, our output is in the form of a categorical variable. In our simple tutorial we will only be considering a binary output binary coded as 0 and 1.</p>
<p>As mentioned in the previous paragraph, we cannot use linear regression to predict a categorical output even if the categorical output is coded as numerical values, especially in such a classification problem with many levels in a categorical variable. As a result, we need to come up with a slightly different <strong>hypothesis</strong> to model our relationship for <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>.</p>
<p>As with any modelling, there should be a â€œformulaâ€ between the <span class="math notranslate nohighlight">\(X\)</span> and the <span class="math notranslate nohighlight">\(Y\)</span>. However, we have already established it is not easy to obtain an direct equation between <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>. Instead, in logistic regression, we are more interested in having a relation between <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(P(Y = 1 ~|~ X)\)</span>. One should immediately be asking, what and why is <span class="math notranslate nohighlight">\(P(Y=1~|~X)\)</span>? Let me give you an intuition by the following example.</p>
</li>
</ol>
<hr class="docutils" />
<ol class="arabic" start="3">
<li><p><strong>Example</strong>: We are trying to predict if a person has malignant tumor or not based on some inputs such as â€œTumor Sizeâ€ and etc. For simplicity sake, we only deal with one variable: The â€œTumor Sizeâ€ <span class="math notranslate nohighlight">\(X\)</span>.</p>
<p>Our output <span class="math notranslate nohighlight">\(Y\)</span> is basically encoded as a binary class where <strong>Yes it is Malignant Tumor</strong> stands for 1 and <strong>No it is not a Malignant Tumor</strong> stands for 0. Consider the data set above, where the response â€œ<span class="math notranslate nohighlight">\(Y\)</span> = Malignantâ€ falls into one of two categories, Yes (1) or No (0). As mentioned in the previous paragraph, rather than modeling this response <span class="math notranslate nohighlight">\(Y\)</span> directly with <span class="math notranslate nohighlight">\(X\)</span>, logistic regression models the probability that <span class="math notranslate nohighlight">\(Y\)</span> belongs to a particular category. One can see the from the below figure, the first row says that the tumour size is 1 cm and our data says that the patientâ€™s tumour is not malignant (thank god!). But as we said, we want the probability of the tumour being malignant <strong>given</strong> <span class="math notranslate nohighlight">\(X = 1\)</span>cm. So we write down the probability as shown below (just get this intuition first and donâ€™t wonder how are we going to find the probability first!).</p>
<p><img alt="1" src="https://raw.githubusercontent.com/ghnreigns/imagesss/master/logistic-8.jpg" /></p>
<p>For this, in a simple (one variable only) logistic regression model, we can define our output <span class="math notranslate nohighlight">\(Y\)</span> as a probability defined as $<span class="math notranslate nohighlight">\(P(Y=1~|~ X)\)</span>$</p>
<p><strong>So to reiterate, instead of modelling our <span class="math notranslate nohighlight">\(Y\)</span> directly with <span class="math notranslate nohighlight">\(X\)</span>, we aim to find a model that can model the probability of <span class="math notranslate nohighlight">\(Y\)</span> given <span class="math notranslate nohighlight">\(X\)</span>.</strong> But why? How does getting a probability help us? Although it should be obvious that <span class="math notranslate nohighlight">\(P(Y=1~|~X)\)</span> should fall in between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span> since it is a probability, <strong>but</strong> it does not answer our question of whether you are in class 1 or 0 because ultimately, we are interested in finding out our output value which is either a 1 or a 0.</p>
<p>I DO NOT CARE if you told me you found that  <span class="math notranslate nohighlight">\(P(Y=1~|~X =1.1 \text{ cm}) = 0.2349538\)</span> which is neither 0 or 1? So this is where <strong>classification threshold</strong> comes in. You need to pre-define a threshold (default is usually 0.5). As a result, if we use a classification threshold of 0.5, then we will predict a <span class="math notranslate nohighlight">\(Y = \text{Yes } (1)\)</span> for any <span class="math notranslate nohighlight">\(P(Y=1|~X) &gt; 0.5\)</span>. To write it more compactly, we define the following indicator function. (For more experienced peeps, there is connection with AUC,ROC curves here as well!)</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{equation}
        Y=\begin{cases}
        1, &amp; \text{if $P(Y=1~|~X) \geq 0.5$}\\
        0, &amp; \text{if $P(Y=1~|~X) &lt; 0.5$}\\
        \end{cases}
    \end{equation}
    \end{split}\]</div>
<p>As a result if your tumour size is 1.1 cm, then the probability of your tumour is malignant is <span class="math notranslate nohighlight">\(0.2349538\)</span>, which is less than 0.5 and we predict your as a No (not malignant). However, the threshold is there for a reason, usually, in medical and healthcare industry, we tend to be more conservative with our predictions as we have 0 tolerance for False Negatives. We rather give you a false alarm than to classify you as No Malignancy when in fact you are already at the last stage of your life. So we can tune and change our threshold to something like <span class="math notranslate nohighlight">\(P(Y=1~|~X) &gt;0.1\)</span> and in this case, <span class="math notranslate nohighlight">\(0.2349538\)</span> will be in the Yes class.</p>
<p>I know we are going off the tracks, but I hope I have provided you with some intuition on how modelling the <span class="math notranslate nohighlight">\(P(Y=1~|~X)\)</span> as a function of <span class="math notranslate nohighlight">\(X\)</span> makes sense here.</p>
</li>
</ol>
<hr class="docutils" />
<ol class="arabic" start="4">
<li><p><strong>Hypothesizing and Modelling the Logistic Function:</strong> For simplicity we call our function <span class="math notranslate nohighlight">\(P(Y=1~|~ X)\)</span> as the function <span class="math notranslate nohighlight">\(p(X)\)</span> and we seek to find a relationship between <span class="math notranslate nohighlight">\(p(X)\)</span> and <span class="math notranslate nohighlight">\(X\)</span>. Although we have gone through a lot of ideas just now, it would be meaningless if we cannot find a suitable function (equation) to model <span class="math notranslate nohighlight">\(p(X)\)</span> and <span class="math notranslate nohighlight">\(X\)</span>.</p>
<p><strong>Hypothesis 1: The Linear Hypothesis</strong></p>
<p>Hmm, so we got quite some success hypothesizing linear regression models with linear functions, can we try that too on <span class="math notranslate nohighlight">\(p(X)\)</span> and <span class="math notranslate nohighlight">\(X\)</span>? Consider that we â€œguess/hypothesizeâ€ that <span class="math notranslate nohighlight">\(p(X)\)</span> have a <strong>linear relationship</strong> with <span class="math notranslate nohighlight">\(X\)</span> as follows:</p>
<div class="math notranslate nohighlight">
\[
    p(X) = \beta_0+\beta_1 X
    \]</div>
<p>However the problem with this modelling is that for very large Tumour sizes <span class="math notranslate nohighlight">\(X\)</span>, say <span class="math notranslate nohighlight">\(X=10 \text{ cm}\)</span>, then our <span class="math notranslate nohighlight">\(p(X)\)</span> may take values greater than <span class="math notranslate nohighlight">\(1\)</span>. And for extremely small Tumour sizes <span class="math notranslate nohighlight">\(X\)</span>, say those very small benign lumps, which may be <span class="math notranslate nohighlight">\(X=0.05 \text{ cm}\)</span> in size, then <span class="math notranslate nohighlight">\(p(X)\)</span> may take negative values. In any case, no matter how likely or unlikely one is to find his/her tumour to be malignant, how big or small the tumour size is, our <span class="math notranslate nohighlight">\(p(X)\)</span> should only output values between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span> because <span class="math notranslate nohighlight">\(p(X)\)</span> is a probability. Hence our linear model may be accurate to a certain extent, but not sensible.</p>
<p><strong>Hypothesis 2</strong></p>
<p>Instead of the linear hypothesis, we come up with another one, recall that we learnt that probability and odds have similar definition. And recall that</p>
<div class="math notranslate nohighlight">
\[
   \text{odds} = \dfrac{P(Y=1~|~X)}{1-P(Y=1~|~X)}
   \]</div>
<p>So why not model the odds against <span class="math notranslate nohighlight">\(X\)</span>? If we can successfully do that, then we can easily get the probability <span class="math notranslate nohighlight">\(P(Y=1~|~X)\)</span> since odds and probability are in a if and only if relationship. So let us try:</p>
<div class="math notranslate nohighlight">
\[
   \text{odds} = \dfrac{p(X)}{1-p(X)} = \beta_0 + \beta_1X
   \]</div>
<p>But ALAS! We soon realise that the odds can only take on values from <span class="math notranslate nohighlight">\(0\)</span> to <span class="math notranslate nohighlight">\(\infty\)</span>, but the problem still exists for the <span class="math notranslate nohighlight">\(\beta_0 + \beta_1X\)</span> since some <span class="math notranslate nohighlight">\(X\)</span> values can output negative values.</p>
<p>But we are close, and if one has <strong>some</strong> mathematical backgrounds, then we know that if we take the log or ln of <span class="math notranslate nohighlight">\(\text{odds}\)</span> then we can have the desired results.</p>
<p><strong>Hypothesis 3: The Chosen one</strong></p>
<p>If we finally consider the modelling of the logarithm of the odds, against the variable <span class="math notranslate nohighlight">\(X\)</span>, where we still assume a linear relationship, then we may be good to go because the logarithm of the odds gives a range of <span class="math notranslate nohighlight">\(-\infty\)</span> to <span class="math notranslate nohighlight">\(\infty\)</span> and matches well with <span class="math notranslate nohighlight">\(\beta_0+\beta_1X\)</span>.</p>
<p>With this we have achieved a regression model, with the output of the model being the logarithm or ln of the odds. i.e: the modelled equation is as follows:</p>
<div class="math notranslate nohighlight">
\[
    \ln\left(\dfrac{p(X)}{1-p(X)}\right) = \beta_0+\beta_1X
    \]</div>
<p>The main reason we reach this step is because both sides of the equation can take in the same range, and thus makes more mathematical sense now. We have yet to estimate or found what the coefficients <span class="math notranslate nohighlight">\(\beta_0, \beta_1\)</span> are. This is just a logical and sound hypothesis.</p>
<p><strong>Recovering the Logistic Function from log odds</strong></p>
<p>So in the previous paragraph we have settled on a hypothesis that there is a <strong>linear relationship</strong> between the predictor variable <span class="math notranslate nohighlight">\(X\)</span> and the <strong>log-odds</strong> of the event that <span class="math notranslate nohighlight">\(Y=1\)</span>. However, do not forget what our original aim is, we modelled log odds against <span class="math notranslate nohighlight">\(X\)</span> simply because the relationship can be mathematically justified, we ultimately want to find the probability of <span class="math notranslate nohighlight">\(Y=1\)</span> given <span class="math notranslate nohighlight">\(X\)</span>. And that is easy, by some reverse engineering, once <span class="math notranslate nohighlight">\(\beta_0, \beta_1\)</span> are fixed, we do some manipulation:</p>
</li>
</ol>
<div class="math notranslate nohighlight">
\[
\ln\left(\dfrac{p(X)}{1-p(X)}\right) = \beta_0+\beta_1X \iff  \dfrac{p(X)}{1-p(X)} = \exp{(\beta_0+\beta_1X)}   \iff p(X)  = \dfrac{\exp{(\beta_0+\beta_1X)}}{\exp{(\beta_0+\beta_1X)}+1} \iff p(X) = \dfrac{1}{1+\exp^{-1}{(\beta_0+\beta_1X)}}
\]</div>
<p><strong>Given the log odd mode (logit model actually), we can recover the probability of <span class="math notranslate nohighlight">\(Y=1\)</span> given <span class="math notranslate nohighlight">\(X\)</span> for each <span class="math notranslate nohighlight">\(X\)</span>.</strong></p>
<hr class="docutils" />
<ol class="arabic" start="5">
<li><p><strong>Important - The workflow process of Logistic Regression</strong></p>
<ul>
<li><p>Given an indepedent variable <span class="math notranslate nohighlight">\(X\)</span>, we aim to predict a binary dependent variable <span class="math notranslate nohighlight">\(Y\)</span>.</p></li>
<li><p>It is not easy to model a relationship between <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> directly, instead, we find the probability of <span class="math notranslate nohighlight">\(Y = 1\)</span> given <span class="math notranslate nohighlight">\(X\)</span> instead. Imagine we are in the shoes of the famous statistician DR Cox in the year 1958, we are building logistic regression from scratch, and we tried to hypothesize that the probability <span class="math notranslate nohighlight">\(p(X) = P(Y=1~|~X)\)</span> can be modelled the same way as <strong>linear regression?</strong> But soon realised that modelling <span class="math notranslate nohighlight">\(p(X) = \beta_0+\beta_1X\)</span> is not good since its range gives values out of <span class="math notranslate nohighlight">\([0,1]\)</span>. In order to overcome this we can make a transformation and fit the sigmoid/logistic function which forces the output <span class="math notranslate nohighlight">\(p(X)\)</span> to be in <span class="math notranslate nohighlight">\([0,1]\)</span>.</p></li>
<li><p>Since the transformation may not be intuitive, I have made a simple explanation above, and showed steps on how to model <span class="math notranslate nohighlight">\(P(X)\)</span> as a sigmoid function.</p>
<p>Sigmoid in logistic regression:</p>
</li>
</ul>
<div class="math notranslate nohighlight">
\[
    p(X) = \dfrac{1}{1+\exp^{-1}{(\beta_0+\beta_1X)}}
    \]</div>
<ul class="simple">
<li><p>So, we have effectively build a model, and in fact it is a probabilistic model behaving as a bernoulli distribution. To recover the probability <span class="math notranslate nohighlight">\(p(X)\)</span>, we have to estimate the coefficients (parameters) in</p></li>
</ul>
</li>
</ol>
<div class="math notranslate nohighlight">
\[
  p(X) = \dfrac{1}{1+\exp^{-1}{(\beta_0+\beta_1X)}}
  \]</div>
<p>and we use a method called <strong>Maximum Likelihood (I will do a part 2 on this as it is also a big topic)</strong>.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>- Once we recover the coefficients $\beta_0, \beta_1$, we can simply plug in the coefficients and the respective values of $X$ to get $p(X)$.

- Once we get the $p(X)$, we can define a indicator function as our classification threshold (mentioned earlier) and subsequently, get all the values of $Y$.
</pre></div>
</div>
<p>First you need to be very clear about what a <strong>probability distribution is.</strong> Consider that we have 10 students and we model their marks where the full marks of the test is 16/16. Define a random variable <span class="math notranslate nohighlight">\(X\)</span> where <span class="math notranslate nohighlight">\(X\)</span> represents the marks of each student. Assume further that this <strong>random variable</strong> <span class="math notranslate nohighlight">\(X\)</span> is following a <strong>normal distribution</strong> with <span class="math notranslate nohighlight">\(\mu = 11\)</span> and <span class="math notranslate nohighlight">\(\sigma = 3\)</span>, can we find the probability distribution for the marks of the whole cohort (10 students)? Yes we can, because we have the parameters. If you do not know what is the meaning of parameters, please go revise on it, it is very important for you to understand that the <strong>parameter</strong> decides the probability distribution of any model.</p>
<p>Recall the general formula for the pdf of the normal distribution is $<span class="math notranslate nohighlight">\(f(X = x) = \dfrac{e^{-(x-\mu)^2}/(2\sigma^2)}{\sigma \sqrt{2\pi}}\)</span><span class="math notranslate nohighlight">\(
and in normal distribution once we have the mean and standard deviation of the dataset, we can recover the whole pdf of the model, hence the mean and standard deviation are our parameters.  So let us say we want to find \)</span>P(11 &lt; X &lt; 13~|~\mu = 11, \sigma = 3)<span class="math notranslate nohighlight">\(, we can easily find it to be around \)</span>0.31 = 31%<span class="math notranslate nohighlight">\(, we can basically find any probabilities **as long as we are given the parameters**. So, **we must have the correct mindset that,** probability density functions (or pmf alike) are legitimate functions that takes in any \)</span>X = x<span class="math notranslate nohighlight">\( and outputs the probability of this \)</span>x<span class="math notranslate nohighlight">\( **happening**. (Of course in continuous distribution we are usually only interested in the range of \)</span>x$, but for the purpose of intuition, we do not need to be so pedantic).</p>
<p><strong>Likelihood Function</strong></p>
<p>However, in the real world setting, more often than not, we have the data <span class="math notranslate nohighlight">\(X\)</span>, like we have conveniently the scores of all the 10 students above, which <strong>could be a random sample taken</strong> from the whole schoolâ€™s population. Now we are tasked to find the probability distribution of the whole population (say 10,000 students), and we would have calculate it ever so easily <strong>if we knew what the parameters were!?</strong> Unfortunately we do not have the true parameters.</p>
<p>Our main motivation now is to find the <strong>parameter</strong>, because without it, we cannot complete the task of finding the distribution of the population. We can never know the real/true parameter <span class="math notranslate nohighlight">\(\theta = (\mu, \sigma)\)</span>, but we can obtain a good estimator of it, by making use of the data that we do have! In this scenario we were given 10 data points (in real life it is usually much more), say the 10 data points are $<span class="math notranslate nohighlight">\(\mathbf{X} = [3,9,4,10,12,16,5,11,9,9]\)</span>$</p>
<p>So we do a sleight of hand using our original <strong>probability density function</strong>, <span class="math notranslate nohighlight">\(P(X = x~|~ \theta)\)</span>. Instead of being a function of <span class="math notranslate nohighlight">\(X = x\)</span> where <span class="math notranslate nohighlight">\(\theta\)</span> is known, we instead let <span class="math notranslate nohighlight">\(X = x\)</span> be fixed, and let <span class="math notranslate nohighlight">\(\theta\)</span> be the variable now. The idea is that this function now is <strong>NO LONGER a function of</strong> <span class="math notranslate nohighlight">\(X=x\)</span>, and is instead a function of <span class="math notranslate nohighlight">\(\theta\)</span>, where it takes in all possible values of <span class="math notranslate nohighlight">\(\theta\)</span>, and outputs a value called the \textbf{likelihood value.} So now, in a less informal way, our new function looks like $<span class="math notranslate nohighlight">\(P(\mathbf{X} = [3,9,4,10,12,16,5,11,9,9]~|~ \theta)\)</span>$ and it means <strong>what is the probability of OBSERVING these data points, given different values of theta.</strong> One needs to plot the graph of likelihood out to get a better idea (Wikipedia).</p>
<p>So imagine our function (plot likelihood value vs parameter) has a local/global maximum, and that maximum is what we are finding ultimately. Because it is reasonable for us to believe that, the <strong>parameter</strong> that gives us the maximum value of <span class="math notranslate nohighlight">\(P(\mathbf{X} = [3,9,4,10,12,16,5,11,9,9]~|~ \theta)\)</span> will suggest that <strong>given these 10 data points</strong>, this <span class="math notranslate nohighlight">\(\theta\)</span> that we just found, gives us the highest <strong>likelihood/probability</strong> that these 10 points are actually observed.</p>
<p>We formally define this function to be $<span class="math notranslate nohighlight">\(\mathcal{L}(\theta~|~ X = x) = P(X = x~|~\theta) \)</span>$</p>
<p>I cannot emphasize enough that even those the likelihood function <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> and the probability function <span class="math notranslate nohighlight">\(P\)</span> have the exact same form, they are fundamentally different in which one is a function of the parameter <span class="math notranslate nohighlight">\(\theta\)</span>, and the other is a function of the data points <span class="math notranslate nohighlight">\(X = x\)</span>.</p>
<p><strong>Maximizing the likelihood function</strong></p>
<p>Many of us may not have rigourous mathematical background, and hence when I said we are trying to find the value of <span class="math notranslate nohighlight">\(\theta\)</span> that maximizes <span class="math notranslate nohighlight">\(\mathcal{L}\)</span>, we might be stunned, indeed, the most intuitive way is to try out every possible theta and see which theta gives us the highest value; or if you can try differentiating <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> and set the derivative to <span class="math notranslate nohighlight">\(0\)</span> to solve for <span class="math notranslate nohighlight">\(\theta\)</span>. In any case, our intuition is to maximize this function. We will illustrate the idea of maximum likelihood with a simple example below.</p>
<p><strong>Example</strong></p>
<p>Inspired and credits to <a class="reference external" href="https://towardsdatascience.com/probability-concepts-explained-maximum-likelihood-estimation-c7b4342fdbb1">Jonny Brooks-Bartlett</a> as the example is almost similar.</p>
<ul class="simple">
<li><p>Letâ€™s say we have a population of 1000 students and a random sample of 3 university students are taken with their respective marks for a Machine learning test. The data are distributed below in python (generate some data of 3 students marks).</p></li>
<li><p>First, we always have to come up with a hypothesized model first, and in this case, based on these 3 sample data points, we make a simple plot and since the plot looks somewhat normal, we hypothesize that our model should be of a normal distribution (do be reminded this is just for illustration, 3 points may be too little for us to hypothesize a model in many scenarios).</p></li>
<li><p>Our aim is to come up with a normal distribution for this 1000 students marks. In order to do that, we need to find the parameters and recall that in normal distribution, our parameters are the mean <span class="math notranslate nohighlight">\(\mu\)</span> and the standard deviation  <span class="math notranslate nohighlight">\(\sigma\)</span>. So we have to find the best parameters that best describe our observed 3 sample points.</p></li>
<li><p>Note that from our previous section, we know that we just need to maximize the likelihood function given by $<span class="math notranslate nohighlight">\(\mathcal{L}\left(\theta = (\mu, \sigma)~|~ X = [9,9.5,11]\right) = P\left(X = [9,9.5,11]~;~ \theta = (\mu, \sigma)\right)\)</span>$</p></li>
<li><p>Here the author made a very good point that we shall use ; instead of | to indicate probability function. However some people like to use | as probability function can be described in a conditional way.</p></li>
<li><p>And since our assumption that this 3 data points are independent of each other, our joint pdf equation can be simplified to</p></li>
</ul>
<div class="math notranslate nohighlight">
\[P\left(X = [9,9.5,11]~;~ \theta = (\mu, \sigma)\right) = P(9;\theta = [\mu, \sigma]) \cdot P(9.5;\theta = [\mu, \sigma]) \cdot  P(11;\theta = [\mu, \sigma]) = \dfrac{1}{\sigma\sqrt{2\pi}}\exp\left(-\dfrac{(9-\mu)^2}{2\sigma^2}\right) \cdot \dfrac{1}{\sigma\sqrt{2\pi}}\exp\left(-\dfrac{(9.5-\mu)^2}{2\sigma^2}\right) \cdot \dfrac{1}{\sigma\sqrt{2\pi}}\exp\left(-\dfrac{(11-\mu)^2}{2\sigma^2}\right)~~~~~~~~(2.1)\]</div>
<p>Equation 2.1 above is the joint probability of observing these 3 points and is a function of the parameters, so we suffice to find the parameters that maximize this function. So the intuition is to differentiate equation 2.1 and set it to 0, and solve for the parameter <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<ul class="simple">
<li><p>However, the expression in 2.1 is very difficult to differentiate, and that is why it is very common to see we take log/ln on both sides of the expression</p></li>
</ul>
<p><span class="math notranslate nohighlight">\(\ln\mathcal{L} = \ln\left[\frac{1}{\sigma\sqrt{2\pi}}\exp\left(-\frac{(9-\mu)^2}{2\sigma^2}\right) \cdot \frac{1}{\sigma\sqrt{2\pi}}\exp\left(-\frac{(9.5-\mu)^2}{2\sigma^2}\right) \cdot \frac{1}{\sigma\sqrt{2\pi}}\exp\left(-\frac{(11-\mu)^2}{2\sigma^2}\right) \right]\)</span></p>
<p>and instead maximize the new log-likelihood function instead. But some may be confused, when we solve the \textbf{parameters} for the above equation, will our parameter still be the one that actually maximizes our equation 2.1? Yes, it does, the proof can be found here (for those who are curious). <a class="reference external" href="https://math.stackexchange.com/questions/647835/why-does-finding-the-x-that-maximizes-lnfx-is-the-same-as-finding-the#_=_">proof</a></p>
<ul class="simple">
<li><p>We simplify the above <span class="math notranslate nohighlight">\(\ln\mathcal{L}\)</span> function to get $<span class="math notranslate nohighlight">\(\ln\mathcal{L} = -3\ln(\sigma)-1.5\ln(2\pi) - \frac{1}{2\sigma^2}\left[(9-\mu)^2+(9.5-\mu)^2+(11-\mu)^2\right]~~~(2.2)\)</span>$</p></li>
<li><p>We take the partial derivatives with respect to <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span> respectively and set to <span class="math notranslate nohighlight">\(0\)</span> to solve equation 2.2, thereby getting our <span class="math notranslate nohighlight">\(\theta\)</span>.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\dfrac{\partial \ln(\mathcal{L})}{\partial \mu} = \dfrac{1}{\sigma^2}[9+9.5+11-3\mu] = 0 \implies \mu = 9.833\]</div>
<div class="math notranslate nohighlight">
\[\dfrac{\partial \ln(\mathcal{L})}{\partial \sigma} = -\dfrac{3}{\sigma} + 4\sigma^{-3}\left[(9-\mu)^2+(9.5-\mu)^2+(11-\mu)^2\right] =0 \implies \sigma = 1.7\]</div>
<ul class="simple">
<li><p>So now we have found the estimate of the real parameter of the population, and we can therefore infer that our populationsâ€™ probability density function.</p></li>
<li><p>However, we are not really done yet, this example was easy and therefore can be calculated with hand. In reality, there is no closed form solution expression for the coefficient values that maximize the likelihood function, so that an iterative process must be used instead; for example Newtonâ€™s method. This process begins with a tentative solution, revises it slightly to see if it can be improved, and repeats this revision until no more improvement is made, at which point the process is said to have converged.(Wikipedia)</p></li>
</ul>
<p><strong>It is important to realise that if convergence for the model is not reached, the coefficients/parameters you obtained may not be meaningful. Therefore, certain assumptions need to be fulfilled for logistic regression.</strong></p>
<p><strong>Assumptions of logistic regression and Regularization will be mentioned in part III, which is also the final part of the Logistic Regression Trilogy</strong>.</p>
</section>
<section id="logistic-regression">
<h2>Logistic Regression<a class="headerlink" href="#logistic-regression" title="Permalink to this heading">#</a></h2>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./machine_learning\linear_models\logistic_regression"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

            </article>
            

            
            
            <footer class="bd-footer-article">
                <!-- Previous / next buttons -->
<div class='prev-next-area'>
  <a class='left-prev' id="prev-link" href="intro.html" title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
          <p class="prev-next-subtitle">previous</p>
          <p class="prev-next-title">Logistic Regression</p>
      </div>
  </a>
  <a class='right-next' id="next-link" href="implementation.html" title="next page">
  <div class="prev-next-info">
      <p class="prev-next-subtitle">next</p>
      <p class="prev-next-title">Implementation</p>
  </div>
  <i class="fa-solid fa-angle-right"></i>
  </a>
</div>
            </footer>
            
          </div>
          
          
          
            <div class="bd-sidebar-secondary bd-toc">
              
<div class="toc-item">
  
<div class="tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
</div>
<nav id="bd-toc-nav" class="page-toc">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#old-notes">
   Old Notes
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#logistic-regression">
   Logistic Regression
  </a>
 </li>
</ul>

</nav>
</div>

            </div>
          
          
        </div>
        <footer class="bd-footer-content">
          <div class="bd-footer-content__inner">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Gao Hongnan
</p>

  </div>
  
  <div class="footer-item">
    
<p class="copyright">

    &copy; Copyright 2023.<br>

</p>

  </div>
  
  <div class="footer-item">
    <p class="last-updated">
Last updated on None.<br>
</p>
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </div>
        </footer>
        

      </main>
    </div>
  </div>

  
    
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/bootstrap.js?digest=796348d33e8b1d947c94"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=796348d33e8b1d947c94"></script>

  </body>
</html>