
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Concept &#8212; Machine Learning Chronicles</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=796348d33e8b1d947c94" rel="stylesheet">
<link href="../../../_static/styles/bootstrap.css?digest=796348d33e8b1d947c94" rel="stylesheet">
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=796348d33e8b1d947c94" rel="stylesheet">

  
  <link href="../../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=796348d33e8b1d947c94" rel="stylesheet">
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2">
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2">
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" href="../../../_static/styles/sphinx-book-theme.css?digest=4ec06e9971c5264fbd345897d5258098f11cc577" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=796348d33e8b1d947c94">
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=796348d33e8b1d947c94">

    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?digest=8bf782fb4ee92b3d3646425e50f299c4e1fd152d"></script>
    <script src="../../../_static/tabs.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"tex": {"macros": {"defeq": "\\overset{\\text{def}}{=}", "defa": "\\overset{\\text{(a)}}{=}", "defb": "\\overset{\\text{(b)}}{=}", "defc": "\\overset{\\text{(c)}}{=}", "defd": "\\overset{\\text{(d)}}{=}", "st": "\\mid", "mod": "\\mid", "S": "\\Omega", "s": "\\omega", "e": "\\exp", "P": "\\mathbb{P}", "R": "\\mathbb{R}", "expectation": "\\mathbb{E}", "v": "\\mathbf{v}", "a": "\\mathbf{a}", "b": "\\mathbf{b}", "c": "\\mathbf{c}", "u": "\\mathbf{u}", "w": "\\mathbf{w}", "x": "\\mathbf{x}", "y": "\\mathbf{y}", "z": "\\mathbf{z}", "0": "\\mathbf{0}", "1": "\\mathbf{1}", "A": "\\mathbf{A}", "B": "\\mathbf{B}", "C": "\\mathbf{C}", "E": "\\mathcal{F}", "eventA": "\\mathcal{A}", "lset": "\\left\\{", "rset": "\\right\\}", "lsq": "\\left[", "rsq": "\\right]", "lpar": "\\left(", "rpar": "\\right)", "lcurl": "\\left\\{", "rcurl": "\\right\\}", "pmf": "p_X", "pdf": "f_X", "pdftwo": "f_{X,Y}", "pdfjoint": "f_{\\mathbf{X}}", "pmfjointxy": "p_{X, Y}", "pdfjointxy": "f_{X, Y}", "cdf": "F_X", "pspace": "(\\Omega, \\mathcal{F}, \\mathbb{P})", "var": "\\operatorname{Var}", "std": "\\operatorname{Std}", "bern": "\\operatorname{Bernoulli}", "binomial": "\\operatorname{Binomial}", "geometric": "\\operatorname{Geometric}", "poisson": "\\operatorname{Poisson}", "uniform": "\\operatorname{Uniform}", "normal": "\\operatorname{Normal}", "gaussian": "\\operatorname{Gaussian}", "gaussiansymbol": "\\mathcal{N}", "exponential": "\\operatorname{Exponential}", "iid": "\\textbf{i.i.d.}", "and": "\\text{and}"}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'machine_learning/generative/naive_bayes/concept';</script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Naives Bayes Implementation" href="implementation.html" />
    <link rel="prev" title="Naive Bayes" href="intro.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="docsearch:language" content="en">
  </head>
  
  
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="180" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>

  
  <input type="checkbox" class="sidebar-toggle" name="__primary" id="__primary">
  <label class="overlay overlay-primary" for="__primary"></label>

  
  <input type="checkbox" class="sidebar-toggle" name="__secondary" id="__secondary">
  <label class="overlay overlay-secondary" for="__secondary"></label>

  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
      
<form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false">
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
    </div>
  </div>

  
  <nav class="bd-header navbar navbar-expand-lg bd-navbar" id="navbar-main"><div class="bd-header__inner bd-page-width">
  <label class="sidebar-toggle primary-toggle" for="__primary">
      <span class="fa-solid fa-bars"></span>
  </label>
  <div id="navbar-start">
    
    
  


<a class="navbar-brand logo" href="../../../intro.html">

  
  
  
  
  
  
  

  
    <img src="../../../_static/logo.png" class="logo__image only-light" alt="Logo image">
    <img src="../../../_static/logo.png" class="logo__image only-dark" alt="Logo image">
  
  
</a>
    
  </div>

  
  <div class="col-lg-9 navbar-header-items">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <nav class="navbar-nav">
    <p class="sidebar-header-items__title" role="heading" aria-level="1" aria-label="Site Navigation">
        Site Navigation
    </p>
    <ul id="navbar-main-elements" class="navbar-nav">
        
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../notations/mathematical_notations.html">
                        Mathematical Notations
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../notations/machine_learning_notations.html">
                        Machine Learning Notations
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../notations/deep_learning_notations.html">
                        Deep Learning Notations
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../probability_theory/01_mathematical_preliminaries/intro.html">
                        Chapter 1. Mathematical Preliminaries
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../probability_theory/02_probability/intro.html">
                        Chapter 2. Probability
                      </a>
                    </li>
                
            <div class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                    More
                </button>
                <div class="dropdown-menu">
                    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../probability_theory/03_discrete_random_variables/intro.html">
                        Chapter 3. Discrete Random Variables
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../probability_theory/04_continuous_random_variables/intro.html">
                        Chapter 4. Continuous Random Variables
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../probability_theory/05_joint_distributions/intro.html">
                        Chapter 5. Joint Distributions
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../probability_theory/06_sample_statistics/intro.html">
                        Chapter 6. Sample Statistics
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../probability_theory/08_estimation_theory/intro.html">
                        Estimation Theory
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../optimization/gradient_descent/intro.html">
                        Gradient Descent
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../framework.html">
                        The Machine Learning Framework
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../fundamentals/intro.html">
                        Fundamentals
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../linear_models/intro.html">
                        Linear Models
                      </a>
                    </li>
                

                    <li class="nav-item current active">
                      <a class="nav-link nav-internal" href="../intro.html">
                        Generative
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../model_selection_and_evaluation/intro.html">
                        Model Selection and Evaluation
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../trees/intro.html">
                        Trees, Forests, Bagging and Boosting
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../decomposition/intro.html">
                        Dimensionality Reduction
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../neighbours/intro.html">
                        Neighbourhood
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../mixtures/intro.html">
                        Mixture Models
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../clustering/intro.html">
                        Clustering
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../deep_learning/natural_language_processing/intro.html">
                        Natural Language Processing (NLP)
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../references_resources_roadmap/bibliography.html">
                        Bibliography
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../references_resources_roadmap/resources.html">
                        Resources
                      </a>
                    </li>
                
                </div>
            </div>
            
    </ul>
</nav>
      </div>
      
    </div>

    <div id="navbar-end">
      
        <div class="navbar-end-item navbar-persistent--container">
          
<button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-toggle="tooltip">
  <i class="fa-solid fa-magnifying-glass"></i>
</button>
        </div>
      
      
      <div class="navbar-end-item">
        <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
</button>
      </div>
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
      </ul>
      </div>
      
    </div>
  </div>


  
  
    <div class="navbar-persistent--mobile">
<button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-toggle="tooltip">
  <i class="fa-solid fa-magnifying-glass"></i>
</button>
    </div>
  

  
  <label class="sidebar-toggle secondary-toggle" for="__secondary">
      <span class="fa-solid fa-outdent"></span>
  </label>
  

</div>
  </nav>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        
  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
      
      <div class="navbar-center-item">
        <nav class="navbar-nav">
    <p class="sidebar-header-items__title" role="heading" aria-level="1" aria-label="Site Navigation">
        Site Navigation
    </p>
    <ul id="navbar-main-elements" class="navbar-nav">
        
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../notations/mathematical_notations.html">
                        Mathematical Notations
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../notations/machine_learning_notations.html">
                        Machine Learning Notations
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../notations/deep_learning_notations.html">
                        Deep Learning Notations
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../probability_theory/01_mathematical_preliminaries/intro.html">
                        Chapter 1. Mathematical Preliminaries
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../probability_theory/02_probability/intro.html">
                        Chapter 2. Probability
                      </a>
                    </li>
                
            <div class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                    More
                </button>
                <div class="dropdown-menu">
                    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../probability_theory/03_discrete_random_variables/intro.html">
                        Chapter 3. Discrete Random Variables
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../probability_theory/04_continuous_random_variables/intro.html">
                        Chapter 4. Continuous Random Variables
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../probability_theory/05_joint_distributions/intro.html">
                        Chapter 5. Joint Distributions
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../probability_theory/06_sample_statistics/intro.html">
                        Chapter 6. Sample Statistics
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../probability_theory/08_estimation_theory/intro.html">
                        Estimation Theory
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../optimization/gradient_descent/intro.html">
                        Gradient Descent
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../framework.html">
                        The Machine Learning Framework
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../fundamentals/intro.html">
                        Fundamentals
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../linear_models/intro.html">
                        Linear Models
                      </a>
                    </li>
                

                    <li class="nav-item current active">
                      <a class="nav-link nav-internal" href="../intro.html">
                        Generative
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../model_selection_and_evaluation/intro.html">
                        Model Selection and Evaluation
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../trees/intro.html">
                        Trees, Forests, Bagging and Boosting
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../decomposition/intro.html">
                        Dimensionality Reduction
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../neighbours/intro.html">
                        Neighbourhood
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../mixtures/intro.html">
                        Mixture Models
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../clustering/intro.html">
                        Clustering
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../deep_learning/natural_language_processing/intro.html">
                        Natural Language Processing (NLP)
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../references_resources_roadmap/bibliography.html">
                        Bibliography
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../references_resources_roadmap/resources.html">
                        Resources
                      </a>
                    </li>
                
                </div>
            </div>
            
    </ul>
</nav>
      </div>
      
      </div>
    

    
    
    <div class="sidebar-header-items__end">
      
      <div class="navbar-end-item">
        <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
</button>
      </div>
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
      </ul>
      </div>
      
    </div>
    
  </div>

  
  <div class="sidebar-start-items sidebar-primary__section">
    <div class="sidebar-start-items__item">
  


<a class="navbar-brand logo" href="../../../intro.html">

  
  
  
  
  
  
  

  
    <img src="../../../_static/logo.png" class="logo__image only-light" alt="Logo image">
    <img src="../../../_static/logo.png" class="logo__image only-dark" alt="Logo image">
  
  
</a>
    </div>
    <div class="sidebar-start-items__item">
<form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false">
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
    </div>
    <div class="sidebar-start-items__item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../../intro.html">
                    Introduction
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Notations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../notations/mathematical_notations.html">Mathematical Notations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notations/machine_learning_notations.html">Machine Learning Notations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notations/deep_learning_notations.html">Deep Learning Notations</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Probability Theory</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../probability_theory/01_mathematical_preliminaries/intro.html">Chapter 1. Mathematical Preliminaries</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/01_mathematical_preliminaries/01_combinatorics.html">Permutations and Combinations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/01_mathematical_preliminaries/02_calculus.html">Calculus</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/01_mathematical_preliminaries/contours.html">Contour Maps</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/01_mathematical_preliminaries/exercises.html">Exercises</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../probability_theory/02_probability/intro.html">Chapter 2. Probability</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/02_probability/0202_probability_space.html">Probability Space</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/02_probability/0203_probability_axioms.html">Probability Axioms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/02_probability/0204_conditional_probability.html">Conditional Probability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/02_probability/0205_independence.html">Independence</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/02_probability/0206_bayes_theorem.html">Bayeâ€™s Theorem and the Law of Total Probability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/02_probability/summary.html">Summary</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../probability_theory/03_discrete_random_variables/intro.html">Chapter 3. Discrete Random Variables</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/03_discrete_random_variables/0301_random_variables.html">Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/03_discrete_random_variables/0302_discrete_random_variables.html">Discrete Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/03_discrete_random_variables/0303_probability_mass_function.html">Probability Mass Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/03_discrete_random_variables/0304_cumulative_distribution_function.html">Cumulative Distribution Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/03_discrete_random_variables/0305_expectation.html">Expectation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/03_discrete_random_variables/0306_moments_and_variance.html">Moments and Variance</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../probability_theory/03_discrete_random_variables/uniform/intro.html">Discrete Uniform Distribution</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/03_discrete_random_variables/uniform/0307_discrete_uniform_distribution_concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/03_discrete_random_variables/uniform/0307_discrete_uniform_distribution_application.html">Application</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../probability_theory/03_discrete_random_variables/bernoulli/intro.html">Bernoulli Distribution</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/03_discrete_random_variables/bernoulli/0308_bernoulli_distribution_concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/03_discrete_random_variables/bernoulli/0308_bernoulli_distribution_application.html">Application</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/03_discrete_random_variables/iid.html">Independent and Identically Distributed (IID)</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../probability_theory/03_discrete_random_variables/binomial/intro.html">Binomial Distribution</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/03_discrete_random_variables/binomial/0309_binomial_distribution_concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/03_discrete_random_variables/binomial/0309_binomial_distribution_implementation.html">Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/03_discrete_random_variables/binomial/0309_binomial_distribution_application.html">Real World Examples</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../probability_theory/03_discrete_random_variables/geometric/intro.html">Geometric Distribution</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/03_discrete_random_variables/geometric/0310_geometric_distribution_concept.html">Concept</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../probability_theory/03_discrete_random_variables/poisson/intro.html">Poisson Distribution</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/03_discrete_random_variables/poisson/0311_poisson_distribution_concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/03_discrete_random_variables/poisson/0311_poisson_distribution_implementation.html">Implementation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/03_discrete_random_variables/summary.html">Important</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/03_discrete_random_variables/exercises.html">Exercises</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../probability_theory/04_continuous_random_variables/intro.html">Chapter 4. Continuous Random Variables</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/04_continuous_random_variables/from_discrete_to_continuous.html">From Discrete to Continuous</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/04_continuous_random_variables/0401_continuous_random_variables.html">Continuous Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/04_continuous_random_variables/0402_probability_density_function.html">Probability Density Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/04_continuous_random_variables/0403_expectation.html">Expectation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/04_continuous_random_variables/0404_moments_and_variance.html">Moments and Variance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/04_continuous_random_variables/0405_cumulative_distribution_function.html">Cumulative Distribution Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/04_continuous_random_variables/0406_mean_median_mode.html">Mean, Median and Mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/04_continuous_random_variables/0407_continuous_uniform_distribution.html">Continuous Uniform Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/04_continuous_random_variables/0408_exponential_distribution.html">Exponential Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/04_continuous_random_variables/0409_gaussian_distribution.html">Gaussian Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/04_continuous_random_variables/0410_skewness_and_kurtosis.html">Skewness and Kurtosis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/04_continuous_random_variables/0411_convolve_and_sum_of_random_variables.html">Convolution and Sum of Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/04_continuous_random_variables/0412_functions_of_random_variables.html">Functions of Random Variables</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../probability_theory/05_joint_distributions/intro.html">Chapter 5. Joint Distributions</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_theory/05_joint_distributions/from_single_variable_to_joint_distributions.html">From Single Variable to Joint Distributions</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../probability_theory/05_joint_distributions/0501_joint_pmf_pdf/intro.html">Joint PMF and PDF</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/05_joint_distributions/0501_joint_pmf_pdf/concept.html">Concept</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../probability_theory/05_joint_distributions/0502_joint_expectation_and_correlation/intro.html">Joint Expectation and Correlation</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/05_joint_distributions/0502_joint_expectation_and_correlation/concept.html">Concept</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../probability_theory/05_joint_distributions/0503_conditional_pmf_pdf/intro.html">Conditional PMF and PDF</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/05_joint_distributions/0503_conditional_pmf_pdf/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/05_joint_distributions/0503_conditional_pmf_pdf/application.html">Application</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../probability_theory/05_joint_distributions/0504_conditional_expectation_variance/intro.html">Conditional Expectation and Variance</a><input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-14"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/05_joint_distributions/0504_conditional_expectation_variance/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/05_joint_distributions/0504_conditional_expectation_variance/exercises.html">Exercises</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../probability_theory/05_joint_distributions/0505_sum_of_random_variables/intro.html">Sum of Random Variables</a><input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-15"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/05_joint_distributions/0505_sum_of_random_variables/concept.html">Concept</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../probability_theory/05_joint_distributions/0506_random_vectors/intro.html">Random Vectors</a><input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-16"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/05_joint_distributions/0506_random_vectors/concept.html">Concept</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../probability_theory/05_joint_distributions/0507_multivariate_gaussian/intro.html">Multivariate Gaussian Distribution</a><input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-17"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/05_joint_distributions/0507_multivariate_gaussian/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/05_joint_distributions/0507_multivariate_gaussian/application_transformation.html">Application: Plots and Transformations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/05_joint_distributions/0507_multivariate_gaussian/psd.html">Covariance Matrix is Positive Semi-Definite</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/05_joint_distributions/0507_multivariate_gaussian/eigendecomposition.html">Eigendecomposition and Covariance Matrix</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/05_joint_distributions/0507_multivariate_gaussian/geometry_of_multivariate_gaussian.html">The Geometry of Multivariate Gaussians</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../probability_theory/06_sample_statistics/intro.html">Chapter 6. Sample Statistics</a><input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-18"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../probability_theory/06_sample_statistics/0601_moment_generating_and_characteristic_functions/intro.html">Moment Generating and Characteristic Functions</a><input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-19"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/06_sample_statistics/0601_moment_generating_and_characteristic_functions/moment_generating_function.html">Moment Generating Function</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/06_sample_statistics/0601_moment_generating_and_characteristic_functions/moment_generating_function_application_sum_of_rv.html">Application: Moment Generating Function and the Sum of Random Variables</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/06_sample_statistics/0601_moment_generating_and_characteristic_functions/characteristic_function.html">Characteristic Function</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../probability_theory/06_sample_statistics/0602_probability_inequalities/intro.html">Probability Inequalities</a><input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-20"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/06_sample_statistics/0602_probability_inequalities/concept.html">Probability Inequalities</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/06_sample_statistics/0602_probability_inequalities/application.html">Application: Learning Theory</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../probability_theory/06_sample_statistics/0603_law_of_large_numbers/intro.html">Law of Large Numbers</a><input class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-21"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/06_sample_statistics/0603_law_of_large_numbers/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/06_sample_statistics/0603_law_of_large_numbers/convergence.html">Convergence of Sample Average</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/06_sample_statistics/0603_law_of_large_numbers/application.html">Application: Learning Theory</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../probability_theory/08_estimation_theory/intro.html">Estimation Theory</a><input class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-22"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../probability_theory/08_estimation_theory/maximum_likelihood_estimation/intro.html">Maximum Likelihood Estimation</a><input class="toctree-checkbox" id="toctree-checkbox-23" name="toctree-checkbox-23" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-23"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_theory/08_estimation_theory/maximum_likelihood_estimation/concept.html">Concept</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Optimization</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../optimization/gradient_descent/intro.html">Gradient Descent</a><input class="toctree-checkbox" id="toctree-checkbox-24" name="toctree-checkbox-24" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-24"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../optimization/gradient_descent/concept.html">Gradient Descent Concept</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../optimization/gradient_descent/implementation.html">Gradient Descent Construction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../optimization/gradient_descent/application.html">Application: Gradient Descent</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Machine Learning</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../framework.html">The Machine Learning Framework</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../fundamentals/intro.html">Fundamentals</a><input class="toctree-checkbox" id="toctree-checkbox-25" name="toctree-checkbox-25" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-25"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../fundamentals/criterions/intro.html">Loss</a><input class="toctree-checkbox" id="toctree-checkbox-26" name="toctree-checkbox-26" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-26"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../fundamentals/criterions/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../fundamentals/criterions/cross_entropy_loss.html">Cross Entropy Loss</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../fundamentals/criterions/focal_loss.html">Focal Loss</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../fundamentals/empirical_risk_minimization/intro.html">Empirical Risk Minimization</a><input class="toctree-checkbox" id="toctree-checkbox-27" name="toctree-checkbox-27" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-27"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../fundamentals/empirical_risk_minimization/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../fundamentals/empirical_risk_minimization/bayes_optimal_classifier.html">Bayes Optimal Classifier</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../fundamentals/learning_theory/intro.html">Is the Learning Problem Solvable?</a><input class="toctree-checkbox" id="toctree-checkbox-28" name="toctree-checkbox-28" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-28"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../fundamentals/learning_theory/concept.html">Concept</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../fundamentals/bias_and_variance/intro.html">Bias and Variance Tradeoff</a><input class="toctree-checkbox" id="toctree-checkbox-29" name="toctree-checkbox-29" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-29"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../fundamentals/bias_and_variance/concept.html">Bias-Variance Tradeoff Concept</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../fundamentals/decision_boundary/intro.html">Decision Boundary</a><input class="toctree-checkbox" id="toctree-checkbox-30" name="toctree-checkbox-30" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-30"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../fundamentals/decision_boundary/concept.html">Concept</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../fundamentals/voronoi_region/intro.html">Voronoi Region</a><input class="toctree-checkbox" id="toctree-checkbox-31" name="toctree-checkbox-31" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-31"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../fundamentals/voronoi_region/concept.html">Concept</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../linear_models/intro.html">Linear Models</a><input class="toctree-checkbox" id="toctree-checkbox-32" name="toctree-checkbox-32" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-32"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../linear_models/linear_regression/intro.html">Linear Regression</a><input class="toctree-checkbox" id="toctree-checkbox-33" name="toctree-checkbox-33" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-33"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../linear_models/linear_regression/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../linear_models/linear_regression/implementation.html">Implementation</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../linear_models/logistic_regression/intro.html">Logistic Regression</a><input class="toctree-checkbox" id="toctree-checkbox-34" name="toctree-checkbox-34" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-34"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../linear_models/logistic_regression/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../linear_models/logistic_regression/implementation.html">Implementation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../linear_models/generalized_linear_models/intro.html">Generalized Linear Models</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../intro.html">Generative</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-35" name="toctree-checkbox-35" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-35"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2 current active has-children"><a class="reference internal" href="intro.html">Naive Bayes</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-36" name="toctree-checkbox-36" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-36"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l3 current active"><a class="current reference internal" href="#">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="implementation.html">Naives Bayes Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="example_penguins.html">Naive Bayes Application: Penguins</a></li>
<li class="toctree-l3"><a class="reference internal" href="application_mnist.html">Naive Bayes Application (MNIST)</a></li>

</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../model_selection_and_evaluation/intro.html">Model Selection and Evaluation</a><input class="toctree-checkbox" id="toctree-checkbox-37" name="toctree-checkbox-37" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-37"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../model_selection_and_evaluation/metrics/intro.html">Metrics and Scoring Rules</a><input class="toctree-checkbox" id="toctree-checkbox-38" name="toctree-checkbox-38" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-38"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../model_selection_and_evaluation/metrics/classification/intro.html">Classification Metrics</a><input class="toctree-checkbox" id="toctree-checkbox-39" name="toctree-checkbox-39" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-39"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../model_selection_and_evaluation/metrics/classification/accuracy.html">Accuracy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../model_selection_and_evaluation/metrics/classification/precision_recall_f1.html">Precision, Recall and F1 Score</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../model_selection_and_evaluation/metrics/classification/brier_score.html">Brier Score</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../model_selection_and_evaluation/metrics/regression/intro.html">Regression Metrics</a><input class="toctree-checkbox" id="toctree-checkbox-40" name="toctree-checkbox-40" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-40"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../model_selection_and_evaluation/metrics/regression/mae.html">Mean Absolute Error</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../model_selection_and_evaluation/metrics/regression/rmse.html">(Root) Mean Squared Error</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../model_selection_and_evaluation/metrics/regression/mape.html">Mean Absolute Percentage Error</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../trees/intro.html">Trees, Forests, Bagging and Boosting</a><input class="toctree-checkbox" id="toctree-checkbox-41" name="toctree-checkbox-41" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-41"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../trees/decision_trees/intro.html">Decision Trees</a><input class="toctree-checkbox" id="toctree-checkbox-42" name="toctree-checkbox-42" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-42"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../trees/decision_trees/concept.html">Braindump</a></li>





</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../trees/ensemble_learning/intro.html">Ensemble Learning</a><input class="toctree-checkbox" id="toctree-checkbox-43" name="toctree-checkbox-43" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-43"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../trees/ensemble_learning/bagging/intro.html">Bagging</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../trees/ensemble_learning/random_forests/intro.html">Random Forests</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../trees/ensemble_learning/boosting/intro.html">Boosting</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../decomposition/intro.html">Dimensionality Reduction</a><input class="toctree-checkbox" id="toctree-checkbox-44" name="toctree-checkbox-44" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-44"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../decomposition/pca/intro.html">Principal Component Analysis</a><input class="toctree-checkbox" id="toctree-checkbox-45" name="toctree-checkbox-45" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-45"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../decomposition/pca/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../decomposition/pca/implementation.html">PCA</a></li>

<li class="toctree-l3"><a class="reference internal" href="../../decomposition/pca/eigenface.html">Eigenface</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../neighbours/intro.html">Neighbourhood</a><input class="toctree-checkbox" id="toctree-checkbox-46" name="toctree-checkbox-46" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-46"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../neighbours/k_nearest_neighbours/intro.html">K-Nearest Neighbours</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../mixtures/intro.html">Mixture Models</a><input class="toctree-checkbox" id="toctree-checkbox-47" name="toctree-checkbox-47" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-47"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../mixtures/gmm/intro.html">Gaussian Mixture Models</a><input class="toctree-checkbox" id="toctree-checkbox-48" name="toctree-checkbox-48" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-48"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../mixtures/gmm/concept.html">Concept</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../clustering/intro.html">Clustering</a><input class="toctree-checkbox" id="toctree-checkbox-49" name="toctree-checkbox-49" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-49"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../clustering/kmeans/intro.html">K-Means</a><input class="toctree-checkbox" id="toctree-checkbox-50" name="toctree-checkbox-50" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-50"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../clustering/kmeans/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../clustering/kmeans/implementation.html">Implementation: K-Means (Lloyd)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../clustering/kmeans/image_segmentation.html">Application: Image Compression and Segmentation</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Deep Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../deep_learning/natural_language_processing/intro.html">Natural Language Processing (NLP)</a><input class="toctree-checkbox" id="toctree-checkbox-51" name="toctree-checkbox-51" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-51"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../deep_learning/natural_language_processing/vector_semantics_and_embeddings/intro.html">Vector Semantics and Embeddings</a><input class="toctree-checkbox" id="toctree-checkbox-52" name="toctree-checkbox-52" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-52"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../deep_learning/natural_language_processing/vector_semantics_and_embeddings/words_and_vectors/intro.html">Words and Vectors</a><input class="toctree-checkbox" id="toctree-checkbox-53" name="toctree-checkbox-53" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-53"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../deep_learning/natural_language_processing/vector_semantics_and_embeddings/words_and_vectors/concept.html">Concept</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../deep_learning/natural_language_processing/vector_semantics_and_embeddings/cosine_similarity/intro.html">Cosine Similarity and Notion of Closeness</a><input class="toctree-checkbox" id="toctree-checkbox-54" name="toctree-checkbox-54" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-54"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../deep_learning/natural_language_processing/vector_semantics_and_embeddings/cosine_similarity/concept.html">Concept</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../deep_learning/natural_language_processing/vector_semantics_and_embeddings/cosine_similarity/implementation.html">Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../deep_learning/natural_language_processing/vector_semantics_and_embeddings/cosine_similarity/word_similarity.html">Application: Word Similarity</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../deep_learning/natural_language_processing/vector_semantics_and_embeddings/tf_idf/intro.html">Term Frequency-Inverse Document Frequency (TF-IDF)</a><input class="toctree-checkbox" id="toctree-checkbox-55" name="toctree-checkbox-55" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-55"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../deep_learning/natural_language_processing/vector_semantics_and_embeddings/tf_idf/concept.html">Concept</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../deep_learning/natural_language_processing/vector_semantics_and_embeddings/tf_idf/implementation.html">Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../deep_learning/natural_language_processing/vector_semantics_and_embeddings/tf_idf/application.html">Movie Recommender System</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">References, Resources and Roadmap</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../references_resources_roadmap/bibliography.html">Bibliography</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../references_resources_roadmap/resources.html">Resources</a></li>
</ul>

    </div>
</nav>
    </div>
  </div>
  

  
  <div class="sidebar-end-items sidebar-primary__section">
    <div class="sidebar-end-items__item">
    </div>
  </div>

  
  <div id="rtd-footer-container"></div>

      </div>
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

        <div class="bd-content">
          <div class="bd-article-container">
            
            <div class="bd-header-article">
                



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        <label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" data-toggle="tooltip" data-placement="right" title="Toggle primary sidebar">
            <span class="fa-solid fa-bars"></span>
        </label>
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="btn btn-sm"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<div class="dropdown dropdown-repository-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      <li><a href="https://github.com/gao-hongnan/gaohn-galaxy" target="_blank"
   class="btn btn-sm dropdown-item"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">repository</span>
</a>
</a>
      
      <li><a href="https://github.com/gao-hongnan/gaohn-galaxy/issues/new?title=Issue%20on%20page%20%2Fmachine_learning/generative/naive_bayes/concept.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm dropdown-item"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">open issue</span>
</a>
</a>
      
  </ul>
</div>



<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      <li><a href="../../../_sources/machine_learning/generative/naive_bayes/concept.md" target="_blank"
   class="btn btn-sm dropdown-item"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</a>
      
      <li>
<button onclick="printPdf(this)"
  class="btn btn-sm dropdown-item"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</a>
      
  </ul>
</div>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary" data-toggle="tooltip" data-placement="left" title="Toggle secondary sidebar">
            <span class="fa-solid fa-list"></span>
        </label>
    </div>
</div>
            </div>
            
            

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Concept</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#notations">
   Notations
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#discriminative-vs-generative">
     Discriminative vs Generative
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#naive-bayes-setup">
   Naive Bayes Setup
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#inference-prediction">
   Inference/Prediction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-naive-bayes-form">
   The Naive Bayes Form
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#simple-form">
     Simple Form
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#extended-form">
     Extended form
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-naive-bayes-assumptions">
   The Naive Bayes Assumptions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#independent-and-identically-distributed-i-i-d">
     Independent and Identically Distributed (i.i.d.)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#conditional-independence">
     Conditional Independence
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#parameter-vector">
   Parameter Vector
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#inductive-bias-distribution-assumptions">
   Inductive Bias (Distribution Assumptions)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#targets-categorical-distribution">
     Targets (Categorical Distribution)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#discrete-features-categorical-distribution">
     Discrete Features (Categorical Distribution)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#continuous-features-gaussian-distribution">
     Continuous Features (Gaussian Distribution)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mixed-features-discrete-and-continuous">
     Mixed Features (Discrete and Continuous)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-fitting">
   Model Fitting
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#fitting-algorithm">
     Fitting Algorithm
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#maximum-likelihood-estimation">
     Maximum Likelihood Estimation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#estimating-priors">
     Estimating Priors
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#maximum-likelihood-estimation-for-priors-categorical-distribution">
     Maximum Likelihood Estimation for Priors (Categorical Distribution)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#estimating-likelihood-gaussian-version">
     Estimating Likelihood (Gaussian Version)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#maximum-likelihood-estimate-for-likelihood-continuous-feature-parameters">
       Maximum Likelihood Estimate for Likelihood (Continuous Feature Parameters)
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#decision-boundary">
   Decision Boundary
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#connection-with-logistic-regression">
     Connection with Logistic Regression
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#time-and-space-complexity">
   Time and Space Complexity
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>

            <article class="bd-article" role="main">
              
  <section class="tex2jax_ignore mathjax_ignore" id="concept">
<h1>Concept<a class="headerlink" href="#concept" title="Permalink to this heading">#</a></h1>
<section id="notations">
<h2>Notations<a class="headerlink" href="#notations" title="Permalink to this heading">#</a></h2>
<div class="proof definition admonition" id="underlying-distributions">
<p class="admonition-title"><span class="caption-number">Definition 124 </span> (Underlying Distributions)</p>
<section class="definition-content" id="proof-content">
<ul>
<li><p><span class="math notranslate nohighlight">\(\mathcal{X}\)</span>: Input space consists of all possible inputs <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathcal{X}\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{Y}\)</span>: Label space = <span class="math notranslate nohighlight">\(\{1, 2, \ldots, K\}\)</span> where <span class="math notranslate nohighlight">\(K\)</span> is the number of classes.</p></li>
<li><p>The mapping between <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> and <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> is given by <span class="math notranslate nohighlight">\(c: \mathcal{X} \rightarrow \mathcal{Y}\)</span> where <span class="math notranslate nohighlight">\(c\)</span> is called <em>concept</em> according to the PAC learning theory.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{D}\)</span>: The fixed but unknown distribution of the data. Usually, this refers
to the joint distribution of the input and the label,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  \mathcal{D} &amp;= \mathbb{P}(\mathcal{X}, \mathcal{Y} ; \boldsymbol{\theta}) \\
  &amp;= \mathbb{P}_{\{\mathcal{X}, \mathcal{Y} ; \boldsymbol{\theta}\}}(\mathbf{x}, y)
  \end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathcal{X}\)</span> and <span class="math notranslate nohighlight">\(y \in \mathcal{Y}\)</span>, and <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> is the
parameter vector of the distribution <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>.</p>
</li>
</ul>
</section>
</div><div class="proof definition admonition" id="dataset-definition">
<p class="admonition-title"><span class="caption-number">Definition 125 </span> (Dataset)</p>
<section class="definition-content" id="proof-content">
<p>Now, consider a dataset <span class="math notranslate nohighlight">\(\mathcal{D}_{\{\mathbf{x}, y\}}\)</span> consisting of <span class="math notranslate nohighlight">\(N\)</span> samples (observations) and <span class="math notranslate nohighlight">\(D\)</span> predictors (features) drawn <strong>jointly</strong> and <strong>indepedently and identically distributed</strong> (i.i.d.) from <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>. Note we will refer to the dataset <span class="math notranslate nohighlight">\(\mathcal{D}_{\{\mathbf{x}, y\}}\)</span> with the same notation as the underlying distribution <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> from now on.</p>
<ul>
<li><p>The training dataset <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> can also be represented compactly as a set:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{align*}
    \mathcal{D} \overset{\mathbf{def}}{=} \mathcal{D}_{\{\mathbf{x}, y\}} &amp;= \left\{\mathbf{x}^{(n)}, y^{(n)}\right\}_{n=1}^N \\
    &amp;= \left\{\left(\mathbf{x}^{(1)}, y^{(1)}\right), \left(\mathbf{x}^{(2)}, y^{(2)}\right), \cdots, \left(\mathbf{x}^{(N)}, y^{(N)}\right)\right\} \\
    &amp;= \left\{\mathbf{X}, \mathbf{y}\right\}
    \end{align*}
    \end{split}\]</div>
<p>where we often subscript <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and <span class="math notranslate nohighlight">\(y\)</span> with <span class="math notranslate nohighlight">\(n\)</span> to denote the <span class="math notranslate nohighlight">\(n\)</span>-th sample from the dataset, i.e.
<span class="math notranslate nohighlight">\(\mathbf{x}^{(n)}\)</span> and <span class="math notranslate nohighlight">\(y^{(n)}\)</span>. Most of the times, <span class="math notranslate nohighlight">\(\mathbf{x}^{(n)}\)</span> is bolded since
it represents a vector of <span class="math notranslate nohighlight">\(D\)</span> number of features, while <span class="math notranslate nohighlight">\(y^{(n)}\)</span> is not bolded since it is a scalar, though
it is not uncommon for <span class="math notranslate nohighlight">\(y^{(n)}\)</span> to be bolded as well if you represent it with K-dim one-hot vector.</p>
</li>
<li><p>For the n-th sample <span class="math notranslate nohighlight">\(\mathbf{x}^{(n)}\)</span>, we often denote the <span class="math notranslate nohighlight">\(d\)</span>-th feature as <span class="math notranslate nohighlight">\(x_d^{(n)}\)</span> and the representation of <span class="math notranslate nohighlight">\(\mathbf{x}^{(n)}\)</span> as a vector as:</p>
<div class="math notranslate nohighlight">
\[
  \mathbf{x}^{(n)} \in \mathbb{R}^{D} = \begin{bmatrix} x_1^{(n)} &amp; x_2^{(n)} &amp; \cdots &amp; x_D^{(n)} \end{bmatrix}_{D \times 1}
  \]</div>
<p>is a sample of size <span class="math notranslate nohighlight">\(D\)</span>, drawn (jointly with <span class="math notranslate nohighlight">\(y\)</span>) <span class="math notranslate nohighlight">\(\textbf{i.i.d.}\)</span> from <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>.</p>
</li>
<li><p>We often add an extra feature <span class="math notranslate nohighlight">\(x_0^{(n)} = 1\)</span> to <span class="math notranslate nohighlight">\(\mathbf{x}^{(n)}\)</span> to represent the bias term.
i.e.</p>
<div class="math notranslate nohighlight">
\[
  \mathbf{x}^{(n)} \in \mathbb{R}^{D+1} = \begin{bmatrix} x_0^{(n)} &amp; x_1^{(n)} &amp; x_2^{(n)} &amp; \cdots &amp; x_D^{(n)} \end{bmatrix}_{(D+1) \times 1}
  \]</div>
</li>
<li><p>For the n-th sampleâ€™s label <span class="math notranslate nohighlight">\(y^{(n)} \overset{\mathbf{def}}{=} c(\mathbf{x}^{(n)})\)</span>, if we were to represent it as K-dim one-hot vector, we would have:</p>
<div class="math notranslate nohighlight">
\[
  y^{(n)} \in \mathbb{R}^{K} = \begin{bmatrix} 0 &amp; 0 &amp; \cdots &amp; 1 &amp; \cdots &amp; 0 \end{bmatrix}_{K \times 1}
  \]</div>
<p>where the <span class="math notranslate nohighlight">\(1\)</span> is at the <span class="math notranslate nohighlight">\(k\)</span>-th position, and <span class="math notranslate nohighlight">\(k\)</span> is the class label of the n-th sample.</p>
</li>
<li><p>Everything defined above is for <strong>one single sample/data point</strong>, to represent it as a matrix, we can define
a design matrix <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> and a label vector <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> as follows,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  \begin{aligned}
  \mathbf{X} \in \mathbb{R}^{N \times D} &amp;= \begin{bmatrix} \mathbf{x}^{(1)} \\ \mathbf{x}^{(2)} \\ \vdots \\ \mathbf{x}^{(N)} \end{bmatrix} = \begin{bmatrix} x_1^{(1)} &amp; x_2^{(1)} &amp; \cdots &amp; x_D^{(1)} \\ x_1^{(2)} &amp; x_2^{(2)} &amp; \cdots &amp; x_D^{(2)} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ x_1^{(N)} &amp; x_2^{(N)} &amp; \cdots &amp; x_D^{(N)} \end{bmatrix}_{N \times D} \\
  \end{aligned}
  \end{split}\]</div>
<p>as the matrix of all samples. Note that each row is a sample and each column is a feature. We can append a column of 1â€™s to the first column of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> to represent the bias term.</p>
<p><strong>In this section, we also talk about random vectors <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> so we will replace the design matrix <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> with <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> to avoid confusion.</strong></p>
<p>Subsequently, for the label vector <span class="math notranslate nohighlight">\(\mathbf{y}\)</span>, we can define it as follows,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  \begin{aligned}
  \mathbf{y} \in \mathbb{R}^{N} = \begin{bmatrix} y^{(1)} \\ y^{(2)} \\ \vdots \\ y^{(N)} \end{bmatrix}
  \end{aligned}
  \end{split}\]</div>
</li>
</ul>
</section>
</div><div class="proof example admonition" id="joint-distribution-example">
<p class="admonition-title"><span class="caption-number">Example 30 </span> (Joint Distribution Example)</p>
<section class="example-content" id="proof-content">
<p>For example, if the number of features, <span class="math notranslate nohighlight">\(D = 2\)</span>, then letâ€™s say</p>
<div class="math notranslate nohighlight">
\[
\mathbf{X}^{(n)} = \begin{bmatrix} X^{(n)}_1 &amp; X^{(n)}_2 \end{bmatrix} \in \mathbb{R}^2
\]</div>
<p>consists of two Gaussian random variables,
with <span class="math notranslate nohighlight">\(\mu_1\)</span> and <span class="math notranslate nohighlight">\(\mu_2\)</span> being the mean of the two distributions,
and <span class="math notranslate nohighlight">\(\sigma_1\)</span> and <span class="math notranslate nohighlight">\(\sigma_2\)</span> being the variance of the two distributions;
furthermore, <span class="math notranslate nohighlight">\(Y^{(n)}\)</span> is a Bernoulli random variable with parameter <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span>, then we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\boldsymbol{\theta} &amp;= \begin{bmatrix} \mu_1 &amp; \sigma_1 &amp; \mu_2 &amp; \sigma_2 &amp; \boldsymbol{\pi}\end{bmatrix} \\
&amp;= \begin{bmatrix} \boldsymbol{\mu} &amp; \boldsymbol{\sigma} &amp; \boldsymbol{\pi} \end{bmatrix}
\end{aligned}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{\mu} = \begin{bmatrix} \mu_1 &amp; \mu_2 \end{bmatrix}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{\sigma} = \begin{bmatrix} \sigma_1 &amp; \sigma_2 \end{bmatrix}\)</span>.</p>
</section>
</div><div class="proof remark admonition" id="some-remarks">
<p class="admonition-title"><span class="caption-number">Remark 37 </span> (Some remarks)</p>
<section class="remark-content" id="proof-content">
<ul class="simple">
<li><p>From now on, we will refer the realization of <span class="math notranslate nohighlight">\(Y\)</span> as <span class="math notranslate nohighlight">\(k\)</span> instead.</p></li>
<li><p>For some sections, when I mention <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>, it means the random vector which resides in the
<span class="math notranslate nohighlight">\(D\)</span>-dimensional space, not the design matrix. This also means that this random vector refers
to a single sample, not the entire dataset.</p></li>
</ul>
</section>
</div><div class="proof definition admonition" id="joint-and-conditional-probability">
<p class="admonition-title"><span class="caption-number">Definition 126 </span> (Joint and Conditional Probability)</p>
<section class="definition-content" id="proof-content">
<p>We are often interested in finding the probability of a label given a sample,</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
\mathbb{P}(Y = k \mid \mathbf{X} = \mathbf{x}) &amp;= \mathbb{P}(Y = k \mid \mathbf{X} = \left(x_1, x_2, \ldots, x_D\right))
\end{aligned}
\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[
\mathbf{X} \in \mathbb{R}^{D} = \begin{bmatrix} X_1 &amp; X_2 &amp; \cdots &amp; X_D \end{bmatrix}
\]</div>
<p>is a random vector and its realizations,</p>
<div class="math notranslate nohighlight">
\[
\mathbf{x} = \begin{bmatrix} x_1 &amp; x_2 &amp; \cdots &amp; x_D \end{bmatrix}
\]</div>
<p>and therefore, <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> can be characterized by an <span class="math notranslate nohighlight">\(D\)</span>-dimensional PDF</p>
<div class="math notranslate nohighlight">
\[
f_{\mathbf{X}}(\mathbf{x}) = f_{X_1, X_2, \ldots, X_D}(x_1, x_2, \ldots, x_D ; \boldsymbol{\theta})
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
Y \in \mathbb{Z} \quad \text{and} \quad k \in \mathbb{Z}
\]</div>
<p>is a discrete random variable (in our case classification) and its realization respectively, and therefore, <span class="math notranslate nohighlight">\(Y\)</span> can be characterized by a discrete PDF (PMF)</p>
<div class="math notranslate nohighlight">
\[
f_{Y}(k ; \boldsymbol{\pi}) \sim \text{Categorical}(\boldsymbol{\pi})
\]</div>
<p><strong>Note that we are talking about one single sample tuple <span class="math notranslate nohighlight">\(\left(\mathbf{x}, y\right)\)</span> here. I did not
index the sample tuple with <span class="math notranslate nohighlight">\(n\)</span> because this sample can be any sample in the unknown distribution <span class="math notranslate nohighlight">\(\mathbb{P}_{\mathcal{X}, \mathcal{Y}}(\mathbf{x}, y)\)</span>
and not only from our given dataset <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>.</strong></p>
</section>
</div><div class="proof definition admonition" id="likelihood">
<p class="admonition-title"><span class="caption-number">Definition 127 </span> (Likelihood)</p>
<section class="definition-content" id="proof-content">
<p>We denote the likelihood function as <span class="math notranslate nohighlight">\(\mathbb{P}(\mathbf{X} = \mathbf{x} \mid Y = k)\)</span>,
which is the probability of observing <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> given that the sample belongs to class <span class="math notranslate nohighlight">\(Y = k\)</span>.</p>
</section>
</div><div class="proof definition admonition" id="prior">
<p class="admonition-title"><span class="caption-number">Definition 128 </span> (Prior)</p>
<section class="definition-content" id="proof-content">
<p>We denote the prior probability of class <span class="math notranslate nohighlight">\(k\)</span> as <span class="math notranslate nohighlight">\(\mathbb{P}(Y = k)\)</span>, which usually
follows a discrete distribution such as the Categorical distribution.</p>
</section>
</div><div class="proof definition admonition" id="posterior">
<p class="admonition-title"><span class="caption-number">Definition 129 </span> (Posterior)</p>
<section class="definition-content" id="proof-content">
<p>We denote the posterior probability of class <span class="math notranslate nohighlight">\(k\)</span> given <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> as <span class="math notranslate nohighlight">\(\mathbb{P}(Y = k \mid \mathbf{X} = \mathbf{x})\)</span>.</p>
</section>
</div><div class="proof definition admonition" id="marginal-distribution-and-normalization-constant">
<p class="admonition-title"><span class="caption-number">Definition 130 </span> (Marginal Distribution and Normalization Constant)</p>
<section class="definition-content" id="proof-content">
<p>We denote the normalizing constant as <span class="math notranslate nohighlight">\(\mathbb{P}(\mathbf{X} = \mathbf{x}) = \sum_{k=1}^K \mathbb{P}(Y = k) \mathbb{P}(\mathbf{X} = \mathbf{x} \mid Y = k)\)</span>.</p>
</section>
</div><section id="discriminative-vs-generative">
<h3>Discriminative vs Generative<a class="headerlink" href="#discriminative-vs-generative" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Discriminative classifiers model the conditional distribution <span class="math notranslate nohighlight">\(\mathbb{P}(Y = k \mid \mathbf{X} =  \mathbf{x})\)</span>.
This means we are modelling the conditional distribution of the target <span class="math notranslate nohighlight">\(Y\)</span> given the input <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>.
This also means that we are using <strong>conditional maximum likelihood</strong> to estimate the parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>.</p></li>
<li><p>Generative classifiers model the conditional distribution <span class="math notranslate nohighlight">\(\mathbb{P}(\mathbf{X} = \mathbf{x} \mid Y = k)\)</span>.
This means we are modelling the conditional distribution of the input <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> given the target <span class="math notranslate nohighlight">\(Y\)</span>.
Then we can use Bayesâ€™ rule to compute the conditional distribution of the target <span class="math notranslate nohighlight">\(Y\)</span> given the input <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>.
This also means that we are using <strong>joint maximum likelihood</strong> to estimate the parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>.</p></li>
<li><p>Both the target <span class="math notranslate nohighlight">\(Y\)</span> and the input <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> are random variables in the generative model.
In the discriminative model, only the target <span class="math notranslate nohighlight">\(Y\)</span> is a random variable as the input <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is fixed (we do not need to estimate anything about the input <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>).</p></li>
<li><p>For example, Logistic Regression models the target <span class="math notranslate nohighlight">\(Y\)</span> as a function of predictorâ€™s <span class="math notranslate nohighlight">\(\mathbf{X} = \begin{bmatrix}X_1 \\ X_2 \\ \vdots \\X_D \end{bmatrix}\)</span>.</p></li>
<li><p>Naive bayes models both the target <span class="math notranslate nohighlight">\(Y\)</span> and the predictors <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> as a function of each other.
This means we are modelling the joint distribution of the target <span class="math notranslate nohighlight">\(Y\)</span> and the predictors <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>.</p></li>
</ul>
</section>
</section>
<section id="naive-bayes-setup">
<h2>Naive Bayes Setup<a class="headerlink" href="#naive-bayes-setup" title="Permalink to this heading">#</a></h2>
<p>Let</p>
<div class="math notranslate nohighlight">
\[
\mathcal{D} = \left \{ \left(\mathrm{X}^{(n)}, Y^{(n)} \right) \right \}_{n=1}^N = \left \{ \left(\mathrm{x}^{(n)}, y^{(n)} \right) \right \}_{n=1}^N
\]</div>
<p>be the dataset with <span class="math notranslate nohighlight">\(N\)</span> samples and <span class="math notranslate nohighlight">\(D\)</span> predictors.</p>
<p>All samples are assumed to be <strong>independent and identically distributed (i.i.d.)</strong> from the unknown but fixed joint distribution
<span class="math notranslate nohighlight">\(\mathbb{P}(\mathcal{X}, \mathcal{Y} ; \boldsymbol{\theta})\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\left \{ \left(\mathrm{X}^{(n)}, Y^{(n)} \right) \right \} \overset{\small{\text{i.i.d.}}}{\sim} \mathbb{P}(\mathcal{X}, \mathcal{Y} ; \boldsymbol{\theta}) \quad \text{for } n = 1, 2, \cdots, N
\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> is the parameter vector of the joint distribution. See <a class="reference internal" href="#joint-distribution-example">Example 30</a> for an example of such.</p>
</section>
<section id="inference-prediction">
<span id="naive-bayes-inference-prediction"></span><h2>Inference/Prediction<a class="headerlink" href="#inference-prediction" title="Permalink to this heading">#</a></h2>
<p>Before we look at the fitting/estimating process, letâ€™s look at the inference/prediction process.</p>
<p>Suppose the problem at hand has <span class="math notranslate nohighlight">\(K\)</span> classes, <span class="math notranslate nohighlight">\(k = 1, 2, \cdots, K\)</span>, where <span class="math notranslate nohighlight">\(k\)</span> is the index of the class.</p>
<p>Then, to find the class of a new test sample <span class="math notranslate nohighlight">\(\mathbf{x}^{(q)} \in \mathbb{R}^{D}\)</span> with <span class="math notranslate nohighlight">\(D\)</span> features,
we can compute the conditional probability of each class <span class="math notranslate nohighlight">\(Y = k\)</span> given the sample <span class="math notranslate nohighlight">\(\mathbf{x}^{(q)}\)</span>:</p>
<div class="proof algorithm admonition" id="naive-bayes-inference-algorithm">
<p class="admonition-title"><span class="caption-number">Algorithm 2 </span> (Naive Bayes Inference Algorithm)</p>
<section class="algorithm-content" id="proof-content">
<ul>
<li><p>Compute the conditional probability of each class <span class="math notranslate nohighlight">\(Y = k\)</span> given the sample <span class="math notranslate nohighlight">\(\mathbf{x}^{(q)}\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-eq-conditional-naive-bayes">
<span class="eqno">(83)<a class="headerlink" href="#equation-eq-conditional-naive-bayes" title="Permalink to this equation">#</a></span>\[
  \mathbb{P}(Y = k \mid \mathbf{X} = \mathbf{x}^{(q)}) = \dfrac{\mathbb{P}(Y = k) \mathbb{P}(\mathbf{X} = \mathbf{x}^{(q)} \mid Y = k)}{\mathbb{P}(\mathbf{X} = \mathbf{x}^{(q)})} \quad \text{for } k = 1, 2, \cdots, K
  \]</div>
</li>
<li><p>Choose the class <span class="math notranslate nohighlight">\(k\)</span> that maximizes the conditional probability:</p>
<div class="math notranslate nohighlight" id="equation-eq-argmax-naive-bayes-1">
<span class="eqno">(84)<a class="headerlink" href="#equation-eq-argmax-naive-bayes-1" title="Permalink to this equation">#</a></span>\[
  \hat{y}^{(q)} = \arg\max_{k=1}^K \mathbb{P}(Y = k \mid \mathbf{X} = \mathbf{x}^{(q)})
  \]</div>
</li>
<li><p>The observant reader would have noticed that the normalizing constant
<span class="math notranslate nohighlight">\(\mathbb{P}\left(X = \mathbf{x}^{(q)}\right)\)</span> is the same for all <span class="math notranslate nohighlight">\(k\)</span>.
Therefore, we can ignore it and simply choose the class <span class="math notranslate nohighlight">\(k\)</span> that maximizes
the numerator of the conditional probability in <a class="reference internal" href="#equation-eq-conditional-naive-bayes">(83)</a>:</p>
<div class="math notranslate nohighlight" id="equation-eq-argmax-naive-bayes-2">
<span class="eqno">(85)<a class="headerlink" href="#equation-eq-argmax-naive-bayes-2" title="Permalink to this equation">#</a></span>\[
  \hat{y}^{(q)} = \arg\max_{k=1}^K \mathbb{P}(Y = k) \mathbb{P}(\mathbf{X} = \mathbf{x}^{(q)} \mid Y = k)
  \]</div>
<p>since where the normalizing constant is ignored, the conditional probability</p>
<div class="math notranslate nohighlight" id="equation-eq-proportional-naive-bayes">
<span class="eqno">(86)<a class="headerlink" href="#equation-eq-proportional-naive-bayes" title="Permalink to this equation">#</a></span>\[
  \mathbb{P}(Y = k \mid \mathbf{X} = \mathbf{x}^{(q)}) \propto \mathbb{P}(Y = k) \mathbb{P}(\mathbf{X} = \mathbf{x}^{(q)} \mid Y = k)
  \]</div>
<p>by a constant factor <span class="math notranslate nohighlight">\(\mathbb{P}(\mathbf{X} = \mathbf{x}^{(q)})\)</span>.</p>
<p>Note however, to recover the normalizing constant is easy, since the numerator <span class="math notranslate nohighlight">\(\mathbb{P}(Y = k) \mathbb{P}(\mathbf{X} = \mathbf{x}^{(q)} \mid Y = k) \)</span>
must sum up to 1 over all <span class="math notranslate nohighlight">\(k\)</span>, and therefore, the normalizing constant is simply <span class="math notranslate nohighlight">\(\mathbb{P}(\mathbf{X} = \mathbf{x}^{(q)}) = \sum_{k=1}^K \mathbb{P}(Y = k) \mathbb{P}(\mathbf{X} = \mathbf{x}^{(q)} \mid Y = k)\)</span>.</p>
</li>
<li><p>Expressing it in vector form, we have</p>
<div class="math notranslate nohighlight" id="equation-eq-argmax-naive-bayes-3">
<span class="eqno">(87)<a class="headerlink" href="#equation-eq-argmax-naive-bayes-3" title="Permalink to this equation">#</a></span>\[\begin{split}
  \begin{aligned}
  \hat{\mathbf{y}} &amp;= \arg\max_{k=1}^K \begin{bmatrix} \mathbb{P}(Y=1) \mathbb{P}(\mathbf{X} = \mathbf{x}\mid Y = 1) \\ \mathbb{P}(Y=2) \mathbb{P}(\mathbf{X} = \mathbf{x}\mid Y = 2) \\ \vdots \\ \mathbb{P}(Y=K) \mathbb{P}(\mathbf{X} = \mathbf{x} \mid Y = K) \end{bmatrix}_{K \times 1} \\
  &amp;= \arg\max_{k=1}^K \begin{bmatrix} \mathbb{P}(Y=1) \\ \mathbb{P}(Y=2) \\ \cdots \\ \mathbb{P}(Y=K) \end{bmatrix}\circ \begin{bmatrix} \mathbb{P}(\mathbf{X} = \mathbf{x} \mid Y = 1) \\ \mathbb{P}(\mathbf{X} = \mathbf{x}\mid Y = 2) \\ \vdots \\ \mathbb{P}(\mathbf{X} = \mathbf{x} \mid Y = K) \end{bmatrix} \\
  &amp;= \arg\max_{k=1}^K \mathbf{M_1} \circ \mathbf{M_2} \\
  &amp;= \arg\max_{k=1}^K \mathbf{M_1} \circ \mathbf{M_3} \\
  \end{aligned}
  \end{split}\]</div>
<p>where</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-m1">
<span class="eqno">(88)<a class="headerlink" href="#equation-eq-naive-bayes-m1" title="Permalink to this equation">#</a></span>\[\begin{split}
  \mathbf{M_1} = \begin{bmatrix}
  \mathbb{P}(Y = 1) \\
  \mathbb{P}(Y = 2) \\
  \vdots \\
  \mathbb{P}(Y = K)
  \end{bmatrix}_{K \times 1}
  \end{split}\]</div>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-m2">
<span class="eqno">(89)<a class="headerlink" href="#equation-eq-naive-bayes-m2" title="Permalink to this equation">#</a></span>\[\begin{split}
  \mathbf{M_2} = \begin{bmatrix}
  \mathbb{P}(\mathbf{X} = \mathbf{x} \mid Y = 1) \\
  \mathbb{P}(\mathbf{X} = \mathbf{x} \mid Y = 2) \\
  \vdots \\
  \mathbb{P}(\mathbf{X} = \mathbf{x} \mid Y = K)
  \end{bmatrix}_{K \times 1}
  \end{split}\]</div>
<p>and</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-m3">
<span class="eqno">(90)<a class="headerlink" href="#equation-eq-naive-bayes-m3" title="Permalink to this equation">#</a></span>\[\begin{split}
  \mathbf{M_3} &amp;= \begin{bmatrix}
  \mathbb{P}(X_1 = x_1 \mid Y = 1 ; \theta_{11}) &amp; \mathbb{P}(X_2 = x_2 \mid Y = 1 ; \theta_{12}) &amp; \cdots &amp; \mathbb{P}(X_D = x_D \mid Y = 1 ; \theta_{1D}) \\
  \mathbb{P}(X_1 = x_1 \mid Y = 2 ; \theta_{21}) &amp; \mathbb{P}(X_2 = x_2 \mid Y = 2 ; \theta_{22}) &amp; \cdots &amp; \mathbb{P}(X_D = x_D \mid Y = 2 ; \theta_{2D}) \\
  \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
  \mathbb{P}(X_1 = x_1 \mid Y = K ; \theta_{K1}) &amp; \mathbb{P}(X_2 = x_2 \mid Y = K ; \theta_{K2}) &amp; \cdots &amp; \mathbb{P}(X_D = x_D \mid Y = K ; \theta_{KD})
  \end{bmatrix}_{K \times D} \\
  \end{split}\]</div>
<p>Note superscript <span class="math notranslate nohighlight">\(q\)</span> is removed for simplicity, and <span class="math notranslate nohighlight">\(\circ\)</span> is the element-wise (Hadamard) product.
We will also explain why we replace <span class="math notranslate nohighlight">\(\mathbf{M_2}\)</span> with <span class="math notranslate nohighlight">\(\mathbf{M_3}\)</span> in <a class="reference internal" href="#naive-bayes-conditional-independence"><span class="std std-ref">Conditional Independence</span></a>.</p>
</li>
</ul>
</section>
</div><p>Now if we just proceed to estimate the conditional probability <span class="math notranslate nohighlight">\(\mathbb{P}(Y = k \mid \mathbf{X} = \mathbf{x}^{(q)})\)</span>, we will need to estimate the joint probability <span class="math notranslate nohighlight">\(\mathbb{P}(X = \mathbf{x}^{(q)}, Y = k)\)</span>, since by definition, we have</p>
<div class="math notranslate nohighlight" id="equation-eq-joint-naive-bayes-1">
<span class="eqno">(91)<a class="headerlink" href="#equation-eq-joint-naive-bayes-1" title="Permalink to this equation">#</a></span>\[
\mathbb{P}(X = \mathbf{x}^{(q)}, Y = k) = \mathbb{P}(Y = k) \mathbb{P}(X = \mathbf{x}^{(q)} \mid Y = k)
\]</div>
<p>which is intractable<a class="footnote-reference brackets" href="#intractable" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>.</p>
<p>However, if we can <em><strong>estimate</strong></em> the conditional probability (likelihood) <span class="math notranslate nohighlight">\(\mathbb{P}(\mathbf{X} = \mathbf{x}^{(q)} \mid Y = k)\)</span>
and the prior probability <span class="math notranslate nohighlight">\(\mathbb{P}(Y = k)\)</span>, then we can use Bayesâ€™ rule to
compute the posterior conditional probability <span class="math notranslate nohighlight">\(\mathbb{P}(Y = k \mid \mathbf{X} = \mathbf{x}^{(q)})\)</span>.</p>
</section>
<section id="the-naive-bayes-form">
<h2>The Naive Bayes Form<a class="headerlink" href="#the-naive-bayes-form" title="Permalink to this heading">#</a></h2>
<p>Quoted from <a class="reference external" href="https://en.wikipedia.org/wiki/Bayes%27_theorem#Simple_form_2">Wikipedia</a>, it is worth noting that
thereâ€™s a few forms of Naive Bayes:</p>
<section id="simple-form">
<h3>Simple Form<a class="headerlink" href="#simple-form" title="Permalink to this heading">#</a></h3>
<p>If <span class="math notranslate nohighlight">\(X\)</span> is continuous and <span class="math notranslate nohighlight">\(Y\)</span> is discrete,</p>
<div class="math notranslate nohighlight">
\[
f_{X \mid Y=y}(x)=\frac{P(Y=y \mid X=x) f_X(x)}{P(Y=y)}
\]</div>
<p>where each <span class="math notranslate nohighlight">\(f\)</span> is a density function.</p>
<p>If <span class="math notranslate nohighlight">\(X\)</span> is discrete and <span class="math notranslate nohighlight">\(Y\)</span> is continuous,</p>
<div class="math notranslate nohighlight">
\[
P(X=x \mid Y=y)=\frac{f_{Y \mid X=x}(y) P(X=x)}{f_Y(y)} .
\]</div>
<p>If both <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are continuous,</p>
<div class="math notranslate nohighlight">
\[
f_{X \mid Y=y}(x)=\frac{f_{Y \mid X=x}(y) f_X(x)}{f_Y(y)} .
\]</div>
</section>
<section id="extended-form">
<h3>Extended form<a class="headerlink" href="#extended-form" title="Permalink to this heading">#</a></h3>
<p>A continuous event space is often conceptualized in terms of the numerator terms. It is then useful to eliminate the denominator using the law of total probability. For <span class="math notranslate nohighlight">\(f_Y(y)\)</span>, this becomes an integral:</p>
<div class="math notranslate nohighlight">
\[
f_Y(y)=\int_{-\infty}^{\infty} f_{Y \mid X=\xi}(y) f_X(\xi) d \xi
\]</div>
</section>
</section>
<section id="the-naive-bayes-assumptions">
<h2>The Naive Bayes Assumptions<a class="headerlink" href="#the-naive-bayes-assumptions" title="Permalink to this heading">#</a></h2>
<p>In this section, we talk about some implicit and explicit assumptions of the Naive Bayes model.</p>
<section id="independent-and-identically-distributed-i-i-d">
<h3>Independent and Identically Distributed (i.i.d.)<a class="headerlink" href="#independent-and-identically-distributed-i-i-d" title="Permalink to this heading">#</a></h3>
<p>In supervised learning, implicitly or explicitly, one <em>always</em> assumes that the training set</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathcal{D} &amp;= \left\{\left(\mathbf{x}^{(1)}, y^{(1)}\right), \left(\mathbf{x}^{(2)}, y^{(2)}\right), \cdots, \left(\mathbf{x}^{(N)}, y^{(N)}\right)\right\} \\
\end{aligned}
\end{split}\]</div>
<p>is composed of <span class="math notranslate nohighlight">\(N\)</span> input/response tuples</p>
<div class="math notranslate nohighlight">
\[
\left({\mathbf{X}}^{(n)} = \mathbf{x}^{(n)}, Y^{(n)} = y^{(n)}\right)
\]</div>
<p>that are <em><strong>independently drawn from the same (identical) joint distribution</strong></em></p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}_{\{\mathcal{X}, \mathcal{Y} ; \boldsymbol{\theta}\}}(\mathbf{x}, y)
\]</div>
<p>with</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}(\mathbf{X} = \mathbf{x}, Y = y ; \boldsymbol{\theta}) = \mathbb{P}(Y = y \mid \mathbf{X} = \mathbf{x}) \mathbb{P}(\mathbf{X} = \mathbf{x})
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbb{P}(Y = y \mid \mathbf{X} = \mathbf{x})\)</span> is the conditional probability of <span class="math notranslate nohighlight">\(Y\)</span> given <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>,
the relationship that the learner algorithm/concept <span class="math notranslate nohighlight">\(c\)</span> is trying to capture.</p>
<div class="proof definition admonition" id="iid-assumption">
<p class="admonition-title"><span class="caption-number">Definition 131 </span> (The i.i.d. Assumption)</p>
<section class="definition-content" id="proof-content">
<p>Mathematically, this i.i.d. assumption writes (also defined in <a class="reference internal" href="../../../probability_theory/03_discrete_random_variables/iid.html#def_iid">Definition 31</a>):</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\left({\mathbf{X}}^{(n)}, Y^{(n)}\right) &amp;\sim \mathbb{P}_{\{\mathcal{X}, \mathcal{Y}, \boldsymbol{\theta}\}}(\mathbf{x}, y) \quad \text{and}\\
\left({\mathbf{X}}^{(n)}, Y^{(n)}\right) &amp;\text{ independent of } \left({\mathbf{X}}^{(m)}, Y^{(m)}\right) \quad \forall n \neq m \in \{1, 2, \ldots, N\}
\end{aligned}
\end{split}\]</div>
<p>and we sometimes denote</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
\left(\mathbf{x}^{(n)}, y^{(n)}\right) \overset{\text{i.i.d.}}{\sim} \mathbb{P}_{\{\mathcal{X}, \mathcal{Y}, \boldsymbol{\theta}\}}(\mathbf{x}, y)
\end{aligned}
\]</div>
</section>
</div><p>The confusion in the <strong>i.i.d.</strong> assumption is that we are not talking about the individual random variables
<span class="math notranslate nohighlight">\(X_1^{(n)}, X_2^{(n)}, \ldots, X_D^{(n)}\)</span> here, but the entire random vector <span class="math notranslate nohighlight">\(\mathbf{X}^{(n)}\)</span>.</p>
<p>This means there is no assumption of <span class="math notranslate nohighlight">\(X_1^{(n)}, X_2^{(n)}, \ldots, X_D^{(n)}\)</span> being <strong>i.i.d.</strong>. Instead, the samples
<span class="math notranslate nohighlight">\(\mathbf{X}^{(1)}, \mathbf{X}^{(2)}, \ldots, \mathbf{X}^{(N)}\)</span> are <strong>i.i.d.</strong>.</p>
</section>
<section id="conditional-independence">
<span id="naive-bayes-conditional-independence"></span><h3>Conditional Independence<a class="headerlink" href="#conditional-independence" title="Permalink to this heading">#</a></h3>
<p>The core assumption of the Naive Bayes model is that the predictors <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>
are conditionally independent given the class label <span class="math notranslate nohighlight">\(Y\)</span>.</p>
<p>But how did we arrive at the conditional independence assumption? Letâ€™s look at what we wanted to achieve in the first place.</p>
<p>Recall that our goal in <a class="reference internal" href="#naive-bayes-inference-prediction"><span class="std std-ref">Inference/Prediction</span></a> is to find the class <span class="math notranslate nohighlight">\(k \in \{1, 2, \cdots, K\}\)</span> that maximizes the <strong>posterior</strong> probability
<span class="math notranslate nohighlight">\(\mathbb{P}(Y = k \mid \mathbf{X} = \mathbf{x}^{(q)} ; \boldsymbol{\theta})\)</span>.</p>
<div class="math notranslate nohighlight" id="equation-eq-argmax-naive-bayes-4">
<span class="eqno">(92)<a class="headerlink" href="#equation-eq-argmax-naive-bayes-4" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{aligned}
\hat{y}^{(q)} &amp;= \arg \max_{k} \mathbb{P}(Y = k \mid \mathbf{X} = \mathbf{x}^{(q)} ; \boldsymbol{\theta}) \\
              &amp;= \arg \max_{k} \frac{\mathbb{P}(Y = k, \mathbf{X} = \mathbf{x}^{(q)} ; \boldsymbol{\theta})}{\mathbb{P}(\mathbf{X} = \mathbf{x}^{(q)} ; \boldsymbol{\theta})} \\
              &amp;= \arg \max_{k} \frac{\mathbb{P}(Y = k ; \boldsymbol{\pi}) \mathbb{P}(\mathbf{X} = \mathbf{x}^{(q)} \mid Y = k ; \boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}})}{\mathbb{P}(\mathbf{X} = \mathbf{x}^{(q)} ; \boldsymbol{\theta})}\\
\end{aligned}
\end{split}\]</div>
<p>We have seen earlier in <a class="reference internal" href="#naive-bayes-inference-algorithm">Algorithm 2</a> that since the denominator
is constant for all <span class="math notranslate nohighlight">\(k\)</span>, we can ignore it and just maximize the numerator.</p>
<div class="math notranslate nohighlight" id="equation-eq-argmax-naive-bayes-5">
<span class="eqno">(93)<a class="headerlink" href="#equation-eq-argmax-naive-bayes-5" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{aligned}
\hat{y}^{(q)} &amp;= \arg \max_{k} \mathbb{P}\left(Y = k ; \boldsymbol{\pi}\right) \mathbb{P}\left(\mathbf{X} = \mathbf{x}^{(q)} \mid Y = k ; \boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}\right) \\
\end{aligned}
\end{split}\]</div>
<p>This suggests we need to find estimates for both the <strong>prior</strong> and the <strong>likelihood</strong>. This of course
involves us finding the <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}\)</span> that maximize the likelihood function<a class="footnote-reference brackets" href="#likelihood-1" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a>, which we will talk about later.</p>
<p>In order to meaningfully optimize the expression, we need to decompose the expression <a class="reference internal" href="#equation-eq-argmax-naive-bayes-5">(93)</a>
into its components that contain the parameters we want to estimate.</p>
<div class="math notranslate nohighlight" id="equation-eq-joint-distribution">
<span class="eqno">(94)<a class="headerlink" href="#equation-eq-joint-distribution" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{aligned}
\mathbb{P}(Y = k ; \boldsymbol{\pi}) \mathbb{P}(\mathbf{X} \mid Y = k ; \boldsymbol{\theta}) &amp;= \mathbb{P}((Y, \mathbf{X}) ; \boldsymbol{\theta}, \boldsymbol{\pi}) \\
&amp;= \mathbb{P}(Y, X_1, X_2, \ldots X_D)
\end{aligned}
\end{split}\]</div>
<p>which is actually the joint distribution of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> and <span class="math notranslate nohighlight">\(Y\)</span><a class="footnote-reference brackets" href="#joint-distribution" id="id3" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a>.</p>
<p>This joint distribution expression <a class="reference internal" href="#equation-eq-joint-distribution">(94)</a> can be further decomposed by the chain rule of probability<a class="footnote-reference brackets" href="#chain-rule-of-probability" id="id4" role="doc-noteref"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></a> as</p>
<div class="math notranslate nohighlight" id="equation-eq-joint-distribution-decomposed">
<span class="eqno">(95)<a class="headerlink" href="#equation-eq-joint-distribution-decomposed" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{aligned}
\mathbb{P}(Y, X_1, X_2, \ldots X_D) &amp;= \mathbb{P}(Y) \mathbb{P}(X_1, X_2, \ldots X_D \mid Y) \\
&amp;= \mathbb{P}(Y) \mathbb{P}(X_1 \mid Y) \mathbb{P}(X_2 \mid Y, X_1) \cdots \mathbb{P}(X_D \mid Y, X_1, X_2, \ldots X_{D-1}) \\
&amp;= \mathbb{P}(Y) \prod_{d=1}^D \mathbb{P}(X_d \mid Y, X_1, X_2, \ldots X_{d-1}) \\
&amp;= \mathbb{P}(Y) \prod_{d=1}^{D} \mathbb{P}\left(X_d \middle \vert \bigcap_{d'=1}^{d-1} X_{d'}\right)
\end{aligned}
\end{split}\]</div>
<p>This alone does not get us any further, we still need to estimate roughly <span class="math notranslate nohighlight">\(2^{D}\)</span> parameters<a class="footnote-reference brackets" href="#dparameters" id="id5" role="doc-noteref"><span class="fn-bracket">[</span>5<span class="fn-bracket">]</span></a>,
which is computationally expensive. Not to forget that we need to estimate for each class <span class="math notranslate nohighlight">\(k \in \{1, 2, 3, \ldots, K\}\)</span>
which has a complexity of <span class="math notranslate nohighlight">\(\sim \mathcal{O}(2^DK)\)</span>.</p>
<div class="proof remark admonition" id="2dparameters">
<p class="admonition-title"><span class="caption-number">Remark 38 </span> (Why <span class="math notranslate nohighlight">\(2^D\)</span> parameters?)</p>
<section class="remark-content" id="proof-content">
<p>Letâ€™s simplify the problem by assuming each feature <span class="math notranslate nohighlight">\(X_d\)</span> and the class label <span class="math notranslate nohighlight">\(Y\)</span> are binary random variables,
i.e. <span class="math notranslate nohighlight">\(X_d \in \{0, 1\}\)</span> and <span class="math notranslate nohighlight">\(Y \in \{0, 1\}\)</span>.</p>
<p>Then <span class="math notranslate nohighlight">\(\mathbb{P}(Y, X_1, X_2, \ldots X_D)\)</span> is a joint distribution of <span class="math notranslate nohighlight">\(D+1\)</span> random variables, each with <span class="math notranslate nohighlight">\(2\)</span> values.</p>
<p>This means the sample space of <span class="math notranslate nohighlight">\(\mathbb{P}(Y, X_1, X_2, \ldots X_D)\)</span> is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathcal{S} &amp;= \{(0, 1)\} \times \{(0, 1)\} \times \{(0, 1)\} \times \cdots \times \{(0, 1)\} \\
&amp;= \{(0, 0, 0, \ldots, 0), (0, 0, 0, \ldots, 1), (0, 0, 1, \ldots, 0), \ldots, (1, 1, 1, \ldots, 1)\}
\end{aligned}
\end{split}\]</div>
<p>which has <span class="math notranslate nohighlight">\(2^{D+1}\)</span> elements.
To really get the exact joint distribution, we need to estimate the probability of each element in the sample space, which is <span class="math notranslate nohighlight">\(2^{D+1}\)</span> parameters.</p>
<p>This has two caveats:</p>
<ol class="arabic simple">
<li><p>There are too many parameters to estimate, which is computationally expensive. Imagine if <span class="math notranslate nohighlight">\(D\)</span> is 1000, we need to estimate <span class="math notranslate nohighlight">\(2^{1000}\)</span> parameters, which is infeasible.</p></li>
<li><p>Even if we can estimate all the parameters, we are essentially overfitting the data by memorizing the training data. There is no learning involved.</p></li>
</ol>
</section>
</div><p>This is where the â€œNaiveâ€ assumption comes in. The Naive Bayesâ€™ classifier assumes that
the features are conditionally independent<a class="footnote-reference brackets" href="#id15" id="id6" role="doc-noteref"><span class="fn-bracket">[</span>6<span class="fn-bracket">]</span></a> given the class label.</p>
<p>More formally stated,</p>
<div class="proof definition admonition" id="conditional-independence">
<p class="admonition-title"><span class="caption-number">Definition 132 </span> (Conditional Independence)</p>
<section class="definition-content" id="proof-content">
<div class="math notranslate nohighlight" id="equation-eq-conditional-independence">
<span class="eqno">(96)<a class="headerlink" href="#equation-eq-conditional-independence" title="Permalink to this equation">#</a></span>\[
\mathbb{P}(X_d \mid Y = k, X_{d^{'}}) = \mathbb{P}(X_d \mid Y = k) \quad \text{for all } d \neq d^{'}
\]</div>
</section>
</div><p>with this assumption, we can further simplify expression <a class="reference internal" href="#equation-eq-joint-distribution-decomposed">(95)</a> as</p>
<div class="math notranslate nohighlight" id="equation-eq-conditional-independence-naive-bayes-1">
<span class="eqno">(97)<a class="headerlink" href="#equation-eq-conditional-independence-naive-bayes-1" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{aligned}
\mathbb{P}(Y, X_1, X_2, \ldots X_D) &amp;= \mathbb{P}(Y ; \boldsymbol{\pi}) \prod_{d=1}^D \mathbb{P}(X_d \mid Y ; \theta_{d}) \\
\end{aligned}
\end{split}\]</div>
<p>More precisely, after the simplification in <a class="reference internal" href="#equation-eq-conditional-independence-naive-bayes-1">(97)</a>,
the argmax expression in <a class="reference internal" href="#equation-eq-conditional-naive-bayes">(83)</a> can be written as</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-classifier-1">
<span class="eqno">(98)<a class="headerlink" href="#equation-eq-naive-bayes-classifier-1" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{aligned}
\mathbb{P}(Y = k \mid \mathbf{X} = \mathbf{x} ; \boldsymbol{\theta}) &amp; = \dfrac{\mathbb{P}(Y = k ; \boldsymbol{\pi}) \mathbb{P}(\mathbf{X} \mid Y = k ; \boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}})}{\mathbb{P}(\mathbf{X})} \\
&amp;= \dfrac{\mathbb{P}(Y, X_1, X_2, \ldots X_D)}{\mathbb{P}(\mathbf{X})} \\
&amp;= \dfrac{\mathbb{P}(Y = k ; \boldsymbol{\pi}) \prod_{d=1}^D \mathbb{P}(X_d = x_d \mid Y = k ; \theta_{kd})}{\mathbb{P}(\mathbf{X} = \mathbf{x})} \\
\end{aligned}
\end{split}\]</div>
<p>Consequently, our argmax expression in <a class="reference internal" href="#equation-eq-argmax-naive-bayes-2">(85)</a> can be written as</p>
<div class="math notranslate nohighlight" id="equation-eq-argmax-naive-bayes-6">
<span class="eqno">(99)<a class="headerlink" href="#equation-eq-argmax-naive-bayes-6" title="Permalink to this equation">#</a></span>\[
\arg \max_{k=1}^K \mathbb{P}(Y = k \mid \mathbf{X}) = \arg \max_{k=1}^K \mathbb{P}(Y = k ; \boldsymbol{\pi}) \prod_{d=1}^D \mathbb{P}(X_d = x_d \mid Y = k ; \theta_{kd})
\]</div>
<p>We also make some updates to the vector form <a class="reference internal" href="#equation-eq-argmax-naive-bayes-3">(87)</a> by updating <span class="math notranslate nohighlight">\(\mathbf{M_2}\)</span> to:</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-m2-updated">
<span class="eqno">(100)<a class="headerlink" href="#equation-eq-naive-bayes-m2-updated" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{aligned}
  \mathbf{M_2} &amp;= \begin{bmatrix}
  \mathbb{P}(\mathbf{X} = \mathbf{x} \mid Y = 1) \\
  \mathbb{P}(\mathbf{X} = \mathbf{x} \mid Y = 2) \\
  \vdots \\
  \mathbb{P}(\mathbf{X} = \mathbf{x} \mid Y = K)
  \end{bmatrix}_{K \times 1} \\
  &amp;= \begin{bmatrix}
  \mathbb{P}(X_1 = x_1 \mid Y = 1 ; \theta_{11}) \mathbb{P}(X_2 = x_2 \mid Y = 1 ; \theta_{12}) \cdots \mathbb{P}(X_D = x_D \mid Y = 1 ; \theta_{1D}) \\
  \mathbb{P}(X_1 = x_1 \mid Y = 2 ; \theta_{21}) \mathbb{P}(X_2 = x_2 \mid Y = 2 ; \theta_{22}) \cdots \mathbb{P}(X_D = x_D \mid Y = 2 ; \theta_{2D}) \\
  \vdots \\
  \mathbb{P}(X_1 = x_1 \mid Y = K ; \theta_{K1}) \mathbb{P}(X_2 = x_2 \mid Y = K ; \theta_{K2}) \cdots \mathbb{P}(X_D = x_D \mid Y = K ; \theta_{KD})
  \end{bmatrix}_{K \times 1} \\
\end{aligned}
\end{split}\]</div>
<p>To easily recover each row of <span class="math notranslate nohighlight">\(\mathbf{M_2}\)</span>, it is efficient to define a <span class="math notranslate nohighlight">\(K \times D\)</span> matrix, denoted <span class="math notranslate nohighlight">\(\mathbf{M_3}\)</span></p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-m3-explained">
<span class="eqno">(101)<a class="headerlink" href="#equation-eq-naive-bayes-m3-explained" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{aligned}
  \mathbf{M_3} &amp;= \begin{bmatrix}
  \mathbb{P}(X_1 = x_1 \mid Y = 1 ; \theta_{11}) &amp; \mathbb{P}(X_2 = x_2 \mid Y = 1 ; \theta_{12}) &amp; \cdots &amp; \mathbb{P}(X_D = x_D \mid Y = 1 ; \theta_{1D}) \\
  \mathbb{P}(X_1 = x_1 \mid Y = 2 ; \theta_{21}) &amp; \mathbb{P}(X_2 = x_2 \mid Y = 2 ; \theta_{22}) &amp; \cdots &amp; \mathbb{P}(X_D = x_D \mid Y = 2 ; \theta_{2D}) \\
  \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
  \mathbb{P}(X_1 = x_1 \mid Y = K ; \theta_{K1}) &amp; \mathbb{P}(X_2 = x_2 \mid Y = K ; \theta_{K2}) &amp; \cdots &amp; \mathbb{P}(X_D = x_D \mid Y = K ; \theta_{KD})
  \end{bmatrix}_{K \times D} \\
\end{aligned}
\end{split}\]</div>
<p>where we can easily recover each row of <span class="math notranslate nohighlight">\(\mathbf{M_2}\)</span> by taking the product of the corresponding row of <span class="math notranslate nohighlight">\(\mathbf{M_3}\)</span>.</p>
</section>
</section>
<section id="parameter-vector">
<h2>Parameter Vector<a class="headerlink" href="#parameter-vector" title="Permalink to this heading">#</a></h2>
<p>In the last section on <a class="reference internal" href="#naive-bayes-conditional-independence"><span class="std std-ref">Conditional Independence</span></a>, we indicated parameters in the expressions.
Here we discuss a little on this newly introduced notation.</p>
<p>Each <span class="math notranslate nohighlight">\(\pi_k\)</span> of <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span> refers to the prior probability of class <span class="math notranslate nohighlight">\(k\)</span>, and <span class="math notranslate nohighlight">\(\theta_{kd}\)</span> refers to the parameter of the
class conditional density for class <span class="math notranslate nohighlight">\(k\)</span> and feature <span class="math notranslate nohighlight">\(d\)</span><a class="footnote-reference brackets" href="#kdparameters" id="id7" role="doc-noteref"><span class="fn-bracket">[</span>7<span class="fn-bracket">]</span></a>. Furthermore,
the boldsymbol <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> is the parameter vector,</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\theta} = \left(\boldsymbol{\pi}, \{\theta_{kd}\}_{k=1, d=1}^{K, D} \right) = \left(\boldsymbol{\pi}, \boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}\right)
\]</div>
<div class="proof definition admonition" id="parameter-vector">
<p class="admonition-title"><span class="caption-number">Definition 133 </span> (The Parameter Vector)</p>
<section class="definition-content" id="proof-content">
<p>There is not much to say about the categorical component <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span>, since we are
just estimating the prior probabilities of the classes.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{\pi} = \begin{bmatrix} \pi_1 \\ \pi_2 \\ \vdots \\ \pi_K \end{bmatrix}_{K \times 1}
\end{split}\]</div>
<p>The parameter vector (matrix) <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}=\{\theta_{kd}\}_{k=1, d=1}^{K, D}\)</span> is a bit more complicated.
It resides in the <span class="math notranslate nohighlight">\(\mathbb{R}^{K \times D}\)</span> space, where each element <span class="math notranslate nohighlight">\(\theta_{kd}\)</span> is the parameter
associated with feature <span class="math notranslate nohighlight">\(d\)</span> conditioned on class <span class="math notranslate nohighlight">\(k\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}} = \begin{bmatrix}
\theta_{11} &amp; \theta_{12} &amp; \dots &amp; \theta_{1D} \\
\theta_{21} &amp; \theta_{22} &amp; \dots &amp; \theta_{2D} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\theta_{K1} &amp; \theta_{K2} &amp; \dots &amp; \theta_{KD}
\end{bmatrix}_{K \times D}
\end{split}\]</div>
<p>So if <span class="math notranslate nohighlight">\(K=3\)</span> and <span class="math notranslate nohighlight">\(D=2\)</span>, then the parameter vector <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> is a <span class="math notranslate nohighlight">\(3 \times 2\)</span> matrix, i.e.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}} = \begin{bmatrix}
\theta_{11} &amp; \theta_{12} \\
\theta_{21} &amp; \theta_{22} \\
\theta_{31} &amp; \theta_{32}
\end{bmatrix}_{3 \times 2}
\end{split}\]</div>
<p>This means we have effectively reduced our complexity from <span class="math notranslate nohighlight">\(\sim \mathcal{O}(2^D)\)</span> to <span class="math notranslate nohighlight">\(\sim \mathcal{O}(KD + 1)\)</span>
assuming the same setup in <a class="reference internal" href="#2dparameters">Remark 38</a>.</p>
<p>One big misconception is that the elements in <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}\)</span> are scalar values.
This is not true, for example, letâ€™s look at the first entry <span class="math notranslate nohighlight">\(\theta_{11}\)</span>, corresponding to
the parameter of class <span class="math notranslate nohighlight">\(K=1\)</span> and feature <span class="math notranslate nohighlight">\(D=1\)</span>, i.e. <span class="math notranslate nohighlight">\(\theta_{11}\)</span> is the parameter of the class conditional
density <span class="math notranslate nohighlight">\(\mathbb{P}(X_1 \mid Y = 1)\)</span>. Now <span class="math notranslate nohighlight">\(X_1\)</span> can take on any value in <span class="math notranslate nohighlight">\(\mathbb{R}\)</span>, which is indeed a scalar,
we further assume that <span class="math notranslate nohighlight">\(X_1\)</span> takes on a univariate Gaussian distribution, then <span class="math notranslate nohighlight">\(\theta_{11}\)</span> is a vector of length 2, i.e.</p>
<div class="math notranslate nohighlight">
\[
\theta_{11} = \begin{bmatrix} \mu_{11} &amp; \sigma_{11} \end{bmatrix}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mu_{11}\)</span> is the mean of the Gaussian distribution and <span class="math notranslate nohighlight">\(\sigma_{11}\)</span> is the standard deviation of the Gaussian distribution.
This is something we need to take note of.</p>
<p><strong>We have also reduced the problem of estimating the joint distribution to just individual conditional distributions.</strong></p>
<p>Overall, before this assumption, you can think of estimating the joint distribution of <span class="math notranslate nohighlight">\(Y\)</span> and <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>,
and after this assumption, you can simply individually estimate each conditional distribution.</p>
</section>
</div><p>Notice that the shape of <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span> is <span class="math notranslate nohighlight">\(K \times 1\)</span>, and the shape of <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}\)</span> is <span class="math notranslate nohighlight">\(K \times D\)</span>.
This corresponds to the shape of the matrix <span class="math notranslate nohighlight">\(\mathbf{M_1}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{M_3}\)</span> as defined in
<a class="reference internal" href="#equation-eq-naive-bayes-m1">(88)</a> and <a class="reference internal" href="#equation-eq-naive-bayes-m3">(90)</a>, respectively. This is expected since
<span class="math notranslate nohighlight">\(\mathbf{M_1}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{M_3}\)</span> hold the PDFs while <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}\)</span> hold the parameters
of these PDFs.</p>
<div class="proof remark admonition" id="remark-empirical-parameters">
<p class="admonition-title"><span class="caption-number">Remark 39 </span> (Empirical Parameters)</p>
<section class="remark-content" id="proof-content">
<p>It is worth noting that we are discussing the parameter vectors <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span>
and <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}\)</span> which represents the true
underlying distribution. However, our ultimate goal is to estimate these parameters because
we do not have the underlying distributions at hand, otherwise there is no need to do
machine learning.</p>
<p>More concretely, our task is to find</p>
<div class="math notranslate nohighlight">
\[
\hat{\boldsymbol{\pi}} \quad \text{ and } \quad \hat{\boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}}
\]</div>
</section>
</div></section>
<section id="inductive-bias-distribution-assumptions">
<h2>Inductive Bias (Distribution Assumptions)<a class="headerlink" href="#inductive-bias-distribution-assumptions" title="Permalink to this heading">#</a></h2>
<p>We still need to introduce some inductive bias into <a class="reference internal" href="#equation-eq-naive-bayes-classifier-1">(98)</a>, more concretely, we need to make some assumptions about the distribution
of <span class="math notranslate nohighlight">\(\mathbb{P}(Y)\)</span> and <span class="math notranslate nohighlight">\(\mathbb{P}(X_d \mid Y)\)</span>.</p>
<p>For the target variable, we typically model it as a categorical distribution,</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}(Y) \sim \mathrm{Categorical}(\boldsymbol{\pi})
\]</div>
<p>For the conditional distribution of the features, we typically model it according to what type of features we have.</p>
<p>For example, if we have binary features, then we can model it as a Bernoulli distribution,</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}(X_d \mid Y) \sim \mathrm{Bernoulli}(\theta_{dk})
\]</div>
<p>If we have categorical features, then we can model it as a multinomial/catgorical distribution,</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}(X_d \mid Y) \sim \mathrm{Multinomial}(\boldsymbol{\theta}_{dk})
\]</div>
<p>If we have continuous features, then we can model it as a Gaussian distribution,</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}(X_d \mid Y) \sim \mathcal{N}(\mu_{dk}, \sigma_{dk}^2)
\]</div>
<p>To reiterate, we want to make some inductive bias assumptions of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> conditional on <span class="math notranslate nohighlight">\(Y\)</span>,
as well as with <span class="math notranslate nohighlight">\(Y\)</span>. Note very carefully that we are not talking about the marginal distribution of
<span class="math notranslate nohighlight">\(\mathbf{X}\)</span> here, instead, we are talking about the conditional distribution of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> given <span class="math notranslate nohighlight">\(Y\)</span>. The distinction is subtle, but important.</p>
<section id="targets-categorical-distribution">
<h3>Targets (Categorical Distribution)<a class="headerlink" href="#targets-categorical-distribution" title="Permalink to this heading">#</a></h3>
<p>As mentioned earlier, both <span class="math notranslate nohighlight">\(Y^{(n)}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{X}^{(n)}\)</span> are random variables/vectors.
This means we need to estimate both of them.</p>
<p>We first conveniently assume that <span class="math notranslate nohighlight">\(Y^{(n)}\)</span> is a discrete random variable, and
follows the <strong><a class="reference external" href="https://en.wikipedia.org/wiki/Categorical_distribution">Category distribution</a></strong><a class="footnote-reference brackets" href="#categorical-distribution" id="id8" role="doc-noteref"><span class="fn-bracket">[</span>8<span class="fn-bracket">]</span></a>,
an extension of the Bernoulli distribution to multiple classes. Instead of a single parameter <span class="math notranslate nohighlight">\(p\)</span> (probability of success for Bernoulli),
the Category distribution has a vector <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span> of <span class="math notranslate nohighlight">\(K\)</span> parameters.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{\pi} = \begin{bmatrix} \pi_1 \\ \vdots \\ \pi_K \end{bmatrix}_{K \times 1}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\pi_k\)</span> is the probability of <span class="math notranslate nohighlight">\(Y^{(n)}\)</span> taking on value <span class="math notranslate nohighlight">\(k\)</span>.</p>
<div class="math notranslate nohighlight" id="equation-eq-target-category-distribution">
<span class="eqno">(102)<a class="headerlink" href="#equation-eq-target-category-distribution" title="Permalink to this equation">#</a></span>\[\begin{split}
Y^{(n)} \overset{\small{\text{i.i.d.}}}{\sim} \text{Category}(\boldsymbol{\pi}) \quad \text{where } \boldsymbol{\pi} = \begin{bmatrix} \pi_1 \\ \vdots \\ \pi_K \end{bmatrix}_{K \times 1}
\end{split}\]</div>
<p>Equivalently,</p>
<div class="math notranslate nohighlight" id="equation-eq-category-distribution">
<span class="eqno">(103)<a class="headerlink" href="#equation-eq-category-distribution" title="Permalink to this equation">#</a></span>\[
\mathbb{P}(Y^{(n)} = k) = \pi_k \quad \text{for } k = 1, 2, \cdots, K
\]</div>
<p>Consequently, we just need to estimate <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span> to recover <span class="math notranslate nohighlight">\(\mathbf{M_1}\)</span> defined in <a class="reference internal" href="#equation-eq-naive-bayes-m1">(88)</a>.</p>
<p>Find <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\pi}}\)</span> such that <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\pi}}\)</span> maximizes the likelihood of the observed data.</p>
<div class="math notranslate nohighlight">
\[
\hat{\boldsymbol{\pi}} = \arg\max_{\boldsymbol{\pi}} \mathcal{L}(\boldsymbol{\pi} \mid \mathcal{D})
\]</div>
<div class="proof definition admonition" id="categorical-distribution">
<p class="admonition-title"><span class="caption-number">Definition 134 </span> (Categorical Distribution)</p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(Y\)</span> be a discrete random variable with <span class="math notranslate nohighlight">\(K\)</span> number of states.
Then <span class="math notranslate nohighlight">\(Y\)</span> follows a categorical distribution with parameters <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span> if</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}(Y = k) = \pi_k \quad \text{for } k = 1, 2, \cdots, K
\]</div>
<p>Consequently, the PMF of the categorical distribution is defined more compactly as,</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}(Y = k) = \prod_{k=1}^K \pi_k^{I\{Y = k\}}
\]</div>
<p>where <span class="math notranslate nohighlight">\(I\{Y = k\}\)</span> is the indicator function that is equal to 1 if <span class="math notranslate nohighlight">\(Y = k\)</span> and 0 otherwise.</p>
</section>
</div><div class="proof definition admonition" id="categorical-multinomial-distribution">
<p class="admonition-title"><span class="caption-number">Definition 135 </span> (Categorical (Multinomial) Distribution)</p>
<section class="definition-content" id="proof-content">
<p>This formulation is adopted by Bishopâ€™s<span id="id9">[<a class="reference internal" href="../../../references_resources_roadmap/bibliography.html#id6" title="CHRISTOPHER M. BISHOP. Pattern recognition and machine learning. SPRINGER-VERLAG NEW YORK, 2016.">BISHOP, 2016</a>]</span>, the categorical distribution is defined as</p>
<div class="math notranslate nohighlight" id="equation-eq-categorical-distribution-bishop">
<span class="eqno">(104)<a class="headerlink" href="#equation-eq-categorical-distribution-bishop" title="Permalink to this equation">#</a></span>\[
\mathbb{P}(\mathbf{Y} = \mathbf{y}; \boldsymbol{\pi}) = \prod_{k=1}^K \pi_k^{y_k}
\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{y} = \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_K \end{bmatrix}
\end{split}\]</div>
<p>is an one-hot encoded vector of size <span class="math notranslate nohighlight">\(K\)</span>,</p>
<p>The <span class="math notranslate nohighlight">\(y_k\)</span> is the <span class="math notranslate nohighlight">\(k\)</span>-th element of <span class="math notranslate nohighlight">\(\mathbf{y}\)</span>, and is equal to 1 if <span class="math notranslate nohighlight">\(Y = k\)</span> and 0 otherwise.
The <span class="math notranslate nohighlight">\(\pi_k\)</span> is the <span class="math notranslate nohighlight">\(k\)</span>-th element of <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span>, and is the probability of <span class="math notranslate nohighlight">\(Y = k\)</span>.</p>
<p>This notation alongside with the indicator notation in the previous definition allows us to manipulate
the likelihood function easier.</p>
</section>
</div><div class="proof example admonition" id="categorical-distribution-example">
<p class="admonition-title"><span class="caption-number">Example 31 </span> (Categorical Distribution Example)</p>
<section class="example-content" id="proof-content">
<p>Consider rolling a fair six-sided die. Let <span class="math notranslate nohighlight">\(Y\)</span> be the random variable that represents the outcome
of the dice roll. Then <span class="math notranslate nohighlight">\(Y\)</span> follows a categorical distribution with parameters <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span> where <span class="math notranslate nohighlight">\(\pi_k = \frac{1}{6}\)</span> for <span class="math notranslate nohighlight">\(k = 1, 2, \cdots, 6\)</span>.</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}(Y = k) = \frac{1}{6} \quad \text{for } k = 1, 2, \cdots, 6
\]</div>
<p>For example, if we roll a 3, then <span class="math notranslate nohighlight">\(\mathbb{P}(Y = 3) = \frac{1}{6}\)</span>.</p>
<p>With the more compact notation, the indicator function is <span class="math notranslate nohighlight">\(I\{Y = k\} = 1\)</span> if <span class="math notranslate nohighlight">\(Y = 3\)</span> and <span class="math notranslate nohighlight">\(0\)</span> otherwise. Therefore, the PMF is</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}(Y = k) = \prod_{k=1}^6 \frac{1}{6}^{I\{Y = k\}} = \left(\frac{1}{6}\right)^0 \cdot \left(\frac{1}{6}\right)^0 \cdot \left(\frac{1}{6}\right)^1 \cdot \left(\frac{1}{6}\right)^0 \cdot \left(\frac{1}{6}\right)^0 \cdot \left(\frac{1}{6}\right)^0 = \frac{1}{6}
\]</div>
<p>Using Bishopâ€™s notation, the PMF is still the same, only the realization <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> is not a scalar,
but instead a vector of size <span class="math notranslate nohighlight">\(6\)</span>. In the case where <span class="math notranslate nohighlight">\(Y = 3\)</span>, the vector <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{y} = \begin{bmatrix} 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0 \end{bmatrix}
\end{split}\]</div>
</section>
</div></section>
<section id="discrete-features-categorical-distribution">
<h3>Discrete Features (Categorical Distribution)<a class="headerlink" href="#discrete-features-categorical-distribution" title="Permalink to this heading">#</a></h3>
<p>Now, our next task is find parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}\)</span> to model the conditional distribution of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> given <span class="math notranslate nohighlight">\(Y = k\)</span>,
and consequently, recovering the matrix <span class="math notranslate nohighlight">\(\mathbf{M_3}\)</span> defined in <a class="reference internal" href="#equation-eq-naive-bayes-m3">(90)</a>.</p>
<p>In the case where (all) the features <span class="math notranslate nohighlight">\(X_d\)</span> are categorical (<span class="math notranslate nohighlight">\(D\)</span> number of features),
i.e. <span class="math notranslate nohighlight">\(X_d \in \{1, 2, \cdots, C\}\)</span>,
we can use the categorical distribution to model the (<span class="math notranslate nohighlight">\(D\)</span>-dimensional) conditional
distribution of <span class="math notranslate nohighlight">\(\mathbf{X} \in \mathbb{R}^{D}\)</span> given <span class="math notranslate nohighlight">\(Y = k\)</span>.</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-categorical-feature-1">
<span class="eqno">(105)<a class="headerlink" href="#equation-eq-naive-bayes-categorical-feature-1" title="Permalink to this equation">#</a></span>\[
\begin{align*}
\mathbf{X} \mid Y = k &amp;\overset{\small{\text{i.i.d.}}}{\sim} \text{Category}\left(\boldsymbol{\pi}_{\{\mathbf{X} \mid Y\}}\right) \quad \text{for } k = 1, 2, \cdots, K
\end{align*}
\]</div>
<p>where</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-categorical-feature-2">
<span class="eqno">(106)<a class="headerlink" href="#equation-eq-naive-bayes-categorical-feature-2" title="Permalink to this equation">#</a></span>\[\begin{split}
\boldsymbol{\pi}_{\{\mathbf{X} \mid Y\}} = \begin{bmatrix} \pi_{1, 1} &amp; \dots &amp; \pi_{1, D} \\ \vdots &amp; \ddots &amp; \vdots \\ \pi_{K, 1} &amp; \dots &amp; \pi_{K, D} \end{bmatrix}_{K \times D} \\
\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\boldsymbol{\pi}_{\{\mathbf{X} \mid Y\}}\)</span> is a matrix of size <span class="math notranslate nohighlight">\(K \times D\)</span> where each
element <span class="math notranslate nohighlight">\(\pi_{k, d}\)</span> is the parameter for the
probability distribution (PDF) of <span class="math notranslate nohighlight">\(X_d\)</span> given <span class="math notranslate nohighlight">\(Y = k\)</span>.</p>
<div class="math notranslate nohighlight">
\[
X_d \mid Y = k \overset{\small{\text{i.i.d.}}}{\sim} \text{Category}(\pi_{k, d})
\]</div>
<p>Furthermore,
each <span class="math notranslate nohighlight">\(\pi_{k, d}\)</span> is <strong>not a scalar</strong> but a <strong>vector of size <span class="math notranslate nohighlight">\(C\)</span></strong> holding the probability of <span class="math notranslate nohighlight">\(X_d = c\)</span> given <span class="math notranslate nohighlight">\(Y = k\)</span>.</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
\pi_{k, d} = \begin{bmatrix} \pi_{k, d, 1} &amp; \dots &amp; \pi_{k, d, C} \end{bmatrix}
\end{align*}
\]</div>
<p>Then the (chained) multi-dimensional conditional PDF of <span class="math notranslate nohighlight">\(\mathbf{X} = \begin{bmatrix} X_1 &amp; \dots &amp; X_D \end{bmatrix}\)</span> given <span class="math notranslate nohighlight">\(Y = k\)</span> is</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-categorical-feature-3">
<span class="eqno">(107)<a class="headerlink" href="#equation-eq-naive-bayes-categorical-feature-3" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{align*}
\mathbb{P}(\mathbf{X} = \mathbf{x} | Y = k ; \boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}) &amp;= \prod_{d=1}^D \text{Categorical}(X_d \mid Y = k; \pi_{k, d}) \\
&amp;= \prod_{d=1}^D \prod_{c=1}^C \pi_{k, d, c}^{x_{c, d}} \quad \text{for } c = 1, 2, \cdots, C \text{ and } k = 1, 2, \cdots, K
\end{align*}
\end{split}\]</div>
<p>As an example, if <span class="math notranslate nohighlight">\(C=3\)</span>, <span class="math notranslate nohighlight">\(D=2\)</span> and <span class="math notranslate nohighlight">\(K=4\)</span>, then the <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}\)</span> is a <span class="math notranslate nohighlight">\(K \times D = 4 \times 2\)</span> matrix, but for
each entry <span class="math notranslate nohighlight">\(\pi_{k, d}\)</span>, is a <span class="math notranslate nohighlight">\(1 \times C\)</span> vector. If one really wants, we can also represent this as a
<span class="math notranslate nohighlight">\(4 \times 2 \times 3\)</span> tensor, especially in the case of implementing it in code.</p>
<p>To be more verbose, when we find</p>
<div class="math notranslate nohighlight">
\[
\mathbf{X} \mid Y = k \overset{\small{\text{i.i.d.}}}{\sim} \text{Category}(\boldsymbol{\pi}_{\{\mathbf{X} \mid Y\}})
\]</div>
<p>we are actually finding for all <span class="math notranslate nohighlight">\(k = 1, 2, \cdots, K\)</span>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\mathbf{X} \mid Y = 1 &amp;\overset{\small{\text{i.i.d.}}}{\sim} \text{Category}(\boldsymbol{\pi}_{\{\mathbf{X} \mid Y=1\}}) \\
\mathbf{X} \mid Y = 2 &amp;\overset{\small{\text{i.i.d.}}}{\sim} \text{Category}(\boldsymbol{\pi}_{\{\mathbf{X} \mid Y=2\}}) \\
\vdots \\
\mathbf{X} \mid Y = K &amp;\overset{\small{\text{i.i.d.}}}{\sim} \text{Category}(\boldsymbol{\pi}_{\{\mathbf{X} \mid Y=K\}})
\end{align*}
\end{split}\]</div>
<p>This is because we are also finding the argmax of the number of classes <span class="math notranslate nohighlight">\(K\)</span> when we seek
the expression <span class="math notranslate nohighlight">\(\arg\max_{k=1, 2, \cdots, K} \mathbb{P}(Y = k | \mathbf{X} = \mathbf{x})\)</span>,
and therefore, we need to find the conditional PDF of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> given <span class="math notranslate nohighlight">\(Y\)</span> for each class <span class="math notranslate nohighlight">\(k\)</span>.</p>
<p>Each row above corresponds to each row of the matrix <span class="math notranslate nohighlight">\(\mathbf{M_2}\)</span> defined in <a class="reference internal" href="#equation-eq-naive-bayes-m2">(89)</a>. We
can further decompose each <span class="math notranslate nohighlight">\(\mathbf{X} \mid Y = k\)</span> into <span class="math notranslate nohighlight">\(D\)</span> independent random variables, each of which
is modeled by a categorical distribution, thereby recovering each element of <span class="math notranslate nohighlight">\(\mathbf{M_3}\)</span> <a class="reference internal" href="#equation-eq-naive-bayes-m3">(90)</a>.</p>
<p>See <strong>Kevin Murphyâ€™s Probabilistic Machine Learning: An Introduction</strong> pp 358 for more details.</p>
</section>
<section id="continuous-features-gaussian-distribution">
<span id="id10"></span><h3>Continuous Features (Gaussian Distribution)<a class="headerlink" href="#continuous-features-gaussian-distribution" title="Permalink to this heading">#</a></h3>
<p>Here, the task is still the same, to find parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}\)</span> to model the conditional distribution of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> given <span class="math notranslate nohighlight">\(Y = k\)</span>,
and consequently, recovering the matrix <span class="math notranslate nohighlight">\(\mathbf{M_3}\)</span> defined in <a class="reference internal" href="#equation-eq-naive-bayes-m3">(90)</a>.</p>
<p>In the case where (all) the features <span class="math notranslate nohighlight">\(X_d\)</span> are continuous (<span class="math notranslate nohighlight">\(D\)</span> number of features),
we can use the Gaussian distribution to model the conditional distribution of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> given <span class="math notranslate nohighlight">\(Y = k\)</span>.</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-continuous-feature-1">
<span class="eqno">(108)<a class="headerlink" href="#equation-eq-naive-bayes-continuous-feature-1" title="Permalink to this equation">#</a></span>\[
\begin{align*}
\mathbf{X} \mid Y = k \overset{\small{\text{i.i.d.}}}{\sim} \mathcal{N}(\theta_{\{\mathbf{X} \mid Y\}}) \quad \text{for } k = 1, 2, \cdots, K
\end{align*}
\]</div>
<p>where</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-continuous-feature-2">
<span class="eqno">(109)<a class="headerlink" href="#equation-eq-naive-bayes-continuous-feature-2" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{align*}
\theta_{\{\mathbf{X} \mid Y\}} &amp;= \begin{bmatrix} \theta_{1, 1} &amp; \dots &amp; \theta_{1, D} \\ \vdots &amp; \ddots &amp; \vdots \\ \theta_{K, 1} &amp; \dots &amp; \theta_{K, D} \end{bmatrix}_{K \times D} \\
&amp;= \begin{bmatrix} (\mu_{1, 1}, \sigma_{1, 1}^2) &amp; \dots &amp; (\mu_{1, D}, \sigma_{1, D}^2) \\ \vdots &amp; \ddots &amp; \vdots \\ (\mu_{K, 1}, \sigma_{K, 1}^2) &amp; \dots &amp; (\mu_{K, D}, \sigma_{K, D}^2) \end{bmatrix}_{K \times D}
\end{align*}
\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}\)</span> is a <span class="math notranslate nohighlight">\(K \times D\)</span> matrix, where each element
<span class="math notranslate nohighlight">\(\theta_{k, d}\)</span> is a tuple of the mean and variance of the
Gaussian distribution modeling the conditional distribution of <span class="math notranslate nohighlight">\(X_d\)</span> given <span class="math notranslate nohighlight">\(Y = k\)</span>.</p>
<p>To be more precise, each element in the matrix <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}\)</span> is a tuple of the mean and variance of the
Gaussian distribution modeling the conditional distribution of <span class="math notranslate nohighlight">\(X_d\)</span> given <span class="math notranslate nohighlight">\(Y = k\)</span>.</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-continuous-feature-3">
<span class="eqno">(110)<a class="headerlink" href="#equation-eq-naive-bayes-continuous-feature-3" title="Permalink to this equation">#</a></span>\[
\begin{align*}
X_d \mid Y = k &amp;\overset{\small{\text{i.i.d.}}}{\sim} \mathcal{N}(\mu_{k, d}, \sigma_{k, d}^2) \quad \text{for } k = 1, 2, \cdots, K
\end{align*}
\]</div>
<p>Then the (chained) multivariate Gaussian distribution of <span class="math notranslate nohighlight">\(\mathbf{X} = \begin{bmatrix} X_1 &amp; \dots &amp; X_D \end{bmatrix}\)</span> given <span class="math notranslate nohighlight">\(Y = k\)</span> is</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-continuous-feature-4">
<span class="eqno">(111)<a class="headerlink" href="#equation-eq-naive-bayes-continuous-feature-4" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{align*}
\mathbb{P}\left(\mathbf{X} = \mathbf{x} \mid Y = k ; \boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}\right) &amp;= \prod_{d=1}^D \mathcal{N}(x_d \mid \mu_{k, d}, \sigma_{k, d}^2) \\
\end{align*}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mu_{k, d}\)</span> and <span class="math notranslate nohighlight">\(\sigma_{k, d}^2\)</span> are the mean and variance of the
Gaussian distribution modeling the conditional distribution of <span class="math notranslate nohighlight">\(X_d\)</span> given <span class="math notranslate nohighlight">\(Y = k\)</span>.</p>
<p>So in this case it amounts to estimating <span class="math notranslate nohighlight">\(\hat{\mu}_{k, d}\)</span> and <span class="math notranslate nohighlight">\(\hat{\sigma}_{k, d}^2\)</span> for each <span class="math notranslate nohighlight">\(k\)</span> and <span class="math notranslate nohighlight">\(d\)</span>.</p>
</section>
<section id="mixed-features-discrete-and-continuous">
<h3>Mixed Features (Discrete and Continuous)<a class="headerlink" href="#mixed-features-discrete-and-continuous" title="Permalink to this heading">#</a></h3>
<p>So far we have assumed that each feature <span class="math notranslate nohighlight">\(X_d\)</span> is either all discrete, or all continuous. This
need not be the case, and may not always be the case. In reality, we may have a mixture of both.</p>
<p>For example, if <span class="math notranslate nohighlight">\(X_1\)</span> corresponds to the smoking status of a person (i.e. whether they smoke or not),
then this feature is binary, and can be modeled by a Bernoulli distribution.
On the other hand, if <span class="math notranslate nohighlight">\(X_2\)</span> corresponds to the weight of a person, then this feature is continuous, and can be modeled by a Gaussian distribution.
The nice thing is since within each class <span class="math notranslate nohighlight">\(k\)</span>, the features <span class="math notranslate nohighlight">\(X_d\)</span> are independent of each other, we can model each feature <span class="math notranslate nohighlight">\(X_d\)</span> by its own distribution.</p>
<p>So, carrying over the example above, we have,</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-mixed-feature-1">
<span class="eqno">(112)<a class="headerlink" href="#equation-eq-naive-bayes-mixed-feature-1" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{align*}
X_1 \mid Y = k &amp;\overset{\small{\text{i.i.d.}}}{\sim} \text{Bernoulli}(\pi_{\{X_1 \mid Y=k\}}) \\
X_2 \mid Y = k &amp;\overset{\small{\text{i.i.d.}}}{\sim} \text{Gaussian}(\mu_{\{X_2 \mid Y=k\}}, \sigma_{\{X_2 \mid Y=k\}}^2)
\end{align*}
\end{split}\]</div>
<p>and subsequently, the chained PDF is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\mathbb{P}\left(X_1 = x_1, X_2 = x_2 \mid Y = k ; \boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}\right) &amp;= \prod_{d=1}^D \mathbb{P}\left(X_d = x_d \mid Y = k ; \boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}\right) \\
&amp;= \mathbb{P}\left(X_1 = x_1 \mid Y = k ; \boldsymbol{\pi}_{\{X_1 \mid Y\}}\right) \mathbb{P}\left(X_2 = x_2 \mid Y = k ; \boldsymbol{\theta}_{\{X_2 \mid Y\}}\right) \\
&amp;= \pi_{\{X_1 \mid Y=k\}}^{x_1} (1 - \pi_{\{X_1 \mid Y=k\}})^{1 - x_1} \mathcal{N}(x_2 \mid \mu_{\{X_2 \mid Y=k\}}, \sigma_{\{X_2 \mid Y=k\}}^2)
\end{align*}
\end{split}\]</div>
<p>See more details in <a class="reference external" href="https://dafriedman97.github.io/mlbook/content/c4/concept.html">Machine Learning from Scratch</a>.</p>
</section>
</section>
<section id="model-fitting">
<h2>Model Fitting<a class="headerlink" href="#model-fitting" title="Permalink to this heading">#</a></h2>
<p>We have so far laid out the model prediction process, the implicit and explicit assumptions, as well as
the model parameters.</p>
<p>Now, we need to figure out how to fit the model parameters to the data. After all, once we
find the model parameters that best fit the data, we can use the model to make predictions
using matrix <span class="math notranslate nohighlight">\(\mathbf{M_1}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{M_3}\)</span> as defined in <a class="reference internal" href="#naive-bayes-inference-algorithm">Algorithm 2</a>.</p>
<section id="fitting-algorithm">
<h3>Fitting Algorithm<a class="headerlink" href="#fitting-algorithm" title="Permalink to this heading">#</a></h3>
<div class="proof algorithm admonition" id="prf:naive-bayes-estimation-algorithm">
<p class="admonition-title"><span class="caption-number">Algorithm 3 </span> (Naive Bayes Estimation Algorithm)</p>
<section class="algorithm-content" id="proof-content">
<p>For each entry in matrix <span class="math notranslate nohighlight">\(\mathbf{M_1}\)</span>, we seek to find its corresponding estimated parameter vector <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\pi}}\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-estimation-1">
<span class="eqno">(113)<a class="headerlink" href="#equation-eq-naive-bayes-estimation-1" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{align*}
\hat{\boldsymbol{\pi}} &amp;= \begin{bmatrix} \hat{\pi}_1 \\ \vdots \\ \hat{\pi}_K \end{bmatrix}_{K \times 1} \\
\end{align*}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{\pi}_k\)</span> is the estimated (empirical) probability of class <span class="math notranslate nohighlight">\(k\)</span>.</p>
<p>For each entry in matrix <span class="math notranslate nohighlight">\(\mathbf{M_3}\)</span>, we seek to find its corresponding estimated parameter matrix <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\theta}}_{\{\mathbf{X} \mid Y\}}\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-estimation-2">
<span class="eqno">(114)<a class="headerlink" href="#equation-eq-naive-bayes-estimation-2" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{align*}
\hat{\boldsymbol{\theta}}_{\{\mathbf{X} \mid Y\}} &amp;= \begin{bmatrix} \hat{\boldsymbol{\theta}}_{\{\mathbf{X} \mid Y=1\}} \\ \vdots \\ \hat{\boldsymbol{\theta}}_{\{\mathbf{X} \mid Y=K\}} \end{bmatrix}_{K \times D} \\
&amp;= \begin{bmatrix} \hat{\theta}_{\{X_1 \mid Y=1\}} &amp; \cdots &amp; \hat{\theta}_{\{X_D \mid Y=1\}} \\ \vdots &amp; \ddots &amp; \vdots \\ \hat{\theta}_{\{X_1 \mid Y=K\}} &amp; \cdots &amp; \hat{\theta}_{\{X_D \mid Y=K\}} \end{bmatrix}_{K \times D} \\
&amp;= \begin{bmatrix} \hat{\theta}_{11} &amp; \cdots &amp; \hat{\theta}_{1D} \\ \vdots &amp; \ddots &amp; \vdots \\ \hat{\theta}_{K1} &amp; \cdots &amp; \hat{\theta}_{KD} \end{bmatrix}_{K \times D} \\
\end{align*}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{\theta}_{kd}\)</span> is the probability of feature <span class="math notranslate nohighlight">\(X_d\)</span> given class <span class="math notranslate nohighlight">\(k\)</span>.</p>
<p>Both the underlying distribution <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}\)</span> are estimated by maximizing the likelihood of the data,
using the Maximum Likelihood Estimation (MLE) method to obtain the maximum likelihood estimates (MLEs), which are denoted by <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\pi}}\)</span> and <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\theta}}_{\{\mathbf{X} \mid Y\}}\)</span> respectively.</p>
</section>
</div></section>
<section id="maximum-likelihood-estimation">
<h3>Maximum Likelihood Estimation<a class="headerlink" href="#maximum-likelihood-estimation" title="Permalink to this heading">#</a></h3>
<p>First, read chapter 8.1 of <span id="id11">[<a class="reference internal" href="../../../references_resources_roadmap/bibliography.html#id2" title="Stanley H. Chan. Introduction to probability for Data Science. Michigan Publishing, 2021.">Chan, 2021</a>]</span> for a refresher on MLE.</p>
<div class="proof remark admonition" id="remark-univariate-mle">
<p class="admonition-title"><span class="caption-number">Remark 40 </span> (Univariate Maximum Likelihood Estimation)</p>
<section class="remark-content" id="proof-content">
<p>In <a class="reference external" href="https://dafriedman97.github.io/mlbook/content/c4/concept.html#linear-discriminative-analysis-lda">LDA</a>,
<span class="math notranslate nohighlight">\(\mathbf{X} \mid Y=k\)</span>, the distribution of the features <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> conditioned on <span class="math notranslate nohighlight">\(Y=k\)</span>, has no
assumption of conditional independence. Therefore, we need to estimate the parameters of
<span class="math notranslate nohighlight">\(\mathbf{X} = \{\mathbf{X}_1, \dots, \mathbf{X}_D\}\)</span> jointly.</p>
<p>More concretely,</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
\mathbf{X} \mid Y = k \overset{\text{i.i.d.}}{\sim} \mathcal{N}\left(\boldsymbol{\mu}_{\{X \mid Y=k\}}, \boldsymbol{\Sigma}_{\{X \mid Y=k\}}\right) \quad \forall k \in \{1, \dots, K\}
\end{align*}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_{\{X \mid Y=k\}}\)</span> is the mean vector of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> given <span class="math notranslate nohighlight">\(Y=k\)</span>, and <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}_{\{X \mid Y=k\}}\)</span> is the covariance matrix of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> given <span class="math notranslate nohighlight">\(Y=k\)</span>.</p>
<p>However, in the case of Naive Bayes, the assumption of conditional independence allows us to estimate the parameters of <span class="math notranslate nohighlight">\(\mathbf{X} = \{\mathbf{X}_1, \dots, \mathbf{X}_D\}\)</span> univariately,
conditional on <span class="math notranslate nohighlight">\(Y=k\)</span>.</p>
<p>Looking at expression <a class="reference internal" href="#equation-eq-naive-bayes-estimation-2">(114)</a>, we can see that each element
is indeed univariate, and we can estimate the parameters of each element univariately.</p>
</section>
</div><p>Everything we have talked about is just 1 single sample, and that wonâ€™t work in the realm of
estimating the best parameters that fit the data. Since we are given a dataset <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>
consisting of <span class="math notranslate nohighlight">\(N\)</span> samples, we can estimate the parameters of the model by maximizing the likelihood of the data.</p>
<div class="proof definition admonition" id="def:naive-bayes-likelihood">
<p class="admonition-title"><span class="caption-number">Definition 136 </span> (Likelihood Function of Naive Bayes)</p>
<section class="definition-content" id="proof-content">
<p>Given <strong>i.i.d.</strong> random variables<a class="footnote-reference brackets" href="#iid-tuple" id="id12" role="doc-noteref"><span class="fn-bracket">[</span>9<span class="fn-bracket">]</span></a> <span class="math notranslate nohighlight">\(\left(\mathbf{X}^{(1)}, Y^{(1)}\right), \left(\mathbf{X}^{(2)}, Y^{(2)}\right), \dots, \left(\mathbf{X}^{(N)}, Y^{(N)}\right)\)</span>,
we can write the likelihood function (joint probability distribution)
as the product of the individual PDF of each sample<a class="footnote-reference brackets" href="#iid-likelihood" id="id13" role="doc-noteref"><span class="fn-bracket">[</span>10<span class="fn-bracket">]</span></a>:</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-likelihood-1">
<span class="eqno">(115)<a class="headerlink" href="#equation-eq-naive-bayes-likelihood-1" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{align*}
\mathcal{L}(\boldsymbol{\theta} \mid \mathcal{D}) \overset{\mathrm{def}}{=} \mathbb{P}(\mathcal{D} ; \boldsymbol{\theta}) &amp;= \mathbb{P}\left(\mathcal{D} ; \left\{\boldsymbol{\pi}, \boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}} \right\}\right) \\
&amp;\overset{(a)}{=} \mathbb{P}\left(\left(\mathbf{X}^{(1)}, Y^{(1)}\right), \left(\mathbf{X}^{(2)}, Y^{(2)}\right), \dots, \left(\mathbf{X}^{(N)}, Y^{(N)}\right) ; \left\{\boldsymbol{\pi}, \boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}} \right\}\right) \\
&amp;\overset{(b)}{=} \prod_{n=1}^N  \mathbb{P}\left(Y^{(n)}=k ; \boldsymbol{\pi}\right) \mathbb{P}\left(\mathrm{X}^{(n)} \mid Y^{(n)} = k ; \boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}} \right)  \\
&amp;\overset{(c)}{=} \prod_{n=1}^N  \left\{\mathbb{P}\left(Y^{(n)}=k ; \boldsymbol{\pi}\right) \prod_{d=1}^D \mathbb{P}\left(X_d^{(n)} \mid Y^{(n)} = k ; \boldsymbol{\theta}_{k, d}\right) \right\} \\
\end{align*}
\end{split}\]</div>
<p>where each <span class="math notranslate nohighlight">\(\left(\mathbf{X}^{(n)}, Y^{(n)}\right)\)</span> in equation <span class="math notranslate nohighlight">\((a)\)</span> is a sample from the dataset <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>
and can be expressed more verbosely as a joint distribution <span class="math notranslate nohighlight">\(\left(\mathbf{X}^{(n)}, Y^{(n)}\right) = \left(\mathbf{X}_1^{(n)}, \dots, \mathbf{X}_D^{(n)}, Y^{(n)}\right)\)</span>
as in <a class="reference internal" href="#equation-eq-joint-distribution">(94)</a>.</p>
<p>Equation <span class="math notranslate nohighlight">\((b)\)</span> is the product of the individual PDF of each sample, where the multiplicand is as in <a class="reference internal" href="#equation-eq-joint-distribution">(94)</a>.</p>
<p>Equation <span class="math notranslate nohighlight">\((c)\)</span> is then a consequence of <a class="reference internal" href="#equation-eq-conditional-independence-naive-bayes-1">(97)</a>.</p>
</section>
</div><p>Then we can maximize</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-likelihood-target">
<span class="eqno">(116)<a class="headerlink" href="#equation-eq-naive-bayes-likelihood-target" title="Permalink to this equation">#</a></span>\[
\prod_{n=1}^N  \mathbb{P}\left(Y^{(n)}=k ; \boldsymbol{\pi}\right)
\]</div>
<p>and</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-likelihood-feature">
<span class="eqno">(117)<a class="headerlink" href="#equation-eq-naive-bayes-likelihood-feature" title="Permalink to this equation">#</a></span>\[
\prod_{n=1}^N  \prod_{d=1}^D \mathbb{P}\left(X_d^{(n)} \middle \vert Y^{(n)} = k ; \boldsymbol{\theta}_{k, d}\right)
\]</div>
<p>individually since the above can be decomposed<a class="footnote-reference brackets" href="#decomposed-likelihood" id="id14" role="doc-noteref"><span class="fn-bracket">[</span>11<span class="fn-bracket">]</span></a>.</p>
<div class="proof definition admonition" id="def:naive-bayes-log-likelihood">
<p class="admonition-title"><span class="caption-number">Definition 137 </span> (Log Likelihood Function of Naive Bayes)</p>
<section class="definition-content" id="proof-content">
<p>For numerical stability, we can take the log of the likelihood function:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\log \mathcal{L}(\boldsymbol{\theta} \mid \mathcal{D}) &amp;= \log \mathbb{P}\left(\mathcal{D} ; \left\{\boldsymbol{\pi}, \boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}} \right\}\right) \\
\end{align*}
\end{split}\]</div>
<p>where the log of the product of the individual PDF of each sample is the sum of the log of each PDF. We
will go into that later.</p>
</section>
</div><p>Stated formally,</p>
<div class="proof definition admonition" id="def:naive-bayes-max-priors">
<p class="admonition-title"><span class="caption-number">Definition 138 </span> (Maximize Priors)</p>
<section class="definition-content" id="proof-content">
<p>The notation for maximizing the prior probabilities is as follows:</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-max-priors">
<span class="eqno">(118)<a class="headerlink" href="#equation-eq-naive-bayes-max-priors" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{align*}
\hat{\boldsymbol{\pi}} &amp;= \arg \max_{\boldsymbol{\pi}} \mathcal{L}(\boldsymbol{\pi} ; \mathcal{D}) \\
&amp;= \arg \max_{\boldsymbol{\pi}} \prod_{n=1}^N  \mathbb{P}\left(Y^{(n)}=k ; \boldsymbol{\pi}\right) \\
\end{align*}
\end{split}\]</div>
<p>A reminder that the shape of <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\pi}}\)</span> is <span class="math notranslate nohighlight">\(K \times 1\)</span>.</p>
</section>
</div><p>Similarly, we can maximize the likelihood function of the feature parameters:</p>
<div class="proof definition admonition" id="def:naive-bayes-max-feature-params">
<p class="admonition-title"><span class="caption-number">Definition 139 </span> (Maximize Feature Parameters)</p>
<section class="definition-content" id="proof-content">
<p>The notation for maximizing the feature parameters is as follows:</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-max-feature-params">
<span class="eqno">(119)<a class="headerlink" href="#equation-eq-naive-bayes-max-feature-params" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{align*}
\hat{\boldsymbol{\theta}}_{\{\mathbf{X} \mid Y\}} &amp;= \arg \max_{\boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}} \mathcal{L}(\boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}} ; \mathcal{D}) \\
&amp;= \arg \max_{\boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}} \prod_{n=1}^N  \prod_{d=1}^D \mathbb{P}\left(X_d^{(n)} \mid Y^{(n)} = k ; \boldsymbol{\theta}_{k, d}\right) \\
\end{align*}
\end{split}\]</div>
<p>A reminder that the shape of <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\theta}}_{\{\mathbf{X} \mid Y\}}\)</span> is <span class="math notranslate nohighlight">\(K \times D\)</span>.</p>
</section>
</div></section>
<section id="estimating-priors">
<h3>Estimating Priors<a class="headerlink" href="#estimating-priors" title="Permalink to this heading">#</a></h3>
<p>Before we start the formal estimation process, it is intuitive to think that the prior probabilities <span class="math notranslate nohighlight">\(\boldsymbol{\pi}_k\)</span> should be proportional to the number of samples in each class. In other words, if we have <span class="math notranslate nohighlight">\(N_1\)</span> samples in class 1, <span class="math notranslate nohighlight">\(N_2\)</span> samples in class 2, and so on, then we should have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\pi_1 &amp;\propto N_1 \\
\pi_2 &amp;\propto N_2 \\
\vdots &amp; \quad \vdots \\
\pi_K &amp;\propto N_K
\end{align*}
\end{split}\]</div>
<p>For instance, if we have a dataset with <span class="math notranslate nohighlight">\(N=100\)</span> samples with <span class="math notranslate nohighlight">\(K=3\)</span> classes, and <span class="math notranslate nohighlight">\(N_1 = 10\)</span>, <span class="math notranslate nohighlight">\(N_2 = 30\)</span> and <span class="math notranslate nohighlight">\(N_3 = 60\)</span>, then we should have <span class="math notranslate nohighlight">\(\pi_1 = \frac{10}{100} = 0.1\)</span>, <span class="math notranslate nohighlight">\(\pi_2 = \frac{30}{100} = 0.3\)</span> and <span class="math notranslate nohighlight">\(\pi_3 = \frac{60}{100} = 0.6\)</span>. This is just the relative frequency of each class and
seems to be a sensible choice.</p>
<p>It turns out our intuition matches the formal estimation process derived from the maximum likelihood estimation (MLE) principle.</p>
</section>
<section id="maximum-likelihood-estimation-for-priors-categorical-distribution">
<h3>Maximum Likelihood Estimation for Priors (Categorical Distribution)<a class="headerlink" href="#maximum-likelihood-estimation-for-priors-categorical-distribution" title="Permalink to this heading">#</a></h3>
<p>We have seen earlier that we can maximize the priors and likelihood (target and feature parameters) separately.</p>
<p>Letâ€™s start with the priors. Letâ€™s state the expression from <a class="reference internal" href="#equation-eq-naive-bayes-max-priors">(118)</a> in definition
<a class="reference internal" href="#def:naive-bayes-max-priors">Definition 138</a> again:</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-max-priors-repeated">
<span class="eqno">(120)<a class="headerlink" href="#equation-eq-naive-bayes-max-priors-repeated" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{align*}
\hat{\boldsymbol{\pi}} &amp;= \arg \max_{\boldsymbol{\pi}} \mathcal{L}(\boldsymbol{\pi} ; \mathcal{D}) \\
&amp;= \arg \max_{\boldsymbol{\pi}} \prod_{n=1}^N  \mathbb{P}\left(Y^{(n)}=k ; \boldsymbol{\pi}\right) \\
\end{align*}
\end{split}\]</div>
<p>We need to write the multiplicand in <a class="reference internal" href="#equation-eq-naive-bayes-max-priors-repeated">(120)</a> in terms of
the PDF of the Category distribution, as decribed in <a class="reference internal" href="#equation-eq-categorical-distribution-bishop">(104)</a>.
Extending from <a class="reference internal" href="#equation-eq-naive-bayes-max-priors-repeated">(120)</a>, we have:</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-max-priors-2">
<span class="eqno">(121)<a class="headerlink" href="#equation-eq-naive-bayes-max-priors-2" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{align*}
\hat{\boldsymbol{\pi}} &amp;= \arg \max_{\boldsymbol{\pi}} \mathcal{L}(\boldsymbol{\pi} ; \mathcal{D}) \\
&amp;= \arg \max_{\boldsymbol{\pi}} \prod_{n=1}^N  \mathbb{P}\left(Y^{(n)}=k ; \boldsymbol{\pi}\right) \\
&amp;\overset{\mathrm{(a)}}{=} \arg \max_{\boldsymbol{\pi}} \prod_{n=1}^N  \left(\prod_{k=1}^K \pi_k^{y^{(n)}_k}\right) \\
\end{align*}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\left(\prod_{k=1}^K \pi_k^{y^{(n)}_k} \right)\)</span> in equation <span class="math notranslate nohighlight">\((a)\)</span> is a consequence
of the definition of the Category distribution in <a class="reference internal" href="#categorical-multinomial-distribution">Definition 135</a>.</p>
<p>Subsequently, knowing maximizing the log likelihood is the same as maximizing the likelihood, we have:</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-max-priors-3">
<span class="eqno">(122)<a class="headerlink" href="#equation-eq-naive-bayes-max-priors-3" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{align*}
\hat{\boldsymbol{\pi}} &amp;= \arg \max_{\boldsymbol{\pi}} \mathcal{L}(\boldsymbol{\pi} ; \mathcal{D}) \\
&amp;= \arg \max_{\boldsymbol{\pi}} \log \mathcal{L}(\boldsymbol{\pi} ; \mathcal{D}) \\
&amp;\overset{\mathrm{(b)}}{=} \arg \max_{\boldsymbol{\pi}} \sum_{n=1}^N \log \left(\prod_{k=1}^K \pi_k^{y^{(n)}_k} \right) \\
&amp;\overset{\mathrm{(c)}}{=} \arg \max_{\boldsymbol{\pi}} \sum_{n=1}^N \sum_{k=1}^K y^{(n)}_k \log \pi_k \\
&amp;\overset{\mathrm{(d)}}{=} \arg \max_{\boldsymbol{\pi}} \sum_{k=1}^K N_k \log \pi_k \\
\end{align*}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(N_k\)</span> is the number of samples that belong to the <span class="math notranslate nohighlight">\(k\)</span>-th category.</p>
<div class="proof remark admonition" id="notation-overload">
<p class="admonition-title"><span class="caption-number">Remark 41 </span> (Notation Overload)</p>
<section class="remark-content" id="proof-content">
<p>We note to ourselves that we are reusing, and hence abusing the notation <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> for the log-likelihood function to be the same as the likelihood function, this is just for the ease of re-defining a new symbol for the log-likelihood function, <span class="math notranslate nohighlight">\(\log \mathcal{L}\)</span>.</p>
</section>
</div><p>Equation <span class="math notranslate nohighlight">\((b)\)</span> is derived because placing the logarithm outside the product is equivalent to summing the logarithms of the terms in the product.</p>
<p>Equation <span class="math notranslate nohighlight">\((d)\)</span> is derived by expanding equation <span class="math notranslate nohighlight">\((c)\)</span>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\sum_{n=1}^N \sum_{k=1}^K y^{(n)}_k \log \pi_k &amp;= \sum_{n=1}^N \left( \sum_{k=1}^K y^{(n)}_k \log \pi_k \right) \\
&amp;= y^{(1)}_1 \log \pi_1 + y^{(1)}_2 \log \pi_2 + \dots + y^{(1)}_K \log \pi_K \\
&amp;+ y^{(2)}_1 \log \pi_1 + y^{(2)}_2 \log \pi_2 + \dots + y^{(2)}_K \log \pi_K \\
&amp;+ \qquad \vdots \qquad \\
&amp;+ y^{(N)}_1 \log \pi_1 + y^{(N)}_2 \log \pi_2 + \dots + y^{(N)}_K \log \pi_K \\
&amp;\overset{(e)}{=} \left( y^{(1)}_1 + y^{(2)}_1 + \dots + y^{(N)}_1 \right) \log \pi_1 \\
&amp;+ \left( y^{(1)}_2 + y^{(2)}_2 + \dots + y^{(N)}_2 \right) \log \pi_2 \\
&amp;+ \qquad \vdots \qquad \\
&amp;+ \left( y^{(1)}_K + y^{(2)}_K + \dots + y^{(N)}_K \right) \log \pi_K \\
&amp;\overset{(f)}{=} N_1 \log \pi_1 + N_2 \log \pi_2 + \dots + N_K \log \pi_K \\
&amp;= \sum_{k=1}^K N_k \log \pi_k \\
\end{align*}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\((e)\)</span> is derived by summing each column, and <span class="math notranslate nohighlight">\(N_k = y^{(1)}_k + y^{(2)}_k + \dots + y^{(N)}_k\)</span>
is nothing but the number of samples that belong to the <span class="math notranslate nohighlight">\(k\)</span>-th category. One just need to recall that
if we have say 6 samples of class <span class="math notranslate nohighlight">\((0, 1, 2, 0, 1, 1)\)</span> where <span class="math notranslate nohighlight">\(K=3\)</span>, then the one-hot encoded
representation of the samples will be</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\left[
\begin{array}{ccc}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1 \\
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
\end{array}
\right]
\end{align*}
\end{split}\]</div>
<p>and summing each column will give us <span class="math notranslate nohighlight">\(N_1 = 2\)</span>, <span class="math notranslate nohighlight">\(N_2 = 3\)</span>, and <span class="math notranslate nohighlight">\(N_3 = 1\)</span>.</p>
<p>Now we are finally ready to solve the estimation (optimization) problem for <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span>.</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-max-priors-4">
<span class="eqno">(123)<a class="headerlink" href="#equation-eq-naive-bayes-max-priors-4" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{align*}
\hat{\boldsymbol{\pi}} &amp;= \underset{\boldsymbol{\pi}}{\mathrm{argmax}} ~~ \sum_{k=1}^K N_k \log \pi_k \\
\end{align*}
\end{split}\]</div>
<p>subject to the constraint that</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-max-priors-constraint">
<span class="eqno">(124)<a class="headerlink" href="#equation-eq-naive-bayes-max-priors-constraint" title="Permalink to this equation">#</a></span>\[
\sum_{k=1}^K \pi_k = 1
\]</div>
<p>which is just saying the probabilities must sum up to 1.</p>
<p>We can also write the expression as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\max_{\boldsymbol{\pi}} &amp;~~ \sum_{k=1}^K N_k \log \pi_k \\
\text{subject to} &amp;~~ \sum_{k=1}^K \pi_k = 1
\end{aligned}
\end{split}\]</div>
<p>This is a constrained optimization problem, and we can solve it using the Lagrangian method.</p>
<div class="proof definition admonition" id="lagrangian-method">
<p class="admonition-title"><span class="caption-number">Definition 140 </span> (Lagrangian Method)</p>
<section class="definition-content" id="proof-content">
<p>The Lagrangian method is a method to solve constrained optimization problems. The idea is to
convert the constrained optimization problem into an unconstrained optimization problem by
introducing a Lagrangian multiplier <span class="math notranslate nohighlight">\(\lambda\)</span> and then solve the unconstrained optimization
problem.</p>
<p>Given a function <span class="math notranslate nohighlight">\(f(\mathrm{x})\)</span> and a constraint <span class="math notranslate nohighlight">\(g(\mathrm{x}) = 0\)</span>, the Lagrangian function,
<span class="math notranslate nohighlight">\(\mathcal{L}(\mathrm{x}, \lambda)\)</span> is defined as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\mathcal{L}(\mathrm{x}, \lambda) &amp;= f(\mathrm{x}) - \lambda g(\mathrm{x}) \\
\end{align*}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\lambda\)</span> is the Lagrangian multiplier and may be either positive or negative. Then,
the critical points of the Lagrangian function are the same as the critical points of the
original constrained optimization problem, i.e. setting the gradient vector of the Lagrangian
function <span class="math notranslate nohighlight">\(\nabla \mathcal{L}(\mathrm{x}, \lambda) = 0\)</span> with respect to <span class="math notranslate nohighlight">\(\mathrm{x}\)</span> and <span class="math notranslate nohighlight">\(\lambda\)</span>.</p>
</section>
</div><p>One note is that the notation of <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> seems to be overloaded again with the Lagrangian function,
we will have to change it to <span class="math notranslate nohighlight">\(\mathcal{L}_\lambda\)</span> to avoid confusion. So, to reiterate, solving the Lagrangian function is equivalent to solving the constrained optimization problem.</p>
<p>In our problem, we can convert it to Lagrangian form as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\hat{\boldsymbol{\pi}} &amp;= \underset{\boldsymbol{\pi}}{\mathrm{argmax}} ~~ \mathcal{L}_\lambda(\boldsymbol{\pi}, \lambda ; \mathcal{D}) \\
&amp;= \underset{\boldsymbol{\pi}}{\mathrm{argmax}} ~~ \underbrace{\mathcal{L}(\boldsymbol{\pi} ; \mathcal{D})}_{f(\boldsymbol{\pi})} - \lambda \left(\underbrace{\sum_{k=1}^K \pi_k - 1}_{g(\boldsymbol{\pi})} \right) \\
&amp;= \underset{\boldsymbol{\pi}}{\mathrm{argmax}} ~~ \sum_{k=1}^K N_k \log \pi_k - \lambda \left( \sum_{k=1}^K \pi_k - 1 \right) \\
\end{align*}
\end{split}\]</div>
<p>which is now an unconstrained optimization problem. Note that we used subtraction instead of addition form of the
Lagrangian function, so that we can frame it as a maximization problem (i.e. we want to reduce the â€œadditional costâ€
<span class="math notranslate nohighlight">\(\lambda\)</span>, which is a positive number, so if we add it, the expression will become a min-max problem,
we can just put a minus sign, so it become a max-max problem).</p>
<p>We can now solve it by setting the gradient vector of the Lagrangian function</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-max-priors-lagrangian-1">
<span class="eqno">(125)<a class="headerlink" href="#equation-eq-naive-bayes-max-priors-lagrangian-1" title="Permalink to this equation">#</a></span>\[
\nabla \mathcal{L}_\lambda(\boldsymbol{\pi}, \lambda ; \mathcal{D}) = 0
\]</div>
<p>with respect to <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span> and <span class="math notranslate nohighlight">\(\lambda\)</span>, as follows,</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-max-priors-lagrangian-2">
<span class="eqno">(126)<a class="headerlink" href="#equation-eq-naive-bayes-max-priors-lagrangian-2" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{align*}
\nabla \mathcal{L}_\lambda(\boldsymbol{\pi}, \lambda ; \mathcal{D}) &amp;\overset{\mathrm{def}}{=} \frac{\partial \mathcal{L}_\lambda(\boldsymbol{\pi}, \lambda ; \mathcal{D})}{\partial \boldsymbol{\pi}} = 0 \quad \text{and} \quad \frac{\partial \mathcal{L}_\lambda(\boldsymbol{\pi}, \lambda ; \mathcal{D})}{\partial \lambda} = 0 \\
&amp;\iff \frac{\partial}{\partial \boldsymbol{\pi}} \left( \sum_{k=1}^K N_k \log \pi_k - \lambda \left( \sum_{k=1}^K \pi_k - 1 \right) \right) = 0 \quad \text{and} \quad \frac{\partial \mathcal{L}_\lambda(\boldsymbol{\pi}, \lambda ; \mathcal{D})}{\partial \lambda} = 0 \\
\\
&amp;\iff \begin{bmatrix} \frac{\partial \mathcal{L}_\lambda}{\partial \pi_1} \\ \vdots \\ \frac{\partial \mathcal{L}_\lambda}{\partial \pi_K} \end{bmatrix} = \begin{bmatrix} 0 \\ \vdots \\ 0 \end{bmatrix} \quad \text{and} \quad \frac{\partial \mathcal{L}_\lambda(\boldsymbol{\pi}, \lambda ; \mathcal{D})}{\partial \lambda} = 0 \\
&amp;\iff \begin{bmatrix} \frac{\partial}{\partial \pi_1} \left( N_1 \log \pi_1 - \lambda \left( \pi_1 - 1 \right) \right) \\ \vdots \\ \frac{\partial}{\partial \pi_K} \left( N_K \log \pi_K - \lambda \left( \pi_K - 1 \right) \right) \end{bmatrix} = \begin{bmatrix} 0 \\ \vdots \\ 0 \end{bmatrix} \quad \text{and} \quad \frac{\partial \mathcal{L}_\lambda(\boldsymbol{\pi}, \lambda ; \mathcal{D})}{\partial \lambda} = 0 \\
&amp;\iff \begin{bmatrix} \frac{N_1}{\pi_1} - \lambda \\ \vdots \\ \frac{N_K}{\pi_K} - \lambda \end{bmatrix} = \begin{bmatrix} 0 \\ \vdots \\ 0 \end{bmatrix} \quad \text{and} \quad \sum_{k=1}^K \pi_k - 1 = 0 \\
\end{align*}
\end{split}\]</div>
<p>The reason we can unpack <span class="math notranslate nohighlight">\(\frac{\partial}{\partial \pi_k}\left( \sum_{k=1}^K N_k \log \pi_k - \lambda \left( \sum_{k=1}^K \pi_k - 1 \right) \right)\)</span> as <span class="math notranslate nohighlight">\(\frac{\partial}{\partial \pi_k} \left( N_k \log \pi_k - \lambda \left( \pi_k - 1 \right) \right)\)</span> is because we are dealing with partial derivatives, so other terms other than <span class="math notranslate nohighlight">\(\pi_k\)</span> are constant.</p>
<p>Finally, we have a system of equations for each <span class="math notranslate nohighlight">\(\pi_k\)</span> and if we can solve for <span class="math notranslate nohighlight">\(\pi_k\)</span> for each <span class="math notranslate nohighlight">\(k\)</span>, we can then find the best estimate of <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span>. It turns out to solve for <span class="math notranslate nohighlight">\(\pi_k\)</span>, we have to find <span class="math notranslate nohighlight">\(\lambda\)</span> first, and this can be solved by setting <span class="math notranslate nohighlight">\(\sum_{k=1}^K \pi_k - 1 = 0\)</span> and solving for <span class="math notranslate nohighlight">\(\lambda\)</span>, which is the last equation in the system of equations above. We first express each <span class="math notranslate nohighlight">\(\pi_k\)</span> in terms of <span class="math notranslate nohighlight">\(\lambda\)</span>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\frac{N_1}{\pi_1} - \lambda &amp;= 0 \implies \pi_1 = \frac{N_1}{\lambda} \\
\frac{N_2}{\pi_2} - \lambda &amp;= 0 \implies \pi_2 = \frac{N_2}{\lambda} \\
\vdots \\
\frac{N_K}{\pi_K} - \lambda &amp;= 0 \implies \pi_K = \frac{N_K}{\lambda} \\
\end{align*}
\end{split}\]</div>
<p>Then we substitute these expressions into the last equation in the system of equations above, and solve for <span class="math notranslate nohighlight">\(\lambda\)</span>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\sum_{k=1}^K \pi_k - 1 = 0 &amp;\implies \sum_{k=1}^K \frac{N_k}{\lambda} - 1 = 0 \\
&amp;\implies \sum_{k=1}^K \frac{N_k}{\lambda} = 1 \\
&amp;\implies \sum_{k=1}^K N_k = \lambda \\
&amp;\implies \lambda = \sum_{k=1}^K N_k \\
&amp;\implies \lambda = N \\
\end{align*}
\end{split}\]</div>
<p>and therefore, we can now solve for <span class="math notranslate nohighlight">\(\pi_k\)</span>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{\hat{\pi}} = \begin{bmatrix}
\pi_1 = \frac{N_1}{N} \\ \pi_2 = \frac{N_2}{N} \\ \vdots \\ \pi_K = \frac{N_K}{N}
\end{bmatrix}_{K \times 1}
\implies \pi_k = \frac{N_k}{N} \quad \text{for} \quad k = 1, 2, \ldots, K
\end{split}\]</div>
<p>We conclude that the maximum likelihood estimate of <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span>
is the same as the empirical relative frequency of each class in the training data. This coincides with our intuition.</p>
<p>For completeness of expression,</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-max-priors-final">
<span class="eqno">(127)<a class="headerlink" href="#equation-eq-naive-bayes-max-priors-final" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{align*}
\hat{\boldsymbol{\pi}} &amp;= \arg \max_{\boldsymbol{\pi}} \mathcal{L}\left( \boldsymbol{\pi} ; \mathcal{D} \right) \\
&amp;= \begin{bmatrix} \hat{\pi}_1 \\ \vdots \\ \hat{\pi}_K \end{bmatrix} \\
&amp;= \begin{bmatrix} \frac{N_1}{N} \\ \vdots \\ \frac{N_K}{N} \end{bmatrix}
\end{align*}
\end{split}\]</div>
</section>
<section id="estimating-likelihood-gaussian-version">
<h3>Estimating Likelihood (Gaussian Version)<a class="headerlink" href="#estimating-likelihood-gaussian-version" title="Permalink to this heading">#</a></h3>
<p>Intuition: The likelihood parameters are the mean and variance of each feature for each class.</p>
<section id="maximum-likelihood-estimate-for-likelihood-continuous-feature-parameters">
<h4>Maximum Likelihood Estimate for Likelihood (Continuous Feature Parameters)<a class="headerlink" href="#maximum-likelihood-estimate-for-likelihood-continuous-feature-parameters" title="Permalink to this heading">#</a></h4>
<p>Now that we have found the maximum likelihood estimate for the prior probabilities,
we now find the maximum likelihood estimate for the likelihood parameters.</p>
<p>Letâ€™s look at the expression <a class="reference internal" href="#equation-eq-naive-bayes-max-feature-params">(119)</a> from <a class="reference internal" href="#def:naive-bayes-max-feature-params">Definition 139</a> again:</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-max-feature-params-repeated">
<span class="eqno">(128)<a class="headerlink" href="#equation-eq-naive-bayes-max-feature-params-repeated" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{align*}
\hat{\boldsymbol{\theta}}_{\{\mathbf{X} \mid Y\}} &amp;= \arg \max_{\boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}} \mathcal{L}(\boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}} ; \mathcal{D}) \\
&amp;= \arg \max_{\boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}} \prod_{n=1}^N  \prod_{d=1}^D \mathbb{P}\left(X_d^{(n)} \middle \vert Y^{(n)} = k ; \boldsymbol{\theta}_{k, d} \right) \\
\end{align*}
\end{split}\]</div>
<p>We will give a formulation for the case when all features <span class="math notranslate nohighlight">\(X_d\)</span> are continuous. As mentioned
in <a class="reference internal" href="#continuous-features-gaussian-distribution"><span class="std std-ref">Continuous Features (Gaussian Distribution)</span></a>, we will assume that the features <span class="math notranslate nohighlight">\(X_d\)</span> given class <span class="math notranslate nohighlight">\(Y=k\)</span>
are distributed according to a Gaussian distribution.</p>
<div class="warning admonition">
<p class="admonition-title">Hand Wavy</p>
<p>This section will be a bit hand wavy as I did not derive it by hand, but one just need to remember we need
to find a total of <span class="math notranslate nohighlight">\(K \times D\)</span> parameters. Of course, in the case of Gaussian distribution, that means
we need to find a total of <span class="math notranslate nohighlight">\(K \times D \times 2\)</span> parameters, where the <span class="math notranslate nohighlight">\(2\)</span> comes from the mean and variance.</p>
</div>
<p>Before we write the multiplicand in <a class="reference internal" href="#equation-eq-naive-bayes-max-feature-params-repeated">(128)</a> in terms of the PDF
of the Gaussian distribution, we will follow Kevin Murphyâ€™s method (pp 329) and represent</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-max-feature-params-kevin-murphy-1">
<span class="eqno">(129)<a class="headerlink" href="#equation-eq-naive-bayes-max-feature-params-kevin-murphy-1" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{align*}
\arg \max_{\boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}} \prod_{n=1}^N  \prod_{d=1}^D \mathbb{P}\left(X_d^{(n)} \middle \vert Y^{(n)} = k ; \boldsymbol{\theta}_{k, d} \right) &amp;= \arg \max_{\boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}} \prod_{n=1}^N  \prod_{k=1}^K \prod_{d=1}^D \mathbb{P}\left(X_d^{(n)} \middle \vert Y^{(n)} = k ; \boldsymbol{\theta}_{k, d} \right)^{I\left\{ Y^{(n)} = k \right\}} \\
\end{align*}
\end{split}\]</div>
<p>Then he applied the log function to both sides of <a class="reference internal" href="#equation-eq-naive-bayes-max-feature-params-kevin-murphy-1">(129)</a>,</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-max-feature-params-kevin-murphy-2">
<span class="eqno">(130)<a class="headerlink" href="#equation-eq-naive-bayes-max-feature-params-kevin-murphy-2" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{align*}
\arg \max_{\boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}} \log \left( \prod_{n=1}^N  \prod_{k=1}^K \prod_{d=1}^D \mathbb{P}\left(X_d^{(n)} \middle \vert Y^{(n)} = k ; \boldsymbol{\theta}_{k, d} \right)^{I\left\{ Y^{(n)} = k \right\}} \right) &amp;= \arg \max_{\boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}} \sum_{n=1}^N  \sum_{k=1}^K \sum_{d=1}^D I\left\{ Y^{(n)} = k \right\} \log \left( \mathbb{P}\left(X_d^{(n)} \middle \vert Y^{(n)} = k ; \boldsymbol{\theta}_{k, d} \right) \right) \\
&amp;= \arg \max_{\boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}} \sum_{k=1}^K \sum_{d=1}^D \left [\sum_{n=1: Y^{(n)} = k}^N \log \left(\mathbb{P}\left(X_d^{(n)} \middle \vert Y = k ; \boldsymbol{\theta}_{k, d} \right) \right)\right]\\
\end{align*}
\end{split}\]</div>
<p>where the notation <span class="math notranslate nohighlight">\(n=1: Y^{(n)} = k\)</span> means that we are summing over all <span class="math notranslate nohighlight">\(n\)</span> where <span class="math notranslate nohighlight">\(Y^{(n)} = k\)</span>. In other words,
we are looking at all the data points where the class label is <span class="math notranslate nohighlight">\(k\)</span>.</p>
<p>We can further simplify <a class="reference internal" href="#equation-eq-naive-bayes-max-feature-params-kevin-murphy-2">(130)</a> as:</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-max-feature-params-kevin-murphy-3">
<span class="eqno">(131)<a class="headerlink" href="#equation-eq-naive-bayes-max-feature-params-kevin-murphy-3" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{align*}
\arg \max_{\boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}} \sum_{k=1}^K \sum_{d=1}^D \left [\sum_{n=1: Y^{(n)} = k}^N \log \left(\mathbb{P}\left(X_d^{(n)} \middle \vert Y = k ; \boldsymbol{\theta}_{k, d} \right) \right)\right] &amp;= \arg \max_{\boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}} \sum_{k=1}^K \sum_{d=1}^D \log \mathbb{P}\left(\mathcal{D}_{dk} ; \boldsymbol{\theta}_{k, d} \right) \\
&amp;= \arg \max_{\boldsymbol{\theta}_{k, d}} \log \mathbb{P}\left(\mathcal{D}_{11} ; \boldsymbol{\theta}_{1, 1} \right) + \log \mathbb{P}\left(\mathcal{D}_{12} ; \boldsymbol{\theta}_{1, 2} \right) + \cdots + \log \mathbb{P}\left(\mathcal{D}_{1D} ; \boldsymbol{\theta}_{1, D} \right) + \log \mathbb{P}\left(\mathcal{D}_{21} ; \boldsymbol{\theta}_{2, 1} \right) + \log \mathbb{P}\left(\mathcal{D}_{22} ; \boldsymbol{\theta}_{2, 2} \right) + \cdots + \log \mathbb{P}\left(\mathcal{D}_{2D} ; \boldsymbol{\theta}_{2, D} \right) + \cdots + \log \mathbb{P}\left(\mathcal{D}_{K1} ; \boldsymbol{\theta}_{K, 1} \right) + \log \mathbb{P}\left(\mathcal{D}_{K2} ; \boldsymbol{\theta}_{K, 2} \right) + \cdots + \log \mathbb{P}\left(\mathcal{D}_{KD} ; \boldsymbol{\theta}_{K, D} \right) \\
\end{align*}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathcal{D}_{dk}\)</span> is the data set of all the data points of feature <span class="math notranslate nohighlight">\(d\)</span> and class <span class="math notranslate nohighlight">\(k\)</span>.</p>
<p>Now we can <em><strong>individually maximize</strong></em> the parameters of each feature and class pair in <a class="reference internal" href="#equation-eq-naive-bayes-max-feature-params-kevin-murphy-3">(131)</a>, i.e. estimate <span class="math notranslate nohighlight">\(\mathcal{D}_{dk}\)</span> for each <span class="math notranslate nohighlight">\(d\)</span> and <span class="math notranslate nohighlight">\(k\)</span>.</p>
<div class="proof example admonition" id="example-naive-bayes-feature-1-class-2">
<p class="admonition-title"><span class="caption-number">Example 32 </span> (Example on Feature 1 and Class 2)</p>
<section class="example-content" id="proof-content">
<p>For example, <span class="math notranslate nohighlight">\(\mathcal{D}_{12}\)</span> refers to all the data points of feature <span class="math notranslate nohighlight">\(1\)</span> and class <span class="math notranslate nohighlight">\(2\)</span> and we can
maximize the parameters of this data set <span class="math notranslate nohighlight">\(\mathcal{D}_{12}\)</span> in a similar vein from <a class="reference internal" href="#def:naive-bayes-max-feature-params">Definition 139</a>,
but now instead of
multiplying the probabilities, we are summing the log probabilities.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\arg \max_{\theta_{1, 2}} \log \mathbb{P}\left(\mathcal{D}_{12} ; \theta_{1, 2} \right) &amp;= \arg \max_{\theta_{1, 2}} \sum_{n=1: Y^{(n)} = 2}^N \log \left(\mathbb{P}\left(X_1^{(n)} \middle \vert Y = 2 ; \theta_{1, 2} \right) \right) \\
\end{align*}
\end{split}\]</div>
<p>where we will attempt to find the best estimate <span class="math notranslate nohighlight">\(\theta_{1, 2} = \left(\mu_{2, 1}, \sigma_{2, 1} \right)\)</span> for the parameters of the Gaussian distribution of feature <span class="math notranslate nohighlight">\(1\)</span> and class <span class="math notranslate nohighlight">\(2\)</span>.</p>
<p>It turns out that the maximum likelihood estimate for the parameters of the Gaussian distribution is the sample mean and sample variance of the data set <span class="math notranslate nohighlight">\(\mathcal{D}_{12}\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\hat{\theta}_{1, 2} := \arg \max_{\theta_{1, 2}} \log \mathbb{P}\left(\mathcal{D}_{12} ; \theta_{1, 2} \right) &amp;= \begin{bmatrix} \hat{\mu_{2, 1}} \\ \hat{\sigma}_{2, 1} \end{bmatrix} \\
&amp;= \begin{bmatrix} \frac{1}{N_2} \sum_{n=1}^{N_2} x_1^{(n)} \\ \sqrt{\frac{1}{N_2} \sum_{n=1}^{N_2} \left( x_1^{(n)} - \hat{\mu}_{2, 1} \right)^2} \end{bmatrix} \\
&amp;= \begin{bmatrix} \bar{x}_{2, 1} \\ s_{2, 1} \end{bmatrix} \\
\end{align*}
\end{split}\]</div>
</section>
</div><p>Now for the general form</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\hat{\theta}_{k, d} := \arg \max_{\theta_{k, d}} \log \mathbb{P}\left(\mathcal{D}_{dk} ; \theta_{k, d} \right) &amp;= \begin{bmatrix} \hat{\mu}_{k, d} \\ \hat{\sigma}_{k, d} \end{bmatrix} \\
&amp;= \begin{bmatrix} \frac{1}{N_k} \sum_{n=1}^{N_k} x_d^{(n)} \\ \sqrt{\frac{1}{N_k} \sum_{n=1}^{N_k} \left( x_d^{(n)} - \hat{\mu}_{k, d} \right)^2} \end{bmatrix} \\
&amp;= \begin{bmatrix} \bar{x}_{k, d} \\ s_{k, d} \end{bmatrix} \\
\end{align*}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\bar{x}_{k, d}\)</span> is the sample mean of the data set <span class="math notranslate nohighlight">\(\mathcal{D}_{dk}\)</span> and <span class="math notranslate nohighlight">\(s_{k, d}\)</span> is the sample standard deviation of the data set <span class="math notranslate nohighlight">\(\mathcal{D}_{dk}\)</span>.</p>
<p>For completeness, the parameter matrix <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}\)</span> defined in <a class="reference internal" href="#equation-eq-naive-bayes-estimation-2">(114)</a> becomes:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}} &amp;= \begin{bmatrix} \boldsymbol{\theta}_{11} &amp; \boldsymbol{\theta}_{12} &amp; \cdots &amp; \boldsymbol{\theta}_{1D} \\ \boldsymbol{\theta}_{21} &amp; \boldsymbol{\theta}_{22} &amp; \cdots &amp; \boldsymbol{\theta}_{2D} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \boldsymbol{\theta}_{K1} &amp; \boldsymbol{\theta}_{K2} &amp; \cdots &amp; \boldsymbol{\theta}_{KD} \end{bmatrix} \\
&amp;= \begin{bmatrix} \left(\bar{x}_{1, 1}, s_{1, 1} \right) &amp; \left(\bar{x}_{1, 2}, s_{1, 2} \right) &amp; \cdots &amp; \left(\bar{x}_{1, D}, s_{1, D} \right) \\ \left(\bar{x}_{2, 1}, s_{2, 1} \right) &amp; \left(\bar{x}_{2, 2}, s_{2, 2} \right) &amp; \cdots &amp; \left(\bar{x}_{2, D}, s_{2, D} \right) \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \left(\bar{x}_{K, 1}, s_{K, 1} \right) &amp; \left(\bar{x}_{K, 2}, s_{K, 2} \right) &amp; \cdots &amp; \left(\bar{x}_{K, D}, s_{K, D} \right) \end{bmatrix} \\
\end{align*}
\end{split}\]</div>
<hr class="docutils" />
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
&amp;= \arg \max_{\boldsymbol{\theta}_{\{\mathbf{X} \mid Y\}}} \sum_{k=1}^K \sum_{d=1}^D \left [\sum_{n=1: Y^{(n)} = k}^N \log \left(\frac{1}{\sqrt{2 \pi \sigma_{k, d}^2}} \exp \left( -\frac{1}{2 \sigma_{k, d}^2} \left( X_d^{(n)} - \mu_{k, d} \right)^2 \right) \right)\right] \\
\end{align*}
\end{split}\]</div>
<p>See derivations from section 4.2.5 and 4.2.6 of Probabilistic Machine Learning: An Introduction by Kevin Murphy
for the univariate and multivariate Gaussian case respectively.</p>
</section>
</section>
</section>
<section id="decision-boundary">
<h2>Decision Boundary<a class="headerlink" href="#decision-boundary" title="Permalink to this heading">#</a></h2>
<figure class="align-default" id="fig-naive-bayes-linear">
<img alt="../../../_images/cs4780_lecture5_naive_bayes.png" src="../../../_images/cs4780_lecture5_naive_bayes.png" />
<figcaption>
<p><span class="caption-number">Fig. 28 </span><span class="caption-text">Naive Bayes leads to a linear decision boundary in many common cases. Illustrated here is the case where <span class="math notranslate nohighlight">\(P\left(x_\alpha \mid y\right)\)</span> is Gaussian and where <span class="math notranslate nohighlight">\(\sigma_{\alpha, c}\)</span> is identical for all <span class="math notranslate nohighlight">\(c\)</span> (but can differ across dimensions <span class="math notranslate nohighlight">\(\alpha\)</span> ). The boundary of the ellipsoids indicate regions of equal probabilities <span class="math notranslate nohighlight">\(P(\mathbf{x} \mid y)\)</span>. The red decision line indicates the decision boundary where <span class="math notranslate nohighlight">\(P(y=1 \mid \mathbf{x})=P(y=2 \mid \mathbf{x})\)</span>.. Image Credit: <a class="reference external" href="https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote05.html">CS 4780</a></span><a class="headerlink" href="#fig-naive-bayes-linear" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Here <span class="math notranslate nohighlight">\(\alpha\)</span> is just index <span class="math notranslate nohighlight">\(d\)</span>.</p>
<p>Suppose that <span class="math notranslate nohighlight">\(y_i \in\{-1,+1\}\)</span> and features are multinomial
We can show that</p>
<div class="math notranslate nohighlight">
\[
h(\mathbf{x})=\underset{y}{\operatorname{argmax}} P(y) \prod_{\alpha-1}^d P\left(x_\alpha \mid y\right)=\operatorname{sign}\left(\mathbf{w}^{\top} \mathbf{x}+b\right)
\]</div>
<p>That is,</p>
<div class="math notranslate nohighlight">
\[
\mathbf{w}^{\top} \mathbf{x}+b&gt;0 \Longleftrightarrow h(\mathbf{x})=+1 .
\]</div>
<p>As before, we define <span class="math notranslate nohighlight">\(P\left(x_\alpha \mid y=+1\right) \propto \theta_{\alpha+}^{x_\alpha}\)</span> and <span class="math notranslate nohighlight">\(P(Y=+1)=\pi_{+}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
{[\mathbf{w}]_\alpha } &amp; =\log \left(\theta_{\alpha+}\right)-\log \left(\theta_{\alpha-}\right) \\
b &amp; =\log \left(\pi_{+}\right)-\log \left(\pi_{-}\right)
\end{aligned}
\end{split}\]</div>
<p>If we use the above to do classification, we can compute for <span class="math notranslate nohighlight">\(\mathbf{w}^{\top} \cdot \mathbf{x}+b\)</span>
Simplifying this further leads to</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
&amp; \mathbf{w}^{\top} \mathbf{x}+b&gt;0 \Longleftrightarrow \sum_{\alpha=1}^d[\mathbf{x}]_\alpha \overbrace{\left(\log \left(\theta_{\alpha+}\right)-\log \left(\theta_{\alpha-}\right)\right)}^{\left[\mathbf{w}]_\alpha\right.}+\overbrace{\log \left(\pi_{+}\right)-\log \left(\pi_{-}\right)}^b&gt;0 \quad \text{ (Plugging in definition of w,b.) } \\
&amp; \Longleftrightarrow \exp \left(\sum_{\alpha=1}^d[\mathbf{x}]_\alpha\left(\log \left(\theta_{\alpha+}\right)-\log \left(\theta_{\alpha-}\right)\right)+\log \left(\pi_{+}\right)-\log \left(\pi_{-}\right)\right)&gt;1 \quad \text { (exponentiating both sides) } \\
&amp; \Longleftrightarrow \prod_{\alpha=1}^d \frac{\exp \left(\log \theta_{\alpha+}^{[\mathbf{x}]_\alpha}+\log \left(\pi_{+}\right)\right)}{\exp \left(\log \theta_{\alpha-}^{[\mathbf{x}]_\alpha}+\log \left(\pi_{-}\right)\right)}&gt;1 \quad \text{ (Because $a \log (b)=\log \left(b^a\right)$ and $\exp (a-b)=\frac{e^a}{e^b}$ operations) } \\
&amp; \Longleftrightarrow \prod_{\alpha=1}^d \frac{\theta_{\alpha+}^{[\mathbf{x}]_\alpha} \pi_{+}}{\theta_{\alpha-}^{[\mathbf{x}]_\alpha} \pi_{-}}&gt;1 \quad \text{ (Because $\exp (\log (a))=a$ and $e^{a+b}=e^a e^b$) } \\
&amp; \Longleftrightarrow \frac{\prod_{\alpha=1}^d P\left([\mathbf{x}]_\alpha \mid Y=+1\right) \pi_{+}}{\prod_{\alpha=1}^d P\left([\mathbf{x}]_\alpha \mid Y=-1\right) \pi_{-}}&gt;1 \quad \text{ (Because $P\left([\mathbf{x}]_\alpha \mid Y=-1\right)=\theta_{\alpha-}^{\mathbf{x}]_\alpha}$) } \\
&amp; \Longleftrightarrow \frac{P(\mathbf{x} \mid Y=+1) \pi_{+}}{P(\mathbf{x} \mid Y=-1) \pi_{-}}&gt;1 \quad \text{ (By the naive Bayes assumption.) } \\
&amp; \Longleftrightarrow \frac{P(Y=+1 \mid \mathbf{x})}{P(Y=-1 \mid \mathbf{x})}&gt;1 \quad \text{ (By Bayes rule (the denominator $P(\mathbf{x})$ cancels out, and $\pi_{+}=P(Y=+1)$.)) } \\
&amp; \Longleftrightarrow P(Y=+1 \mid \mathbf{x})&gt;P(Y=-1 \mid \mathbf{x}) \\
&amp; \Longleftrightarrow \underset{y}{\operatorname{argmax}} P(Y=y \mid \mathbf{x})=+1 \quad \text{ (the point x lies on the positive side of the hyperplane iff Naive Bayes predicts +1) } \\
&amp;
\end{aligned}
\end{split}\]</div>
<section id="connection-with-logistic-regression">
<h3>Connection with Logistic Regression<a class="headerlink" href="#connection-with-logistic-regression" title="Permalink to this heading">#</a></h3>
<p>In the case of continuous features (Gaussian Naive Bayes), when the variance is independent of the class <span class="math notranslate nohighlight">\(\left(\sigma_{\alpha c}\right.\)</span> is identical for all <span class="math notranslate nohighlight">\(c\)</span> ), we can show that</p>
<div class="math notranslate nohighlight">
\[
P(y \mid \mathbf{x})=\frac{1}{1+e^{-y\left(\mathbf{w}^{\top} \mathbf{x}+b\right)}}
\]</div>
<p>This model is also known as logistic regression. <span class="math notranslate nohighlight">\(N B\)</span> and <span class="math notranslate nohighlight">\(L R\)</span> produce asymptotically the same model if the Naive Bayes assumption holds.</p>
<p>See more proofs below:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote05.html">https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote05.html</a>.</p></li>
<li><p><a class="reference external" href="https://stats.stackexchange.com/questions/142215/how-is-naive-bayes-a-linear-classifier">https://stats.stackexchange.com/questions/142215/how-is-naive-bayes-a-linear-classifier</a></p></li>
<li><p>Section 9.3.4 of Probabilistic Machine Learning: An Introduction.</p></li>
</ul>
</section>
</section>
<section id="time-and-space-complexity">
<h2>Time and Space Complexity<a class="headerlink" href="#time-and-space-complexity" title="Permalink to this heading">#</a></h2>
<p>Let <span class="math notranslate nohighlight">\(N\)</span> be the number of training samples, <span class="math notranslate nohighlight">\(D\)</span> be the number of features, and <span class="math notranslate nohighlight">\(K\)</span> be the number of classes.</p>
<p>During training, the time complexity is <span class="math notranslate nohighlight">\(\mathcal{O}(NKD)\)</span> if we are using a brute force approach.
In my <a class="reference external" href="https://github.com/gao-hongnan/gaohn-probability-stats/blob/naive-bayes/src/generative/naive_bayes/naive_bayes.py">implementation</a>,
the main training loop is in <code class="docutils literal notranslate"><span class="pre">_estimate_prior_parameters</span></code> and <code class="docutils literal notranslate"><span class="pre">_estimate_likelihood_parameters</span></code> methods.</p>
<p>In the former, we are looping through the classes <span class="math notranslate nohighlight">\(K\)</span> times, but a hidden operation is calculating the
sum of the class counts, which is <span class="math notranslate nohighlight">\(\mathcal{O}(N)\)</span>, so the time complexity is <span class="math notranslate nohighlight">\(\mathcal{O}(NK)\)</span>, using the
vectorized operation <code class="docutils literal notranslate"><span class="pre">np.sum</span></code> helps speed up a bit.</p>
<p>In the latter, we are looping through the classes <span class="math notranslate nohighlight">\(K\)</span> times, and for each class, we are looping through
the features <span class="math notranslate nohighlight">\(D\)</span> times, and in each feature loop, we are calculating the mean and variance of the feature,
so the operation of calculating the mean and variance is <span class="math notranslate nohighlight">\(\mathcal{O}(N) + \mathcal{O}(N)\)</span> respectively, bringing the time complexity
to <span class="math notranslate nohighlight">\(\mathcal{O}(2NKD) \approx \mathcal{O}(NKD)\)</span>. Even though we are using <code class="docutils literal notranslate"><span class="pre">np.mean</span></code> and <code class="docutils literal notranslate"><span class="pre">np.var</span></code> to speed up, the time complexity
for brute force approach is still <span class="math notranslate nohighlight">\(\mathcal{O}(NKD)\)</span>.</p>
<p>For the space complexity, we are storing the prior parameters and likelihood parameters, which are of size <span class="math notranslate nohighlight">\(K\)</span> and <span class="math notranslate nohighlight">\(KD\)</span> respectively,
in code, that corresponds to <code class="docutils literal notranslate"><span class="pre">self.pi</span></code> and <code class="docutils literal notranslate"><span class="pre">self.theta</span></code>, so the space complexity is <span class="math notranslate nohighlight">\(\mathcal{O}(K + KD) \approx \mathcal{O}(KD)\)</span>.</p>
<p>During inference/prediction, the time complexity for predicting one single sample
is <span class="math notranslate nohighlight">\(\mathcal{O}(KD)\)</span>, because the <code class="docutils literal notranslate"><span class="pre">predict_one_sample</span></code> method primarily calls the <code class="docutils literal notranslate"><span class="pre">_calculate_posterior</span></code> method, which in
turn invokes <code class="docutils literal notranslate"><span class="pre">_calculate_prior</span></code> and <code class="docutils literal notranslate"><span class="pre">_calculate_joint_likelihood</span></code> methods, and the time complexity of these two methods
is <span class="math notranslate nohighlight">\(\mathcal{O}(1)\)</span> and <span class="math notranslate nohighlight">\(\mathcal{O}(KD)\)</span> respectively. For <code class="docutils literal notranslate"><span class="pre">_calculate_prior</span></code>, it just involves us looking up the
<code class="docutils literal notranslate"><span class="pre">self.prior</span></code> parameter, which is a constant time operation. For <code class="docutils literal notranslate"><span class="pre">_calculate_joint_likelihood</span></code>, it involves us looping through
the class <span class="math notranslate nohighlight">\(K\)</span> times and looping through the features <span class="math notranslate nohighlight">\(D\)</span> times, so the time complexity is <span class="math notranslate nohighlight">\(\mathcal{O}(KD)\)</span>, the <code class="docutils literal notranslate"><span class="pre">mean</span></code>
and <code class="docutils literal notranslate"><span class="pre">var</span></code> parameters are now constant time since they are just looked up from <code class="docutils literal notranslate"><span class="pre">self.theta</span></code>. There is however a <code class="docutils literal notranslate"><span class="pre">np.prod</span></code> operation
towards the end, but the overall time complexity should still be in the order of <span class="math notranslate nohighlight">\(\mathcal{O}(KD)\)</span>.</p>
<p>For the space complexity, besides the stored (not counted) parameters, we are storing the posterior probabilities, which is of size <span class="math notranslate nohighlight">\(K\)</span>,
in code, that corresponds to <code class="docutils literal notranslate"><span class="pre">self.posterior</span></code>, so the space complexity is <span class="math notranslate nohighlight">\(\mathcal{O}(K)\)</span>, and if <span class="math notranslate nohighlight">\(K\)</span> is small, then the space complexity
is <span class="math notranslate nohighlight">\(\mathcal{O}(1)\)</span>.</p>
<table class="table" id="time-complexity-naive-bayes">
<caption><span class="caption-number">Table 13 </span><span class="caption-text">Time Complexity of Naive Bayes</span><a class="headerlink" href="#time-complexity-naive-bayes" title="Permalink to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Train</p></th>
<th class="head"><p>Inference</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\mathcal{O}(NKD)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{O}(KD)\)</span></p></td>
</tr>
</tbody>
</table>
<table class="table" id="space-complexity-naive-bayes">
<caption><span class="caption-number">Table 14 </span><span class="caption-text">Space Complexity of Naive Bayes</span><a class="headerlink" href="#space-complexity-naive-bayes" title="Permalink to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Train</p></th>
<th class="head"><p>Inference</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\mathcal{O}(KD)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{O}(1)\)</span></p></td>
</tr>
</tbody>
</table>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J. â€œChapter 22.7 Maximum Likelihood.â€ In Dive into Deep Learning, 2021.</p></li>
<li><p>Chan, Stanley H. â€œChapter 8.1. Maximum-Likelihood Estimation.â€ In Introduction to Probability for Data Science, 172-180. Ann Arbor, Michigan: Michigan Publishing Services, 2021</p></li>
<li><p>Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J. â€œChapter 22.9 Naive Bayes.â€ In Dive into Deep Learning, 2021.</p></li>
<li><p>Hal DaumÃ© III. â€œChapter 9.3. Naive Bayes Models.â€ In A Course in Machine Learning, January 2017.</p></li>
<li><p>Murphy, Kevin P. â€œChapter 9.3. Naive Bayes Models.â€ In Probabilistic Machine Learning: An Introduction. Cambridge (Massachusetts): The MIT Press, 2022.</p></li>
<li><p>James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. â€œChapter 4.4.4. Naive Bayesâ€ In An Introduction to Statistical Learning: With Applications in R. Boston: Springer, 2022.</p></li>
<li><p>Mitchell, Tom Michael. Machine Learning. New York: McGraw-Hill, 1997. (His new chapter on Generate and Discriminative Classifiers: Naive Bayes and Logistic Regression)</p></li>
<li><p>Jurafsky, Dan, and James H. Martin. â€œChapter 4. Naive Bayes and Sentiment Classificationâ€ In Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition. Noida: Pearson, 2022.</p></li>
<li><p>Bishop, Christopher M. â€œChapter 4.2. Probabilistic Generative Models.â€ In Pattern Recognition and Machine Learning. New York: Springer-Verlag, 2016</p></li>
</ul>
<hr class="footnotes docutils" />
<aside class="footnote brackets" id="intractable" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p>Cite Dive into Deep Learning on this. Also, the joint probability is intractable because the number of parameters to estimate is exponential in the number of features. Use binary bits example, see my notes.</p>
</aside>
<aside class="footnote brackets" id="likelihood-1" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">2</a><span class="fn-bracket">]</span></span>
<p>Not to be confused with the likelihood term <span class="math notranslate nohighlight">\(\mathbb{P}(\mathbf{X} \mid Y)\)</span> in Bayesâ€™ terminology.</p>
</aside>
<aside class="footnote brackets" id="joint-distribution" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">3</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Joint_probability_distribution#Discrete_case">Joint Probability Distribution</a></p>
</aside>
<aside class="footnote brackets" id="chain-rule-of-probability" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">4</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Chain_rule_(probability)">Chain Rule of Probability</a></p>
</aside>
<aside class="footnote brackets" id="dparameters" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">5</a><span class="fn-bracket">]</span></span>
<p>Dive into Deep Learning, Section 22.9, this is only assuming that each feature <span class="math notranslate nohighlight">\(\mathbf{x}_d^{(n)}\)</span> is binary, i.e. <span class="math notranslate nohighlight">\(\mathbf{x}_d^{(n)} \in \{0, 1\}\)</span>.</p>
</aside>
<aside class="footnote brackets" id="id15" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">6</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Conditional_independence">Conditional Independence</a></p>
</aside>
<aside class="footnote brackets" id="kdparameters" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id7">7</a><span class="fn-bracket">]</span></span>
<p>Probablistic Machine Learning: An Introduction, Section 9.3, pp 328</p>
</aside>
<aside class="footnote brackets" id="categorical-distribution" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id8">8</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Categorical_distribution">Category Distribution</a></p>
</aside>
<aside class="footnote brackets" id="iid-tuple" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id12">9</a><span class="fn-bracket">]</span></span>
<p><span class="math notranslate nohighlight">\(\left(\mathbf{X}^{(n)}, Y^{(1)}\right)\)</span> is written as a tuple, when in fact they can be considered 1 single variable.</p>
</aside>
<aside class="footnote brackets" id="iid-likelihood" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id13">10</a><span class="fn-bracket">]</span></span>
<p>Refer to page 470 of <span id="id16">[<a class="reference internal" href="../../../references_resources_roadmap/bibliography.html#id2" title="Stanley H. Chan. Introduction to probability for Data Science. Michigan Publishing, 2021.">Chan, 2021</a>]</span>. Note that we cannot write it as a product if the data is not independent and identically distributed.</p>
</aside>
<aside class="footnote brackets" id="decomposed-likelihood" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id14">11</a><span class="fn-bracket">]</span></span>
<p>Cite Kevin Murphy and Bishop.</p>
</aside>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./machine_learning/generative/naive_bayes"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

            </article>
            

            
            
            <footer class="bd-footer-article">
                <!-- Previous / next buttons -->
<div class='prev-next-area'>
  <a class='left-prev' id="prev-link" href="intro.html" title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
          <p class="prev-next-subtitle">previous</p>
          <p class="prev-next-title">Naive Bayes</p>
      </div>
  </a>
  <a class='right-next' id="next-link" href="implementation.html" title="next page">
  <div class="prev-next-info">
      <p class="prev-next-subtitle">next</p>
      <p class="prev-next-title">Naives Bayes Implementation</p>
  </div>
  <i class="fa-solid fa-angle-right"></i>
  </a>
</div>
            </footer>
            
          </div>
          
          
          
            <div class="bd-sidebar-secondary bd-toc">
              
<div class="toc-item">
  
<div class="tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
</div>
<nav id="bd-toc-nav" class="page-toc">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#notations">
   Notations
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#discriminative-vs-generative">
     Discriminative vs Generative
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#naive-bayes-setup">
   Naive Bayes Setup
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#inference-prediction">
   Inference/Prediction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-naive-bayes-form">
   The Naive Bayes Form
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#simple-form">
     Simple Form
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#extended-form">
     Extended form
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-naive-bayes-assumptions">
   The Naive Bayes Assumptions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#independent-and-identically-distributed-i-i-d">
     Independent and Identically Distributed (i.i.d.)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#conditional-independence">
     Conditional Independence
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#parameter-vector">
   Parameter Vector
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#inductive-bias-distribution-assumptions">
   Inductive Bias (Distribution Assumptions)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#targets-categorical-distribution">
     Targets (Categorical Distribution)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#discrete-features-categorical-distribution">
     Discrete Features (Categorical Distribution)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#continuous-features-gaussian-distribution">
     Continuous Features (Gaussian Distribution)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mixed-features-discrete-and-continuous">
     Mixed Features (Discrete and Continuous)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-fitting">
   Model Fitting
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#fitting-algorithm">
     Fitting Algorithm
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#maximum-likelihood-estimation">
     Maximum Likelihood Estimation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#estimating-priors">
     Estimating Priors
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#maximum-likelihood-estimation-for-priors-categorical-distribution">
     Maximum Likelihood Estimation for Priors (Categorical Distribution)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#estimating-likelihood-gaussian-version">
     Estimating Likelihood (Gaussian Version)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#maximum-likelihood-estimate-for-likelihood-continuous-feature-parameters">
       Maximum Likelihood Estimate for Likelihood (Continuous Feature Parameters)
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#decision-boundary">
   Decision Boundary
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#connection-with-logistic-regression">
     Connection with Logistic Regression
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#time-and-space-complexity">
   Time and Space Complexity
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

</nav>
</div>

            </div>
          
          
        </div>
        <footer class="bd-footer-content">
          <div class="bd-footer-content__inner">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Gao Hongnan
</p>

  </div>
  
  <div class="footer-item">
    
<p class="copyright">

    &copy; Copyright 2023.<br>

</p>

  </div>
  
  <div class="footer-item">
    <p class="last-updated">
Last updated on None.<br>
</p>
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </div>
        </footer>
        

      </main>
    </div>
  </div>

  
    
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/bootstrap.js?digest=796348d33e8b1d947c94"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=796348d33e8b1d947c94"></script>

  </body>
</html>