
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Cross Entropy Loss &#8212; Machine Learning Chronicles</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=796348d33e8b1d947c94" rel="stylesheet">
<link href="../../../_static/styles/bootstrap.css?digest=796348d33e8b1d947c94" rel="stylesheet">
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=796348d33e8b1d947c94" rel="stylesheet">

  
  <link href="../../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=796348d33e8b1d947c94" rel="stylesheet">
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2">
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2">
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" href="../../../_static/styles/sphinx-book-theme.css?digest=4ec06e9971c5264fbd345897d5258098f11cc577" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=796348d33e8b1d947c94">
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=796348d33e8b1d947c94">

    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?digest=8bf782fb4ee92b3d3646425e50f299c4e1fd152d"></script>
    <script src="../../../_static/tabs.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"tex": {"macros": {"defeq": "\\overset{\\text{def}}{=}", "defa": "\\overset{\\text{(a)}}{=}", "defb": "\\overset{\\text{(b)}}{=}", "defc": "\\overset{\\text{(c)}}{=}", "defd": "\\overset{\\text{(d)}}{=}", "st": "\\mid", "mod": "\\mid", "S": "\\Omega", "s": "\\omega", "e": "\\exp", "P": "\\mathbb{P}", "R": "\\mathbb{R}", "expectation": "\\mathbb{E}", "v": "\\mathbf{v}", "a": "\\mathbf{a}", "b": "\\mathbf{b}", "c": "\\mathbf{c}", "u": "\\mathbf{u}", "w": "\\mathbf{w}", "x": "\\mathbf{x}", "y": "\\mathbf{y}", "z": "\\mathbf{z}", "0": "\\mathbf{0}", "1": "\\mathbf{1}", "A": "\\mathbf{A}", "B": "\\mathbf{B}", "C": "\\mathbf{C}", "E": "\\mathcal{F}", "eventA": "\\mathcal{A}", "lset": "\\left\\{", "rset": "\\right\\}", "lsq": "\\left[", "rsq": "\\right]", "lpar": "\\left(", "rpar": "\\right)", "lcurl": "\\left\\{", "rcurl": "\\right\\}", "pmf": "p_X", "pdf": "f_X", "pdftwo": "f_{X,Y}", "pdfjoint": "f_{\\mathbf{X}}", "pmfjointxy": "p_{X, Y}", "pdfjointxy": "f_{X, Y}", "cdf": "F_X", "pspace": "(\\Omega, \\mathcal{F}, \\mathbb{P})", "var": "\\operatorname{Var}", "std": "\\operatorname{Std}", "bern": "\\operatorname{Bernoulli}", "binomial": "\\operatorname{Binomial}", "geometric": "\\operatorname{Geometric}", "poisson": "\\operatorname{Poisson}", "uniform": "\\operatorname{Uniform}", "normal": "\\operatorname{Normal}", "gaussian": "\\operatorname{Gaussian}", "gaussiansymbol": "\\mathcal{N}", "exponential": "\\operatorname{Exponential}", "iid": "\\textbf{i.i.d.}", "and": "\\text{and}"}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'machine_learning/fundamentals/criterions/cross_entropy_loss';</script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Focal Loss" href="focal_loss.html" />
    <link rel="prev" title="Concept" href="concept.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="docsearch:language" content="en">
  </head>
  
  
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="180" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>

  
  <input type="checkbox" class="sidebar-toggle" name="__primary" id="__primary">
  <label class="overlay overlay-primary" for="__primary"></label>

  
  <input type="checkbox" class="sidebar-toggle" name="__secondary" id="__secondary">
  <label class="overlay overlay-secondary" for="__secondary"></label>

  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
      
<form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false">
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
    </div>
  </div>

  
  <nav class="bd-header navbar navbar-expand-lg bd-navbar" id="navbar-main"><div class="bd-header__inner bd-page-width">
  <label class="sidebar-toggle primary-toggle" for="__primary">
      <span class="fa-solid fa-bars"></span>
  </label>
  <div id="navbar-start">
    
    
  


<a class="navbar-brand logo" href="../../../intro.html">

  
  
  
  
  
  
  

  
    <img src="../../../_static/logo.png" class="logo__image only-light" alt="Logo image">
    <img src="../../../_static/logo.png" class="logo__image only-dark" alt="Logo image">
  
  
</a>
    
  </div>

  
  <div class="col-lg-9 navbar-header-items">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <nav class="navbar-nav">
    <p class="sidebar-header-items__title" role="heading" aria-level="1" aria-label="Site Navigation">
        Site Navigation
    </p>
    <ul id="navbar-main-elements" class="navbar-nav">
        
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../notations/mathematical_notations.html">
                        Mathematical Notations
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../notations/machine_learning_notations.html">
                        Machine Learning Notations
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../notations/deep_learning_notations.html">
                        Deep Learning Notations
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../optimization/gradient_descent/intro.html">
                        Gradient Descent
                      </a>
                    </li>
                

                    <li class="nav-item current active">
                      <a class="nav-link nav-internal" href="../intro.html">
                        Fundamentals
                      </a>
                    </li>
                
            <div class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                    More
                </button>
                <div class="dropdown-menu">
                    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../mixtures/intro.html">
                        Mixture Models
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../clustering/intro.html">
                        Clustering
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../deep_learning/intro.html">
                        Introduction
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../deep_learning/natural_language_processing/intro.html">
                        Natural Language Processing (NLP)
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../references_resources_roadmap/bibliography.html">
                        Bibliography
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../references_resources_roadmap/resources.html">
                        Resources
                      </a>
                    </li>
                
                </div>
            </div>
            
    </ul>
</nav>
      </div>
      
    </div>

    <div id="navbar-end">
      
        <div class="navbar-end-item navbar-persistent--container">
          
<button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-toggle="tooltip">
  <i class="fa-solid fa-magnifying-glass"></i>
</button>
        </div>
      
      
      <div class="navbar-end-item">
        <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
</button>
      </div>
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
      </ul>
      </div>
      
    </div>
  </div>


  
  
    <div class="navbar-persistent--mobile">
<button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-toggle="tooltip">
  <i class="fa-solid fa-magnifying-glass"></i>
</button>
    </div>
  

  
  <label class="sidebar-toggle secondary-toggle" for="__secondary">
      <span class="fa-solid fa-outdent"></span>
  </label>
  

</div>
  </nav>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        
  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
      
      <div class="navbar-center-item">
        <nav class="navbar-nav">
    <p class="sidebar-header-items__title" role="heading" aria-level="1" aria-label="Site Navigation">
        Site Navigation
    </p>
    <ul id="navbar-main-elements" class="navbar-nav">
        
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../notations/mathematical_notations.html">
                        Mathematical Notations
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../notations/machine_learning_notations.html">
                        Machine Learning Notations
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../notations/deep_learning_notations.html">
                        Deep Learning Notations
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../optimization/gradient_descent/intro.html">
                        Gradient Descent
                      </a>
                    </li>
                

                    <li class="nav-item current active">
                      <a class="nav-link nav-internal" href="../intro.html">
                        Fundamentals
                      </a>
                    </li>
                
            <div class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                    More
                </button>
                <div class="dropdown-menu">
                    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../mixtures/intro.html">
                        Mixture Models
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../clustering/intro.html">
                        Clustering
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../deep_learning/intro.html">
                        Introduction
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../deep_learning/natural_language_processing/intro.html">
                        Natural Language Processing (NLP)
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../references_resources_roadmap/bibliography.html">
                        Bibliography
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../references_resources_roadmap/resources.html">
                        Resources
                      </a>
                    </li>
                
                </div>
            </div>
            
    </ul>
</nav>
      </div>
      
      </div>
    

    
    
    <div class="sidebar-header-items__end">
      
      <div class="navbar-end-item">
        <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
</button>
      </div>
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
      </ul>
      </div>
      
    </div>
    
  </div>

  
  <div class="sidebar-start-items sidebar-primary__section">
    <div class="sidebar-start-items__item">
  


<a class="navbar-brand logo" href="../../../intro.html">

  
  
  
  
  
  
  

  
    <img src="../../../_static/logo.png" class="logo__image only-light" alt="Logo image">
    <img src="../../../_static/logo.png" class="logo__image only-dark" alt="Logo image">
  
  
</a>
    </div>
    <div class="sidebar-start-items__item">
<form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false">
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
    </div>
    <div class="sidebar-start-items__item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../../intro.html">
                    Introduction
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Notations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../notations/mathematical_notations.html">Mathematical Notations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notations/machine_learning_notations.html">Machine Learning Notations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notations/deep_learning_notations.html">Deep Learning Notations</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Optimization</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../optimization/gradient_descent/intro.html">Gradient Descent</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../optimization/gradient_descent/concept.html">Gradient Descent Concept</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../optimization/gradient_descent/implementation.html">Gradient Descent Construction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../optimization/gradient_descent/application.html">Application: Gradient Descent</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Machine Learning</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../intro.html">Fundamentals</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2 current active has-children"><a class="reference internal" href="intro.html">Loss</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="concept.html">Concept</a></li>
<li class="toctree-l3 current active"><a class="current reference internal" href="#">Cross Entropy Loss</a></li>
<li class="toctree-l3"><a class="reference internal" href="focal_loss.html">Focal Loss</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../empirical_risk_minimization/intro.html">Empirical Risk Minimization</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../empirical_risk_minimization/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../empirical_risk_minimization/bayes_optimal_classifier.html">Bayes Optimal Classifier</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../learning_theory/intro.html">Is the Learning Problem Solvable?</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../learning_theory/concept.html">Concept</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../bias_and_variance/intro.html">Bias and Variance Tradeoff</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../bias_and_variance/concept.html">Bias-Variance Tradeoff Concept</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../decision_boundary/intro.html">Decision Boundary</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../decision_boundary/concept.html">Concept</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../voronoi_region/intro.html">Voronoi Region</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../voronoi_region/concept.html">Concept</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../mixtures/intro.html">Mixture Models</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../mixtures/gmm/intro.html">Gaussian Mixture Models</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../mixtures/gmm/concept.html">Concept</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../clustering/intro.html">Clustering</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../clustering/kmeans/intro.html">K-Means</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../clustering/kmeans/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../clustering/kmeans/implementation.html">Implementation: K-Means (Lloyd)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../clustering/kmeans/image_segmentation.html">Application: Image Compression and Segmentation</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Deep Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../deep_learning/intro.html">Introduction</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../deep_learning/natural_language_processing/intro.html">Natural Language Processing (NLP)</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../deep_learning/natural_language_processing/vector_semantics_and_embeddings/intro.html">Vector Semantics and Embeddings</a><input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-14"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../deep_learning/natural_language_processing/vector_semantics_and_embeddings/words_and_vectors/intro.html">Words and Vectors</a><input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-15"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../deep_learning/natural_language_processing/vector_semantics_and_embeddings/words_and_vectors/concept.html">Concept</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../deep_learning/natural_language_processing/vector_semantics_and_embeddings/cosine_similarity/intro.html">Cosine Similarity and Notion of Closeness</a><input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-16"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../deep_learning/natural_language_processing/vector_semantics_and_embeddings/cosine_similarity/concept.html">Concept</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../deep_learning/natural_language_processing/vector_semantics_and_embeddings/cosine_similarity/implementation.html">Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../deep_learning/natural_language_processing/vector_semantics_and_embeddings/cosine_similarity/word_similarity.html">Application: Word Similarity</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../deep_learning/natural_language_processing/vector_semantics_and_embeddings/tf_idf/intro.html">Term Frequency-Inverse Document Frequency (TF-IDF)</a><input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-17"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../deep_learning/natural_language_processing/vector_semantics_and_embeddings/tf_idf/concept.html">Concept</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../deep_learning/natural_language_processing/vector_semantics_and_embeddings/tf_idf/implementation.html">Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../deep_learning/natural_language_processing/vector_semantics_and_embeddings/tf_idf/application.html">IMDB Recommender System</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">References, Resources and Roadmap</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../references_resources_roadmap/bibliography.html">Bibliography</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../references_resources_roadmap/resources.html">Resources</a></li>
</ul>

    </div>
</nav>
    </div>
  </div>
  

  
  <div class="sidebar-end-items sidebar-primary__section">
    <div class="sidebar-end-items__item">
    </div>
  </div>

  
  <div id="rtd-footer-container"></div>

      </div>
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

        <div class="bd-content">
          <div class="bd-article-container">
            
            <div class="bd-header-article">
                



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        <label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" data-toggle="tooltip" data-placement="right" title="Toggle primary sidebar">
            <span class="fa-solid fa-bars"></span>
        </label>
    </div>
    <div class="header-article__right">


<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      <li><a href="https://colab.research.google.com/github/gao-hongnan/gaohn-galaxy/blob/main/galaxy/machine_learning/fundamentals/criterions/cross_entropy_loss.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Colab"
>
  

<span class="btn__icon-container">
  
    <img src="../../../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</a>
      
  </ul>
</div>

<button onclick="toggleFullScreen()"
  class="btn btn-sm"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<div class="dropdown dropdown-repository-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      <li><a href="https://github.com/gao-hongnan/gaohn-galaxy" target="_blank"
   class="btn btn-sm dropdown-item"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">repository</span>
</a>
</a>
      
      <li><a href="https://github.com/gao-hongnan/gaohn-galaxy/issues/new?title=Issue%20on%20page%20%2Fmachine_learning/fundamentals/criterions/cross_entropy_loss.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm dropdown-item"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">open issue</span>
</a>
</a>
      
  </ul>
</div>



<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      <li><a href="../../../_sources/machine_learning/fundamentals/criterions/cross_entropy_loss.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</a>
      
      <li>
<button onclick="printPdf(this)"
  class="btn btn-sm dropdown-item"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</a>
      
  </ul>
</div>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary" data-toggle="tooltip" data-placement="left" title="Toggle secondary sidebar">
            <span class="fa-solid fa-list"></span>
        </label>
    </div>
</div>
            </div>
            
            

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Cross Entropy Loss</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#intuition">
   Intuition
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#cross-entropy-as-a-loss-function">
   Cross-Entropy as a Loss Function
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cross-entropy-setup">
     Cross-Entropy Setup
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#z-logits">
     Z Logits
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#softmax">
     Softmax
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#categorical-cross-entropy-loss">
     Categorical Cross Entropy Loss
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#using-dot-product-to-calculate">
       Using Dot Product to Calculate
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#stackoverflow-answer">
   Stackoverflow answer
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#machinelearningmastery-example">
   MachineLearningMastery example
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#further-readings">
   Further Readings
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>

            <article class="bd-article" role="main">
              
  <div class="math notranslate nohighlight">
\[
\newcommand{\ytrue}{\mathbf{y_{\textbf{true}}}}
\newcommand{\yprob}{\mathbf{y_{\textbf{prob}}}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\x}{\mathbf{x}}
\]</div>
<section class="tex2jax_ignore mathjax_ignore" id="cross-entropy-loss">
<h1>Cross Entropy Loss<a class="headerlink" href="#cross-entropy-loss" title="Permalink to this heading">#</a></h1>
<section id="intuition">
<h2>Intuition<a class="headerlink" href="#intuition" title="Permalink to this heading">#</a></h2>
<p>We need to make sense of entropy in the form of a loss function, we have to just enhance our thinking a little.</p>
<p>We define our target to be a one-hot encoded vector of class 0 and 1.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">target</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
<p>Intuitively, take the cat vs dog binary classification again, we made 11 predictions for ONLY ONE query image using different model, and find that after going through many layers, the <strong>softmax</strong> predictions on the logits are as such:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">[</span>
    <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span>
<span class="p">]</span>
</pre></div>
</div>
<p>where the first index corresponds to the logits of class 0 and second index corresponds to the logits of class 1.</p>
<p>For example, <code class="docutils literal notranslate"><span class="pre">[1,</span> <span class="pre">0]</span></code> means the model is 100 percent confident the prediction is a class 0 (cat), and obviously we need to punish the model for spitting nonsense like this.</p>
<p>As we can see in the <code class="docutils literal notranslate"><span class="pre">binary_cross_entropy</span></code> function below, we only need to add up two things. And note that we are hinging on class 1 and therefore <code class="docutils literal notranslate"><span class="pre">y_true[0]</span> <span class="pre">*</span> <span class="pre">log(y_pred[0]+eps)</span></code> goes to 0 as we are just relying on our feedback of probability of class 1.</p>
<p>And in our graph, we can see that as predictions gets more wrong, meaning to say, if the query image is a dog, but our predictions is <code class="docutils literal notranslate"><span class="pre">[1,</span> <span class="pre">0]</span></code>, which says it is a cat, our entropy loss will blow up to very high because</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">y_true</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">log</span><span class="p">(</span><span class="n">y_pred</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="n">eps</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="mi">1</span> <span class="o">*</span> <span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">eps</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">almost</span> <span class="n">infinity</span>
</pre></div>
</div>
<p>Note again we do not calculate for class 0 because</p>
<ol class="arabic simple">
<li><p>We one-hot encoded.</p></li>
<li><p>We only look at class 1’s probability and that’s enough as we can deduce class 0’s probability anyways.</p></li>
</ol>
<p>And conversely, note how the entropy loss goes to 0 if our prediction is say <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>. In general, as our probability for the query image gets close to 1, or in agreement with our class, then our entropy loss becomes smaller.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">torch</span><span class="o">.</span><span class="n">__version__</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;1.13.0+cpu&#39;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">torch_to_np</span><span class="p">(</span><span class="n">tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Convert a PyTorch tensor to a numpy array.</span>

<span class="sd">    Args:</span>
<span class="sd">        tensor (torch.Tensor): The PyTorch tensor to convert.</span>

<span class="sd">    Returns:</span>
<span class="sd">        np.ndarray: The converted numpy array.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tensor</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">compare_equality_two_tensors</span><span class="p">(</span>
    <span class="n">tensor1</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">tensor2</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compare two PyTorch tensors for equality.</span>

<span class="sd">    Args:</span>
<span class="sd">        tensor1 (torch.Tensor): The first PyTorch tensor to compare.</span>
<span class="sd">        tensor2 (torch.Tensor): The second PyTorch tensor to compare.</span>

<span class="sd">    Returns:</span>
<span class="sd">        bool: Whether the two tensors are equal.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">tensor1</span><span class="p">,</span> <span class="n">tensor2</span><span class="p">)):</span>
        <span class="k">return</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="kc">False</span>


<span class="k">def</span> <span class="nf">compare_closeness_two_tensors</span><span class="p">(</span>
    <span class="n">tensor1</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">tensor2</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">epsilon</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
    <span class="o">*</span><span class="n">args</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compare two PyTorch tensors for closeness.</span>

<span class="sd">    Args:</span>
<span class="sd">        tensor1 (torch.Tensor): The first PyTorch tensor to compare.</span>
<span class="sd">        tensor2 (torch.Tensor): The second PyTorch tensor to compare.</span>
<span class="sd">        epsilon (float): The epsilon value to use for closeness.</span>

<span class="sd">    Returns:</span>
<span class="sd">        bool: Whether the two tensors are close.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">tensor1</span><span class="p">,</span> <span class="n">tensor2</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="n">epsilon</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="kc">False</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="cross-entropy-as-a-loss-function">
<h2>Cross-Entropy as a Loss Function<a class="headerlink" href="#cross-entropy-as-a-loss-function" title="Permalink to this heading">#</a></h2>
<section id="cross-entropy-setup">
<h3>Cross-Entropy Setup<a class="headerlink" href="#cross-entropy-setup" title="Permalink to this heading">#</a></h3>
<p>We first understand the idea and intuition of <strong>Cross-Entropy Loss</strong> on one single example. Consider a dataset of cat (class 0) and dogs (class 1) where after one hot encoding we have class 0 to be <span class="math notranslate nohighlight">\([1, 0]\)</span> and class 1 to be <span class="math notranslate nohighlight">\([0, 1]\)</span>.</p>
<p>We first understand the idea and intuition of <strong>Cross-Entropy Loss</strong> on one single example. Consider a dataset of cat (class 0) and dogs (class 1) where after one hot encoding we have class 0 to be <span class="math notranslate nohighlight">\([1, 0]\)</span> and class 1 to be <span class="math notranslate nohighlight">\([0, 1]\)</span>.</p>
<p>We are given the following:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\mathcal{S}\)</span>: The dataset <span class="math notranslate nohighlight">\(\mathcal{S}\)</span> generated from the underlying distribution <span class="math notranslate nohighlight">\(\mathbb{P}_{\mathcal{D}}\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{X}\)</span>: This is a <span class="math notranslate nohighlight">\(2 \times 3 \times 224 \times 224\)</span> tensor of the shape <span class="math notranslate nohighlight">\(B \times C \times H \times W\)</span>. This can be understood as 2 RGB images of size 224.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{y}\)</span>: This is the corresponding ground truth one-hot encoded matrix, we have cat, dog and pig respectively (3 classes):</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \ytrue = \begin{bmatrix}  1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{bmatrix}
    \end{split}\]</div>
</li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{x}_i\)</span>: We represent each <span class="math notranslate nohighlight">\(x_{i}\)</span> as the <span class="math notranslate nohighlight">\(i\)</span>-th image. This can be a <strong>random variable</strong>. Image 1 is a cat, and Image 2 is a pig.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{y}_i\)</span>: The corresponding label for the <span class="math notranslate nohighlight">\(i\)</span>-th sample/image, for sample 1, it is <span class="math notranslate nohighlight">\(\begin{bmatrix} 1 &amp; 0 &amp; 0 \end{bmatrix}\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(P\)</span>: The probability distribution for the ground truth target - when it is <span class="math notranslate nohighlight">\([1, 0, 0]\)</span>, one can understand it as the distribution where cat’s probability is 1, and dog’s probability is 0 and pig 0.</p></li>
<li><p><span class="math notranslate nohighlight">\(Q\)</span>: The probability distribtion of the estimate on the <span class="math notranslate nohighlight">\(\mathbf{x}_i\)</span>, which is say, <span class="math notranslate nohighlight">\([0.9, 0.01, 0.09]\)</span>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">y_true</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
<span class="n">y_true_ohe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y_true_ohe</span><span class="p">)</span>
<span class="n">compare_equality_two_tensors</span><span class="p">(</span><span class="n">y_true_ohe</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">3</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[1, 0, 0],
        [0, 0, 1]])
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>True
</pre></div>
</div>
</div>
</div>
</section>
<section id="z-logits">
<h3>Z Logits<a class="headerlink" href="#z-logits" title="Permalink to this heading">#</a></h3>
<p>Then we have the following, assume a hypothesis <span class="math notranslate nohighlight">\(h_{\theta}(\mathcal{X})\)</span> on the dataset <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> and it outputs logits of the form, note that we are passing in inputs of shape <span class="math notranslate nohighlight">\((2, 3, 224, 224)\)</span> but after some transformations we have the logits to be shape <span class="math notranslate nohighlight">\((2, 3)\)</span>:</p>
<ul class="simple">
<li><p><strong>z_logits:</strong> $<span class="math notranslate nohighlight">\(\textbf{z_logits} = \begin{bmatrix} 1 &amp; 2 &amp; 3 \\ 2 &amp; 4 &amp; 6 \end{bmatrix}\)</span>$</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">z_logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">z_logits</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[1., 2., 3.],
        [2., 4., 6.]])
</pre></div>
</div>
</div>
</div>
</section>
<section id="softmax">
<h3>Softmax<a class="footnote-reference brackets" href="#id2" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a><a class="headerlink" href="#softmax" title="Permalink to this heading">#</a></h3>
<p>The softmax function takes as input a vector <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> of <span class="math notranslate nohighlight">\(K\)</span> real numbers, and normalizes it into a probability distribution consisting of <span class="math notranslate nohighlight">\(K\)</span> probabilities proportional to the exponentials of the input numbers. That is, prior to applying softmax, some vector components could be negative, or greater than one; and might not sum to 1; but after applying softmax, each component will be in the interval of 0 and 1, and the components will add up to 1, so that they can be interpreted as probabilities. Furthermore, the larger input components will correspond to larger probabilities.</p>
<hr class="docutils" />
<p>The standard (unit) softmax function</p>
<div class="math notranslate nohighlight">
\[\sigma : \mathbb{R}^K\to (0,1)^K\]</div>
<p>is defined when <span class="math notranslate nohighlight">\(K\)</span> is greater than one by the formula:</p>
<div class="math notranslate nohighlight">
\[
\sigma(\mathbf{z})_i = \frac{e^{z_i}}{\sum_{j=1}^K e^{z_j}} \ \ \ \ \text{ for } i = 1, \dotsc , K \text{ and } \mathbf z=(z_1,\dotsc,z_K) \in \mathbb{R}^K
\]</div>
<p>In linear algebra notation, the <span class="math notranslate nohighlight">\(\sigma\)</span> soft(arg)max function takes in a <strong>real vector</strong> <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> from the <span class="math notranslate nohighlight">\(K\)</span> dimensional space, indicating that the vector <span class="math notranslate nohighlight">\(\mathbf{z} \in \mathbb{R}^K\)</span> has <span class="math notranslate nohighlight">\(K\)</span> number of elements, and maps to the <span class="math notranslate nohighlight">\(K\)</span> dimensional <strong>0-1</strong> space, which is also a vector of <span class="math notranslate nohighlight">\(K\)</span> elements; in other words, given an input vector <span class="math notranslate nohighlight">\(\mathbf{z} \in \mathbb{R}^K\)</span>, the soft(arg)max maps it to <span class="math notranslate nohighlight">\(\sigma(\mathbf{z}) \in (0,1)^K\)</span>.</p>
<p>Now we break down what the soft(arg)max function actually does. It applies the standard exponential function to <strong>each</strong> element <span class="math notranslate nohighlight">\(z_i\)</span> of the input vector <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> and normalizes these values by dividing by the sum of all these exponentials; this normalization ensures that the sum of the components of the output vector <span class="math notranslate nohighlight">\(\sigma(\mathbf z)\)</span> is 1.</p>
<p>After applying softmax to the logits we have:</p>
<ul class="simple">
<li><p><strong>y_prob = z_softargmax:</strong> $<span class="math notranslate nohighlight">\(\yprob = \textbf{z_softargmax} = \begin{bmatrix} 0.09 &amp; 0.2447 &amp; 0.6652 \\ 0.0159 &amp; 0.1173 &amp; 0.8668\end{bmatrix}\)</span>$</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compute_softargmax</span><span class="p">(</span><span class="n">z</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute the softargmax of a PyTorch tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        z (torch.Tensor): The PyTorch tensor to compute the softargmax of.</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: The softargmax of the PyTorch tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># the output matrix should be the same size as the input matrix</span>
    <span class="n">z_softargmax</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">z</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">row_index</span><span class="p">,</span> <span class="n">each_row</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
        <span class="n">denominator</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">each_row</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">element_index</span><span class="p">,</span> <span class="n">each_element</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">each_row</span><span class="p">):</span>
            <span class="n">z_softargmax</span><span class="p">[</span><span class="n">row_index</span><span class="p">,</span> <span class="n">element_index</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">each_element</span><span class="p">)</span> <span class="o">/</span> <span class="n">denominator</span>
            <span class="p">)</span>

    <span class="k">assert</span> <span class="n">compare_closeness_two_tensors</span><span class="p">(</span>
        <span class="n">z_softargmax</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)(</span><span class="n">z</span><span class="p">),</span> <span class="mf">1e-15</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">z_softargmax</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">z_softargmax</span> <span class="o">=</span> <span class="n">compute_softargmax</span><span class="p">(</span><span class="n">z_logits</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">z_softargmax</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[0.0900, 0.2447, 0.6652],
        [0.0159, 0.1173, 0.8668]])
</pre></div>
</div>
</div>
</div>
</section>
<section id="categorical-cross-entropy-loss">
<h3>Categorical Cross Entropy Loss<a class="headerlink" href="#categorical-cross-entropy-loss" title="Permalink to this heading">#</a></h3>
<p>We will start with this because the Binary Cross Entropy Loss is merely a special case of this. Finding the full compact formula for this took me a while since most tutorials cover the binary case.</p>
<p>Given <span class="math notranslate nohighlight">\(N\)</span> samples, and <span class="math notranslate nohighlight">\(C\)</span> classes, the <strong>Categorical Cross Entropy Loss</strong> is the average loss across <span class="math notranslate nohighlight">\(N\)</span> samples, given by:</p>
<div class="math notranslate nohighlight">
\[\textbf{CE}(\ytrue, \yprob) = -\dfrac{1}{N}\sum_{i=1}^N\sum_{c=1}^C \mathbb{1}_{\y_{i} \in C_c} \log\left(p_{\textbf{model}}[\y_i \in C_c]\right)\]</div>
<p>where</p>
<ul class="simple">
<li><p>The outer loop <span class="math notranslate nohighlight">\(i\)</span> iterates over <span class="math notranslate nohighlight">\(N\)</span> observations/samples.</p></li>
<li><p>The inner loop <span class="math notranslate nohighlight">\(c\)</span> iterates over <span class="math notranslate nohighlight">\(C\)</span> classes.</p></li>
<li><p><span class="math notranslate nohighlight">\(\y_i\)</span> represents the true label (in this formula it should be one-hot encoded) of the <span class="math notranslate nohighlight">\(i\)</span>-th sample.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbb{1}_{y_{i} \in C_c}\)</span> is an indicator function, simply put, for sample <span class="math notranslate nohighlight">\(i\)</span>, if the true label <span class="math notranslate nohighlight">\(\y_i\)</span> belongs to the <span class="math notranslate nohighlight">\(c\)</span>-th category, then we assign a <span class="math notranslate nohighlight">\(1\)</span>, else <span class="math notranslate nohighlight">\(0\)</span>. We can see it with an example later.</p></li>
<li><p><span class="math notranslate nohighlight">\(\left(p_{\textbf{model}}[\y_i \in C_c]\right)\)</span> means the probability predicted by the model for the <span class="math notranslate nohighlight">\(i\)</span>-th observation that belongs to the <span class="math notranslate nohighlight">\(c\)</span>-th class category.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\ytrue = \begin{bmatrix}  1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{bmatrix}
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\textbf{z_logits} = \begin{bmatrix} 1 &amp; 2 &amp; 3 \\ 2 &amp; 4 &amp; 6 \end{bmatrix}
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\yprob = \textbf{z_softargmax} = \begin{bmatrix} 0.09 &amp; 0.2447 &amp; 0.6652 \\ 0.0159 &amp; 0.1173 &amp; 0.8668\end{bmatrix}
\end{split}\]</div>
<ul class="simple">
<li><p>We first look at the first sample, index <span class="math notranslate nohighlight">\(i = 1\)</span>:</p>
<ul>
<li><p>We have the one-hot encoded label for first sample to be <span class="math notranslate nohighlight">\(\y_1 = \begin{bmatrix} 1 &amp; 0 &amp; 0 \end{bmatrix}\)</span>. This means the label is a cat since the sequence is cat, dog and pig, and thus 1, 0, 0 corresponds to cat 1, dog 0 and pig 0.</p></li>
<li><p>We have the one-hot encoded probability predicted by the model for the first sample to be <span class="math notranslate nohighlight">\(\hat{\y_1} = \begin{bmatrix} 0.09 &amp; 0.2447 &amp; 0.6652 \end{bmatrix}\)</span>. This means the probability associated with this sample <span class="math notranslate nohighlight">\(1\)</span> is probability of a cat from the model is <span class="math notranslate nohighlight">\(9\%\)</span>, a dog <span class="math notranslate nohighlight">\(24.47\%\)</span> and a pig <span class="math notranslate nohighlight">\(66.52\%\)</span>.</p></li>
</ul>
</li>
</ul>
<hr class="docutils" />
<ul>
<li><p>With these information, we go on to the first outer loop’s content:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\sum_{c=1}^C \mathbb{1}_{\y_{i} \in C_c} \log\left(p_{\textbf{model}}[\y_i \in C_c]\right)\)</span></p></li>
<li><p>We are looping through the classes, which in this case is loop from <span class="math notranslate nohighlight">\(c=1\)</span> to <span class="math notranslate nohighlight">\(c=3\)</span> since <span class="math notranslate nohighlight">\(C=3\)</span> (3 classes).</p></li>
<li><p><span class="math notranslate nohighlight">\(c = 1\)</span>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbb{1}_{\y_{i} \in C_c} = \mathbb{1}_{\y_{1} \in C_1}\)</span>: The true label for the first sample is actually the first class, and hence belongs to the <span class="math notranslate nohighlight">\(c=1\)</span> category, so our indicator function returns me a <span class="math notranslate nohighlight">\(1\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\log\left(p_{\textbf{model}}[\y_i \in C_c]\right) = \log\left(p_{\textbf{model}}[\y_1 \in C_1]\right)\)</span>: Applies the log function (natural log here) to the each probability associated with the class. So in this case, since <span class="math notranslate nohighlight">\(c=1\)</span>, we apply the log function to the first entry <span class="math notranslate nohighlight">\(0.09\)</span>. We get <span class="math notranslate nohighlight">\(\log(0.09) = -2.4079\)</span></p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(c = 2\)</span>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbb{1}_{\y_{i} \in C_c} = \mathbb{1}_{\y_{1} \in C_2}\)</span>: The true label for the first sample is actually the first class, and hence does not belong to the <span class="math notranslate nohighlight">\(c=2\)</span> category, so our indicator function returns me a <span class="math notranslate nohighlight">\(0\)</span>.</p></li>
<li><p>Regardless, the log of this probability is <span class="math notranslate nohighlight">\(\log(0.2447) = -1.4076\)</span></p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(c = 3\)</span>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbb{1}_{\y_{i} \in C_c} = \mathbb{1}_{\y_{1} \in C_3}\)</span>: The true label for the first sample is actually the first class, and hence does not belong to the <span class="math notranslate nohighlight">\(c=3\)</span> category, so our indicator function returns me a <span class="math notranslate nohighlight">\(0\)</span>.</p></li>
<li><p>Regardless, the log of this probability is <span class="math notranslate nohighlight">\(\log(0.6652) = -0.4076\)</span></p></li>
</ul>
</li>
<li><p>Lastly, we sum them up and get <span class="math notranslate nohighlight">\(-2.4076 + 0 + 0 = -2.4076\)</span>, note here we only have the first entry! The second and third are <span class="math notranslate nohighlight">\(0\)</span>.</p></li>
<li><p>In code, this corresponds to the following:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>    <span class="c1"># loop = 1</span>
    <span class="n">current_sample_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">each_y_true_element</span><span class="p">,</span> <span class="n">each_y_prob_element</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span>
        <span class="n">each_y_true_one_hot_vector</span><span class="p">,</span> <span class="n">each_y_prob_one_hot_vector</span>
    <span class="p">):</span>
        <span class="c1"># Indicator Function</span>
        <span class="k">if</span> <span class="n">each_y_true_element</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">current_sample_loss</span> <span class="o">+=</span> <span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">each_y_prob_element</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">current_sample_loss</span> <span class="o">+=</span> <span class="mi">0</span>
</pre></div>
</div>
</li>
<li><p>Bonus: If you realize this is just a vector dot product: <span class="math notranslate nohighlight">\(\begin{bmatrix} 1 &amp; 0 &amp; 0 \end{bmatrix} \cdot \log\left(\begin{bmatrix} 0.09 \\ 0.2447 \\ 0.6652 \end{bmatrix}\right) = \begin{bmatrix} 1 &amp; 0 &amp; 0 \end{bmatrix} \cdot \left(\begin{bmatrix} -2.4076 \\ -1.4076 \\ -0.4076 \end{bmatrix}\right) = -2.4076\)</span></p></li>
</ul>
</li>
</ul>
<hr class="docutils" />
<ul class="simple">
<li><p>We now look at the second sample, index <span class="math notranslate nohighlight">\(i = 2\)</span>:</p>
<ul>
<li><p>We have the one-hot encoded label for second sample to be <span class="math notranslate nohighlight">\(\y_2 = \begin{bmatrix} 0 &amp; 0 &amp; 1 \end{bmatrix}\)</span>. This means the label is a pig since the sequence is cat, dog and pig, and thus 0, 0, 1 corresponds to cat 0, dog 0 and pig 1.</p></li>
<li><p>We have the one-hot encoded probability predicted by the model for the second sample to be <span class="math notranslate nohighlight">\(\hat{\y_2} = \begin{bmatrix} 0.0159 &amp; 0.1173 &amp; 0.8868 \end{bmatrix}\)</span>. This means the probability associated with this sample <span class="math notranslate nohighlight">\(2\)</span> is probability of a cat from the model is <span class="math notranslate nohighlight">\(1.59\%\)</span>, a dog <span class="math notranslate nohighlight">\(11.73\%\)</span> and a pig <span class="math notranslate nohighlight">\(88.68\%\)</span>.</p></li>
</ul>
</li>
</ul>
<hr class="docutils" />
<ul>
<li><p>With these information, we go on to the second outer loop’s content:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\sum_{c=2}^C \mathbb{1}_{\y_{i} \in C_c} \log\left(p_{\textbf{model}}[\y_i \in C_c]\right)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(c = 2\)</span>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbb{1}_{\y_{i} \in C_c} = \mathbb{1}_{\y_{2} \in C_1}\)</span>: The true label for the second sample is actually the third class, and hence belongs to the <span class="math notranslate nohighlight">\(c=3\)</span> category, so our indicator function returns me a <span class="math notranslate nohighlight">\(0\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\log\left(p_{\textbf{model}}[\y_i \in C_c]\right)\)</span>: Applies the log function (natural log here) to the each probability associated with the class. So in this case, since <span class="math notranslate nohighlight">\(c=1\)</span>, we apply the log function to the first entry <span class="math notranslate nohighlight">\(0.0159\)</span>. We get <span class="math notranslate nohighlight">\(\log(0.0159) = -4.1429\)</span></p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(c = 2\)</span>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbb{1}_{\y_{i} \in C_c} = \mathbb{1}_{\y_{2} \in C_2}\)</span>: The true label for the second sample is actually the third class, and hence does not belong to the <span class="math notranslate nohighlight">\(c=2\)</span> category, so our indicator function returns me a <span class="math notranslate nohighlight">\(0\)</span>.</p></li>
<li><p>Regardless, the log of this probability is <span class="math notranslate nohighlight">\(\log(0.1173) = -2.1429\)</span></p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(c = 3\)</span>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbb{1}_{\y_{i} \in C_c} = \mathbb{1}_{\y_{2} \in C_3}\)</span>: The true label for the second sample is actually the third class, so our indicator function returns me a <span class="math notranslate nohighlight">\(1\)</span>.</p></li>
<li><p>The log of this probability is <span class="math notranslate nohighlight">\(\log(0.6652) = -0.1429\)</span></p></li>
</ul>
</li>
<li><p>Lastly, we sum them up and get <span class="math notranslate nohighlight">\(0 + 0 + (-0.1429) = -0.1429\)</span>, note here we only have the third entry! The first and second entries are <span class="math notranslate nohighlight">\(0\)</span>.</p></li>
<li><p>In code, this corresponds to the following:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>    <span class="c1"># loop = 2</span>
    <span class="n">current_sample_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">each_y_true_element</span><span class="p">,</span> <span class="n">each_y_prob_element</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span>
        <span class="n">each_y_true_one_hot_vector</span><span class="p">,</span> <span class="n">each_y_prob_one_hot_vector</span>
    <span class="p">):</span>
        <span class="c1"># Indicator Function</span>
        <span class="k">if</span> <span class="n">each_y_true_element</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">current_sample_loss</span> <span class="o">+=</span> <span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">each_y_prob_element</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">current_sample_loss</span> <span class="o">+=</span> <span class="mi">0</span>
</pre></div>
</div>
</li>
<li><p>Bonus: If you realize this is just a vector dot product: <span class="math notranslate nohighlight">\(\begin{bmatrix} 0 &amp; 0 &amp; 1 \end{bmatrix} \cdot \log\left(\begin{bmatrix} 0.0159 \\ 0.1173 \\ 0.8868\end{bmatrix}\right) = \begin{bmatrix} 0 &amp; 0 &amp; 1 \end{bmatrix} \cdot \left(\begin{bmatrix} -4.1429 \\ -2.1429 \\ -0.1429 \end{bmatrix}\right) = -0.1429\)</span></p></li>
</ul>
</li>
</ul>
<p>To summarize the whole process:</p>
<ul class="simple">
<li><p>set <code class="docutils literal notranslate"><span class="pre">all_samples_loss</span> <span class="pre">=</span> <span class="pre">0</span></code></p></li>
<li><p>Start Outer Loop:</p>
<ul>
<li><p>loop over first sample <code class="docutils literal notranslate"><span class="pre">i</span> <span class="pre">=</span> <span class="pre">1</span></code> (actually index is 0 in python):</p>
<ul>
<li><p>set <code class="docutils literal notranslate"><span class="pre">current_sample_loss</span> <span class="pre">=</span> <span class="pre">0</span></code></p></li>
<li><p>loop over <span class="math notranslate nohighlight">\(C=3\)</span> classes:</p>
<ul>
<li><p>when <span class="math notranslate nohighlight">\(c = 1\)</span>: the loss associated is <span class="math notranslate nohighlight">\(-2.4076\)</span>. Add this to <code class="docutils literal notranslate"><span class="pre">current_sample_loss</span></code>.</p></li>
<li><p>when <span class="math notranslate nohighlight">\(c = 2\)</span>: the loss associated is <span class="math notranslate nohighlight">\(0\)</span>. Add this to <code class="docutils literal notranslate"><span class="pre">current_sample_loss</span></code>.</p></li>
<li><p>when <span class="math notranslate nohighlight">\(c = 3\)</span>: the loss associated is <span class="math notranslate nohighlight">\(0\)</span>. Add this to <code class="docutils literal notranslate"><span class="pre">current_sample_loss</span></code>.</p></li>
</ul>
</li>
<li><p>end first loop: update <code class="docutils literal notranslate"><span class="pre">all_samples_loss</span></code> by adding <code class="docutils literal notranslate"><span class="pre">current_sample_loss</span></code> to be <code class="docutils literal notranslate"><span class="pre">all_samples_loss</span> <span class="pre">=</span> <span class="pre">-2.4076</span></code>.</p></li>
</ul>
</li>
<li><p>loop over second sample <code class="docutils literal notranslate"><span class="pre">i</span> <span class="pre">=</span> <span class="pre">2</span></code> (actually index is 1 in python):</p>
<ul>
<li><p>set <code class="docutils literal notranslate"><span class="pre">current_sample_loss</span> <span class="pre">=</span> <span class="pre">0</span></code></p></li>
<li><p>loop over <span class="math notranslate nohighlight">\(C=3\)</span> classes:</p>
<ul>
<li><p>when <span class="math notranslate nohighlight">\(c = 1\)</span>: the loss associated is <span class="math notranslate nohighlight">\(0\)</span>. Add this to <code class="docutils literal notranslate"><span class="pre">current_sample_loss</span></code>.</p></li>
<li><p>when <span class="math notranslate nohighlight">\(c = 2\)</span>: the loss associated is <span class="math notranslate nohighlight">\(0\)</span>. Add this to <code class="docutils literal notranslate"><span class="pre">current_sample_loss</span></code>.</p></li>
<li><p>when <span class="math notranslate nohighlight">\(c = 3\)</span>: the loss associated is <span class="math notranslate nohighlight">\(-0.1429\)</span>. Add this to <code class="docutils literal notranslate"><span class="pre">current_sample_loss</span></code>.</p></li>
</ul>
</li>
<li><p>end second loop: update <code class="docutils literal notranslate"><span class="pre">all_samples_loss</span></code> by adding <code class="docutils literal notranslate"><span class="pre">current_sample_loss</span></code> to be <code class="docutils literal notranslate"><span class="pre">all_samples_loss</span> <span class="pre">=</span> <span class="pre">-2.4076</span> <span class="pre">+</span> <span class="pre">(-0.1429)</span> <span class="pre">=</span> <span class="pre">-2.5505</span></code>.</p></li>
</ul>
</li>
</ul>
</li>
<li><p>End all loops: You can multiply by negative <span class="math notranslate nohighlight">\(-1\)</span> to make <code class="docutils literal notranslate"><span class="pre">all_samples_loss</span></code> positive and get <code class="docutils literal notranslate"><span class="pre">all_samples_average_loss</span> <span class="pre">=</span> <span class="pre">all_samples_loss</span> <span class="pre">/</span> <span class="pre">num_of_samples</span> <span class="pre">=</span> <span class="pre">2.5505</span> <span class="pre">/</span> <span class="pre">2</span> <span class="pre">=</span> <span class="pre">1.2753</span></code>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compute_categorical_cross_entropy_loss</span><span class="p">(</span>
    <span class="n">y_true</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">y_prob</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute the categorical cross entropy loss between two PyTorch tensors.</span>

<span class="sd">    Args:</span>
<span class="sd">        y_true (torch.Tensor): The true labels.</span>
<span class="sd">        y_prob (torch.Tensor): The predicted labels.</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: The categorical cross entropy loss.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">all_samples_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">each_y_true_one_hot_vector</span><span class="p">,</span> <span class="n">each_y_prob_one_hot_vector</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span>
        <span class="n">y_true</span><span class="p">,</span> <span class="n">y_prob</span>
    <span class="p">):</span>
        <span class="n">current_sample_loss</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">each_y_true_element</span><span class="p">,</span> <span class="n">each_y_prob_element</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span>
            <span class="n">each_y_true_one_hot_vector</span><span class="p">,</span> <span class="n">each_y_prob_one_hot_vector</span>
        <span class="p">):</span>
            <span class="c1"># in case y_prob has elements that is 0 or very small, then torch.log(0) might go to -inf</span>
            <span class="n">each_y_prob_element</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">each_y_prob_element</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mf">1.0e-20</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mf">1.0</span><span class="o">-</span><span class="mf">1.0e-20</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> 
            <span class="c1"># Indicator Function</span>
            <span class="k">if</span> <span class="n">each_y_true_element</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">current_sample_loss</span> <span class="o">+=</span> <span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">each_y_prob_element</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">current_sample_loss</span> <span class="o">+=</span> <span class="mi">0</span>

        <span class="n">all_samples_loss</span> <span class="o">+=</span> <span class="n">current_sample_loss</span>

    <span class="n">all_samples_average_loss</span> <span class="o">=</span> <span class="n">all_samples_loss</span> <span class="o">/</span> <span class="n">y_true</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">all_samples_average_loss</span>
</pre></div>
</div>
</div>
</div>
<section id="using-dot-product-to-calculate">
<h4>Using Dot Product to Calculate<a class="headerlink" href="#using-dot-product-to-calculate" title="Permalink to this heading">#</a></h4>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\textbf{CE}(\ytrue, \yprob) &amp;= -\dfrac{1}{N}\sum_{i=1}^N\sum_{c=1}^C \mathbb{1}_{\y_{i} \in C_c} \log\left(p_{\textbf{model}}[\y_i \in C_c]\right)\\
                            &amp;= \textbf{SUM}\left[\textbf{diag}\left(\ytrue \cdot -\log(\yprob)^\top\right)\right]
\end{aligned}\end{split}\]</div>
<p>We can easily see</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\ytrue = \begin{bmatrix}  1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{bmatrix}
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\textbf{z_logits} = \begin{bmatrix} 1 &amp; 2 &amp; 3 \\ 2 &amp; 4 &amp; 6 \end{bmatrix}
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\yprob = \textbf{z_softargmax} = \begin{bmatrix} 0.09 &amp; 0.2447 &amp; 0.6652 \\ 0.0159 &amp; 0.1173 &amp; 0.8668\end{bmatrix}
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}\log(\yprob) = \begin{bmatrix} 2.4076 &amp; 1.4076 &amp; 0.4076 \\ 4.1429 &amp; 2.1429 &amp; 0.1429 \end{bmatrix}\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\ytrue \cdot -\log(\yprob)^\top = \begin{bmatrix}  1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{bmatrix} \cdot \begin{bmatrix} 2.4076 &amp; 4.1429 \\ 1.4076 &amp; 2.1429 \\ 0.4076  &amp; 0.1429 \end{bmatrix} = \begin{bmatrix} 2.4076 &amp; 4.1429 \\ 0.4076 &amp; 0.1429 \end{bmatrix}
\end{split}\]</div>
<p>The matrix <span class="math notranslate nohighlight">\(\ytrue \cdot -\log(\yprob)^\top\)</span> diagonals are what we need, where we sum them up and divide by the number of samples. That is <span class="math notranslate nohighlight">\(\frac{2.4076+0.1429}{2} = \frac{2.5505}{2} = 1.2753\)</span>.</p>
<p>This makes sense because the one hot encoded <span class="math notranslate nohighlight">\(\ytrue\)</span> vector guarantees only the indicator functions 1 gets activated and the rest gets zeroed out. Furthermore, we are only interested in the diagonal of the matrix as we are only interested in the dot product between the <span class="math notranslate nohighlight">\(i\)</span>-th row and the <span class="math notranslate nohighlight">\(i\)</span>-th column of <span class="math notranslate nohighlight">\(\ytrue\)</span> and <span class="math notranslate nohighlight">\(-\log(\yprob)^\top\)</span> respectively.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compute_categorical_cross_entropy_loss_dot_product</span><span class="p">(</span>
    <span class="n">y_true</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">y_prob</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute the categorical cross entropy loss between two PyTorch tensors using dot product.</span>

<span class="sd">    Args:</span>
<span class="sd">        y_true (torch.Tensor): The true labels in one-hot form.</span>
<span class="sd">        y_prob (torch.Tensor): The predicted labels in one-hot form.</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: The categorical cross entropy loss.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">y_true</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span> <span class="n">torch</span><span class="o">.</span><span class="n">neg</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y_prob</span><span class="o">.</span><span class="n">float</span><span class="p">())</span><span class="o">.</span><span class="n">T</span><span class="p">))</span>
    <span class="n">all_loss_vector</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">diagonal</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">all_loss_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">all_loss_vector</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">average_loss</span> <span class="o">=</span> <span class="n">all_loss_sum</span> <span class="o">/</span> <span class="n">y_true</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">average_loss</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">compute_categorical_cross_entropy_loss</span><span class="p">(</span><span class="n">y_true</span> <span class="o">=</span> <span class="n">y_true_ohe</span><span class="p">,</span> <span class="n">y_prob</span> <span class="o">=</span> <span class="n">compute_softargmax</span><span class="p">(</span><span class="n">z_logits</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(1.2753)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">compute_categorical_cross_entropy_loss_dot_product</span><span class="p">(</span><span class="n">y_true</span> <span class="o">=</span> <span class="n">y_true_ohe</span><span class="p">,</span> <span class="n">y_prob</span> <span class="o">=</span> <span class="n">compute_softargmax</span><span class="p">(</span><span class="n">z_logits</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(1.2753)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()(</span><span class="n">z_logits</span><span class="p">,</span> <span class="n">y_true</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(1.2753)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">z_logits</span><span class="p">,</span> <span class="n">z_softargmax</span><span class="p">,</span> <span class="n">y_true_ohe</span><span class="p">,</span> <span class="n">y_true</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(tensor([[1., 2., 3.],
         [2., 4., 6.]]),
 tensor([[0.0900, 0.2447, 0.6652],
         [0.0159, 0.1173, 0.8668]]),
 tensor([[1, 0, 0],
         [0, 0, 1]]),
 tensor([0, 2]))
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">z_softargmax</span><span class="p">))</span><span class="o">.</span><span class="n">T</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[2.4076, 4.1429],
        [1.4076, 2.1429],
        [0.4076, 0.1429]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">m</span> <span class="o">=</span> <span class="n">y_true_ohe</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">matmul</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">z_softargmax</span><span class="p">))</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="n">m</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[2.4076, 4.1429],
        [0.4076, 0.1429]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">diagonal</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(2.5505)
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="stackoverflow-answer">
<h2>Stackoverflow answer<a class="headerlink" href="#stackoverflow-answer" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">cross_entropy</span><span class="p">(</span><span class="n">y_true</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-12</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        https://stackoverflow.com/questions/47377222/what-is-the-problem-with-my-implementation-of-the-cross-entropy-function</span>
<span class="sd">        Computes cross entropy between targets (encoded as one-hot vectors)</span>
<span class="sd">        and y_pred.</span>
<span class="sd">        Input: y_pred (N, k) ndarray</span>
<span class="sd">               y_true (N, k) ndarray</span>
<span class="sd">        Returns: scalar</span>
<span class="sd">        predictions = np.array([[0.25,0.25,0.25,0.25],</span>
<span class="sd">                            [0.01,0.01,0.01,0.96]])</span>
<span class="sd">    targets = np.array([[0,0,0,1],</span>
<span class="sd">                       [0,0,0,1]])</span>
<span class="sd">                       ans = 0.71355817782  #Correct answer</span>
<span class="sd">    x = cross_entropy(predictions, targets)</span>
<span class="sd">    print(np.isclose(x,ans))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">epsilon</span><span class="p">)</span>
    <span class="c1"># take note that y_pred is of shape 1 x n_samples as stated in our framework</span>
    <span class="n">n_samples</span> <span class="o">=</span> <span class="n">y_pred</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># cross entropy function</span>
    <span class="n">cross_entropy_function</span> <span class="o">=</span> <span class="n">y_true</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y_true</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">)</span>
    <span class="c1"># cross entropy function here is same shape as y_true and y_pred since we are</span>
    <span class="c1"># just performing element wise operations on both of them.</span>
    <span class="k">assert</span> <span class="n">cross_entropy_function</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>

    <span class="c1"># we sum up all the loss for each individual sample</span>
    <span class="n">total_cross_entropy_loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">cross_entropy_function</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">total_cross_entropy_loss</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span>

    <span class="c1"># we then average out the total loss across m samples, but we squeeze it to</span>
    <span class="c1"># make it a scalar; squeeze along axis = None since there is no column axix</span>
    <span class="n">average_cross_entropy_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">total_cross_entropy_loss</span> <span class="o">/</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

    <span class="c1"># cross_entropy_loss = -np.sum(y_true * np.log(y_pred)) / n_samples</span>
    <span class="c1"># print(np.isclose(average_cross_entropy_loss, cross_entropy_loss))</span>
    <span class="k">return</span> <span class="n">average_cross_entropy_loss</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]),</span> <span class="n">y_pred</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">]]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array(0.01005034)
</pre></div>
</div>
</div>
</div>
</section>
<section id="machinelearningmastery-example">
<h2>MachineLearningMastery example<a class="headerlink" href="#machinelearningmastery-example" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">log</span>

<span class="c1"># calculate cross-entropy</span>
<span class="k">def</span> <span class="nf">cross_entropy</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">ets</span><span class="o">=</span><span class="mf">1e-15</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="nb">sum</span><span class="p">([</span><span class="n">p</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="n">log</span><span class="p">(</span><span class="n">q</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">+</span><span class="n">ets</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="p">))])</span>

<span class="k">def</span> <span class="nf">binary_cross_entropy</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-15</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="p">(</span><span class="n">y_true</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">log</span><span class="p">(</span><span class="n">y_pred</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="n">eps</span><span class="p">)</span> <span class="o">+</span> <span class="n">y_true</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">log</span><span class="p">(</span><span class="n">y_pred</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="n">eps</span><span class="p">))</span>

<span class="c1"># define the target distribution for two events</span>
<span class="n">target</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]</span>
<span class="c1"># define probabilities for the first event</span>
<span class="n">probs</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="c1"># cat is 0% and dog is 100% confidence</span>
    <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span>
<span class="p">]</span>

<span class="c1"># create probability distributions for the two events</span>
<span class="c1"># dists = [[1.0 - p, p] for p in probs]</span>
<span class="c1"># calculate cross-entropy for each distribution</span>
<span class="n">ents</span> <span class="o">=</span> <span class="p">[</span><span class="n">binary_cross_entropy</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">target</span><span class="p">,</span> <span class="n">y_pred</span><span class="o">=</span><span class="n">d</span><span class="p">)</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">probs</span><span class="p">]</span>
<span class="c1"># plot probability distribution vs cross-entropy</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">p</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">probs</span><span class="p">],</span> <span class="n">ents</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;.&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Probability Distribution vs Cross-Entropy&#39;</span><span class="p">)</span>
<span class="c1">#pyplot.xticks([1-p for p in probs], [&#39;[%.1f,%.1f]&#39;%(d[0],d[1]) for d in dists], rotation=70)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">bottom</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Probability Distribution for Query Image when ground truth is 1&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Cross-Entropy (nats)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/9428b4157bb1556c865973c6369079c4c5160aa36b214208fa57750e711001e9.png" src="../../../_images/9428b4157bb1556c865973c6369079c4c5160aa36b214208fa57750e711001e9.png" />
</div>
</div>
</section>
<section id="further-readings">
<h2>Further Readings<a class="headerlink" href="#further-readings" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://medium.com/analytics-vidhya/understanding-entropy-the-golden-measurement-of-machine-learning-4ea97c663dc3#:~:text=By%20using%20entropy%20in%20machine,be%20desired%20in%20model%2Dbuilding.">analytics-vidhya-entropy-loss</a></p></li>
<li><p><a class="reference external" href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/">cross-entropy-loss-machine-learning-mastery</a></p></li>
<li><p><a class="reference external" href="https://towardsdatascience.com/entropy-how-decision-trees-make-decisions-2946b9c18c8">entropy-how-decision-trees-make-decisions</a></p></li>
<li><p><a class="reference external" href="https://ramsane.github.io/articles/cross-entropy-explained-with-entropy-and-kl-divergence">https://ramsane.github.io/articles/cross-entropy-explained-with-entropy-and-kl-divergence</a></p></li>
<li><p><a class="reference external" href="https://neptune.ai/blog/cross-entropy-loss-and-its-applications-in-deep-learning">https://neptune.ai/blog/cross-entropy-loss-and-its-applications-in-deep-learning</a></p></li>
</ul>
<p><a class="reference external" href="https://stackoverflow.com/questions/41990250/what-is-cross-entropy/41990932">https://stackoverflow.com/questions/41990250/what-is-cross-entropy/41990932</a>
<a class="reference external" href="https://towardsdatascience.com/cross-entropy-loss-function-f38c4ec8643e">https://towardsdatascience.com/cross-entropy-loss-function-f38c4ec8643e</a>
<a class="reference external" href="https://neptune.ai/blog/cross-entropy-loss-and-its-applications-in-deep-learning">https://neptune.ai/blog/cross-entropy-loss-and-its-applications-in-deep-learning</a>
<a class="reference external" href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/">https://machinelearningmastery.com/cross-entropy-for-machine-learning/</a>
<a class="reference external" href="https://d2l.ai/chapter_linear-networks/softmax-regression.html">https://d2l.ai/chapter_linear-networks/softmax-regression.html</a>
<a class="reference external" href="https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/geometry-linear-algebraic-ops.html#invertibility">https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/geometry-linear-algebraic-ops.html#invertibility</a>
<a class="reference external" href="https://leimao.github.io/blog/Cross-Entropy-KL-Divergence-MLE/">https://leimao.github.io/blog/Cross-Entropy-KL-Divergence-MLE/</a>
<a class="reference external" href="https://towardsdatascience.com/cross-entropy-for-classification-d98e7f974451">https://towardsdatascience.com/cross-entropy-for-classification-d98e7f974451</a>
<a class="reference external" href="https://gist.github.com/yang-zhang/217dcc6ae9171d7a46ce42e215c1fee0">https://gist.github.com/yang-zhang/217dcc6ae9171d7a46ce42e215c1fee0</a>
<a class="reference external" href="https://datascience.stackexchange.com/questions/20296/cross-entropy-loss-explanation">https://datascience.stackexchange.com/questions/20296/cross-entropy-loss-explanation</a>
<a class="reference external" href="https://ramsane.github.io/articles/cross-entropy-explained-with-entropy-and-kl-divergence">https://ramsane.github.io/articles/cross-entropy-explained-with-entropy-and-kl-divergence</a></p>
<hr class="footnotes docutils" />
<aside class="footnote brackets" id="id2" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Softmax_function">https://en.wikipedia.org/wiki/Softmax_function</a></p>
</aside>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./machine_learning\fundamentals\criterions"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

            </article>
            

            
            
            <footer class="bd-footer-article">
                <!-- Previous / next buttons -->
<div class='prev-next-area'>
  <a class='left-prev' id="prev-link" href="concept.html" title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
          <p class="prev-next-subtitle">previous</p>
          <p class="prev-next-title">Concept</p>
      </div>
  </a>
  <a class='right-next' id="next-link" href="focal_loss.html" title="next page">
  <div class="prev-next-info">
      <p class="prev-next-subtitle">next</p>
      <p class="prev-next-title">Focal Loss</p>
  </div>
  <i class="fa-solid fa-angle-right"></i>
  </a>
</div>
            </footer>
            
          </div>
          
          
          
            <div class="bd-sidebar-secondary bd-toc">
              
<div class="toc-item">
  
<div class="tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
</div>
<nav id="bd-toc-nav" class="page-toc">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#intuition">
   Intuition
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#cross-entropy-as-a-loss-function">
   Cross-Entropy as a Loss Function
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cross-entropy-setup">
     Cross-Entropy Setup
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#z-logits">
     Z Logits
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#softmax">
     Softmax
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#categorical-cross-entropy-loss">
     Categorical Cross Entropy Loss
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#using-dot-product-to-calculate">
       Using Dot Product to Calculate
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#stackoverflow-answer">
   Stackoverflow answer
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#machinelearningmastery-example">
   MachineLearningMastery example
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#further-readings">
   Further Readings
  </a>
 </li>
</ul>

</nav>
</div>

            </div>
          
          
        </div>
        <footer class="bd-footer-content">
          <div class="bd-footer-content__inner">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Gao Hongnan
</p>

  </div>
  
  <div class="footer-item">
    
<p class="copyright">

    &copy; Copyright 2023.<br>

</p>

  </div>
  
  <div class="footer-item">
    <p class="last-updated">
Last updated on None.<br>
</p>
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </div>
        </footer>
        

      </main>
    </div>
  </div>

  
    
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/bootstrap.js?digest=796348d33e8b1d947c94"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=796348d33e8b1d947c94"></script>

  </body>
</html>