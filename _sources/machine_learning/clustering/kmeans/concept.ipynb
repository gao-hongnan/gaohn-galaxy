{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f2126c3",
   "metadata": {
    "tags": [
     "remove-input",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "parent_dir = str(Path().resolve().parents[3])\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from src.utils.plot import use_svg_display\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "use_svg_display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4426cf",
   "metadata": {},
   "source": [
    "# Concept\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "```{prf:remark} Remark\n",
    ":label: remark-kmeans-problem-statement\n",
    "\n",
    "Although K-Means does not explicitly model the underlying distribution $\\mathcal{D}$,\n",
    "we can still apply the learning theory framework to K-Means.\n",
    "```\n",
    "\n",
    "**Given** a set $\\mathcal{S}$ containing $N$ data points:\n",
    "\n",
    "$$\n",
    "\\mathcal{S} = \\left\\{\\mathbf{x}^{(1)}, \\mathbf{x}^{(2)}, \\dots, \\mathbf{x}^{(N)}\\right\\} \\subset \\mathbb{R}^{D}\n",
    "$$\n",
    "\n",
    "where the vector $\\mathbf{x}^{(n)}$ is the $n$-th sample with $D$ number of features, given by:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}^{(n)} \\in \\mathbb{R}^{D} = \\begin{bmatrix} x_1^{(n)} & x_2^{(n)} & \\cdots & x_D^{(n)} \\end{bmatrix}^{\\mathrm{T}} \\quad \\text{where } n = 0, 1, \\ldots, N.\n",
    "$$\n",
    "\n",
    "We can further write $\\mathcal{S}$ as a disjoint union [^disjoint_union] of $K$ sets, as follows:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{S} &:= \\left\\{\\mathbf{x}^{(1)}, \\mathbf{x}^{(2)}, \\dots, \\mathbf{x}^{(N)}\\right\\} \\subset \\mathbb{R}^{D} = C_1 \\sqcup C_2 \\sqcup \\cdots \\sqcup C_K \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $C_k$ is the set of data points that belong to cluster $k$:\n",
    "\n",
    "$$\n",
    "C_k = \\left\\{\\mathbf{x}^{(n)} \\in \\mathbb{R}^{D} \\middle\\vert y^{(n)} = k\\right\\} .\n",
    "$$ (eq:cluster-def)\n",
    "\n",
    "The notation $y^{(n)} \\in \\{1, 2, \\dots, K\\}$ may seem strange at first glance, since we are not given the labels $y^{(n)}$\n",
    "in an unsupervised problem. Indeed, this $y^{(n)}$ (**latent**) is generally not known to us,\n",
    "but we can have a mental model that\n",
    "for each data point, there is an underlying cluster label $y^{(n)}$ that it should belong to.\n",
    "\n",
    "More concretely, we say that $y^{(n)} \\in \\{1, 2, \\dots, K\\}$ in equation {eq}`eq:cluster-def` refers to the cluster (ground truth) label of data point $\\mathbf{x}^{(n)}$.\n",
    "\n",
    "We further define $\\mathcal{C}$ as the collection of these clusters[^collection_of_clusters],\n",
    "\n",
    "$$\n",
    "\\mathcal{C} = \\left\\{C_1, C_2, \\dots, C_K\\right\\}.\n",
    "$$\n",
    "\n",
    "To this end, we have decomposed the $N$ data points into $K$ clusters, where $K$\n",
    "is a [*priori*](https://en.wikipedia.org/wiki/A_priori_and_a_posteriori), a pre-defined number.\n",
    "\n",
    "---\n",
    "\n",
    "The **K-Means** algorithm aims to group the data points into a set $\\hat{\\mathcal{C}}$ containing $K$ clusters:\n",
    "\n",
    "$$\n",
    "\\hat{\\mathcal{C}} = \\left\\{ \\hat{C}_1, \\hat{C}_2 \\dots, \\hat{C}_K \\right\\}\n",
    "$$\n",
    "\n",
    "where $ \\hat{C}_k $ is the set of data points $ \\mathbf{x}^{(n)} \\in \\mathcal{S}$ assigned by $ \\mathcal{A}(\\cdot) $ (explained shortly) to the $ k $-th cluster:\n",
    "\n",
    "$$\n",
    "\\hat{C}_k = \\left\\{\\mathbf{x}^{(n)} \\in \\mathbb{R}^{D} \\middle\\vert \\mathcal{A}(n):= \\hat{y}^{(n)} = k\\right\\}.\n",
    "$$\n",
    "\n",
    "Note that $\\mathcal{A}(\\cdot)$ is the assignment map that \"predicts\" and \"classifies\" each data point into their respective clusters.\n",
    "\n",
    "\n",
    "To this end, the **goal** of such an **unsupervised problem** is to find the ***clusters***\n",
    "$\\hat{\\mathcal{C}}$, the predicted clusters learnt by K-Means that best approximate the ground truth clusters $\\mathcal{C}$.\n",
    "\n",
    "In other words, we want to find the clusters $\\hat{\\mathcal{C}}$ that are the closest to the ground truth clusters $\\mathcal{C}$, we will make precise the notion of *close* later.\n",
    "\n",
    "It is also customary to denote $\\hat{C}_k$ to be the set that contains the indices of the data points that belong to cluster $k$:\n",
    "\n",
    "$$\n",
    "\\hat{C}_k = \\left\\{n \\in \\{1, 2, \\dots, N\\} \\middle\\vert \\mathcal{A}(n):= \\hat{y}^{(n)} = k\\right\\}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "[K-Means clustering](https://en.wikipedia.org/wiki/K-means_clustering)'s goal is to find the clusters $\\hat{\\mathcal{C}}$ that are the closest to the ground truth clusters $\\mathcal{C}$ (**hard clustering**).\n",
    "In other words, we aim to partition $N$ data points $\\mathcal{S}$ into $K$ clusters $\\hat{\\mathcal{C}}$.\n",
    "The problem in itself seems manageable, since we can simply partition the data points into $K$ clusters\n",
    "and minimize the intra-cluster distance (variances). However, it is computationally challenging to solve the problem ([NP-hard](https://en.wikipedia.org/wiki/NP-hardness)).\n",
    "\n",
    "Consequently, there are many heuristics that are used to solve the problem. We will talk about one of the most popular heuristics,\n",
    "the [Lloyd's algorithm](https://en.wikipedia.org/wiki/Lloyd%27s_algorithm) in this section.\n",
    "\n",
    "In this algorithm, there exists $K$ centroids (centers)\n",
    "\n",
    "$$\n",
    "\\boldsymbol{v}_1, \\boldsymbol{v}_2, \\dots, \\boldsymbol{v}_K \\in \\mathbb{R}^{D}\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{v}_k$ is defined to be the centroid of cluster $C_k$. Each centroid $\\boldsymbol{v}_k$\n",
    "is a vector that has the same dimension as the data points $\\mathbf{x}^{(n)}$ and\n",
    "is the **representative vector** of the cluster $C_k$.\n",
    "\n",
    "By representative vector, we mean that\n",
    "$\\boldsymbol{v}_k$ is a vector that can \"describe\" the cluster $C_k$.\n",
    "By construction, the centroids can be defined as any vector $\\boldsymbol{v}_k$ that\n",
    "has the same dimension as the data points $\\mathbf{x}^{(n)}$. However, an\n",
    "intuitive choice is to use the **mean** of the data points $\\boldsymbol{\\mu}_k$ in the cluster $\\hat{C}_k$\n",
    "as the representative vector $\\boldsymbol{v}_k$.\n",
    "\n",
    "Next, the formulation of the assignment rule $\\mathcal{A}(\\cdot)$ can be made clear by the intuition below:\n",
    "\n",
    "> Since $\\mathcal{A}(\\cdot)$ is an assignment rule, an intuitive way is to find a representative vector $\\boldsymbol{v}_k$ in each cluster $\\hat{C}_k$, and assign every data point $\\mathbf{x}^{(n)}$ that is closest to this representative. This is similar to the [nearest-neighbour search algorithm](https://en.wikipedia.org/wiki/Nearest_neighbor_search#:~:text=Nearest%20neighbor%20search%20(NNS)%2C,the%20larger%20the%20function%20values.).\n",
    "\n",
    "Consequently, given the representative vectors $\\left\\{\\boldsymbol{v}_k\\right\\}_{k=1}^K$,\n",
    "we need an assignment function $\\mathcal{A}(n) = \\hat{y}^{(n)}$ that assigns each data point $\\mathbf{x}^{(n)}$\n",
    "to the cluster $\\hat{C}_k$. An intuitive choice is to compare \"closeness\" of each $\\mathbf{x}^{(n)}$\n",
    "to the representative vectors $\\left\\{\\boldsymbol{v}_k\\right\\}_{k=1}^K$ and assign\n",
    "it to the cluster $\\hat{C}_k$ that is closest to the representative vector $\\boldsymbol{v}_k$.\n",
    "\n",
    "We will make these intuition more precise later by proving it.\n",
    "\n",
    "---\n",
    "\n",
    "To this end, we have tidied up the flow of the Lloyd's algorithm (more details in subsequent sections), we\n",
    "now finalize the problem statement by defining an appropriate [**loss**](https://en.wikipedia.org/wiki/Loss_function) and [**objective**](https://en.wikipedia.org/wiki/Mathematical_optimization) function. More formally, we want to\n",
    "find the assignment $\\mathcal{A}(\\cdot)$ and the cluster center $\\boldsymbol{v}_k$ such that the [**sum of squared distances**](https://en.wikipedia.org/wiki/Residual_sum_of_squares#:~:text=In%20statistics%2C%20the%20residual%20sum,such%20as%20a%20linear%20regression.) between each data point and its cluster center is minimized. This means partitioning the data points according to the [**Voronoi Diagram**](https://en.wikipedia.org/wiki/Voronoi_diagram).\n",
    "\n",
    "To this end, we can define an *empirical* cost function that measures the quality of the\n",
    "requirements listed earlier.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\widehat{\\mathcal{J}}\\left(\\left\\{\\hat{y}^{(n)}\\right\\}_{n=1}^N,\\left\\{\\boldsymbol{v}_{k}\\right\\}_{k=1}^k \\middle \\vert \\mathcal{S}\\right) &= \\sum_{n=1}^{N} \\left\\|\\mathbf{x}^{(n)} - \\boldsymbol{v}_{\\hat{y}^{(n)}} \\right\\|^2 \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Note that the clustering error $\\widehat{\\mathcal{J}}$ depends on **both** the **cluster assignments** $\\hat{y}^{(n)}$, which\n",
    "define the clusters $\\hat{C}_k$, and the **cluster representatives** $\\boldsymbol{v}_k$, for $k=1, \\ldots, K$.\n",
    "As mentioned earlier, finding the optimal cluster means $\\left\\{\\boldsymbol{v}_k\\right\\}_{k=1}^K$\n",
    "and cluster assignments $\\left\\{\\hat{y}^{(n)}\\right\\}_{n=1}^N$ that minimize the clustering error $\\widehat{\\mathcal{J}}$\n",
    "is a [NP-hard problem](https://cseweb.ucsd.edu/~avattani/papers/kmeans_hardness.pdf).\n",
    "The difficulty stems from the fact that the clustering error $\\mathcal{J}$ is a [non-convex](https://en.wikipedia.org/wiki/Convex_optimization)\n",
    "function of the cluster means and assignments. In other words, there are many local minima of the clustering error $\\mathcal{J}$, and finding the global minimum is hard.\n",
    "\n",
    "While jointly optimizing the cluster means and assignments is hard[^jointly_optimizing],\n",
    "separately optimizing either the cluster means for given assignments or vice-versa is easy.\n",
    "In what follows, we present simple closed-form solutions for these sub-problems.\n",
    "The $k$-means method simply combines these solutions in an alternating fashion {cite}`jung_2023`.\n",
    "\n",
    "More concretely, we want to show:\n",
    "\n",
    "- For fixed cluster assignments $\\mathcal{A}(n) = \\hat{y}^{(n)}$,\n",
    "the clustering error $\\widehat{\\mathcal{J}}$ is minimized by setting the cluster representatives $\\boldsymbol{v}_k$\n",
    "equal to the cluster means, this means\n",
    "the mean vector is the optimal\n",
    "choice for the cluster center.\n",
    "\n",
    "    $$\n",
    "    \\boldsymbol{\\mu}_1, \\boldsymbol{\\mu}_2, \\dots, \\boldsymbol{\\mu}_K \\in \\mathbb{R}^{D}\n",
    "    $$\n",
    "\n",
    "    where each $\\boldsymbol{\\mu}_k$ is the mean vector of the data points in cluster $C_k$.\n",
    "\n",
    "- Furthermore, now when we obtain the cluster means $\\boldsymbol{\\mu}_k$ (now we fix $\\boldsymbol{\\mu}_k$,\n",
    "  we can assign data points\n",
    "$\\mathbf{x}^{(n)}$ to the cluster $\\hat{C}_k$ that is closest to the cluster mean $\\boldsymbol{\\mu}_k$.\n",
    "This assignment action is called the **assignment function** $\\mathcal{A}$, a function that\n",
    "does the assignment of data points to clusters. We will show later that the clustering error $\\widehat{\\mathcal{J}}$ is minimized\n",
    "when the assignment function is the **nearest neighbor assignment** function $\\mathcal{A}^{*}(\\cdot)$,\n",
    "\n",
    "\n",
    "    $$\n",
    "    \\mathcal{A}^{*}(n) = \\underset{k}{\\operatorname{argmin}} \\left\\|\\mathbf{x}^{(n)} - \\boldsymbol{\\mu}_k \\right\\|^2\n",
    "    $$\n",
    "\n",
    "    where it assigns data points $\\mathbf{x}^{(n)}$ to the cluster $k$ whose center $\\boldsymbol{\\mu}_k$ is closest.\n",
    "\n",
    "We see that instead of jointly optimizing the cluster means and assignments in one step, we alternate between\n",
    "the two steps. We first fix the cluster assignments and optimize the cluster means, and then we fix the cluster means\n",
    "and optimize the cluster assignments. Readings who are familiar with data structures and algorithms will notice\n",
    "this looks like a [greedy algorithm](https://en.wikipedia.org/wiki/Greedy_algorithm), and those who have learnt the [expectation maximization](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm) algorithm will notice\n",
    "that this is a special case of the expectation maximization algorithm.\n",
    "\n",
    "In the following sections, we will phrase K-Means (Lloyd's algorithm) as an optimization problem, in which the goal is to find the optimal\n",
    "cluster centers and cluster assignments that minimize the clustering error. We will also prove why this is the case.\n",
    "\n",
    "## Intuition\n",
    "\n",
    "Some intuition on choosing the cost function $\\widehat{\\mathcal{J}}$.\n",
    "\n",
    "In supervised learning, we have our typical loss functions such as cross-entropy loss (classification),\n",
    "and in regression, we have mean squared error. We also have\n",
    "metrics like accuracy, precision, recall, etc to measure the performance of the model.\n",
    "\n",
    "This means, given a hypothesis $\\hat{y}:=h(\\mathbf{x})$, how close is it to the true label $y$?\n",
    "In unsupervised, we do not have such ground truth label $y$ to compare with, but the notion of\n",
    "closeness is still there.\n",
    "\n",
    "### Example\n",
    "\n",
    "Let's first look at an example by randomly generating data points[^y] that can be\n",
    "partitioned into 3 distinct clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72a8dacf",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"451.442625pt\" height=\"335.86825pt\" viewBox=\"0 0 451.442625 335.86825\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2023-03-08T09:31:46.872264</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.6.0, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 335.86825 \n",
       "L 451.442625 335.86825 \n",
       "L 451.442625 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 34.240625 298.312 \n",
       "L 444.242625 298.312 \n",
       "L 444.242625 7.2 \n",
       "L 34.240625 7.2 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"PathCollection_1\">\n",
       "    <defs>\n",
       "     <path id=\"m8304ef0839\" d=\"M 0 3.535534 \n",
       "C 0.937635 3.535534 1.836992 3.163008 2.5 2.5 \n",
       "C 3.163008 1.836992 3.535534 0.937635 3.535534 0 \n",
       "C 3.535534 -0.937635 3.163008 -1.836992 2.5 -2.5 \n",
       "C 1.836992 -3.163008 0.937635 -3.535534 0 -3.535534 \n",
       "C -0.937635 -3.535534 -1.836992 -3.163008 -2.5 -2.5 \n",
       "C -3.163008 -1.836992 -3.535534 -0.937635 -3.535534 0 \n",
       "C -3.535534 0.937635 -3.163008 1.836992 -2.5 2.5 \n",
       "C -1.836992 3.163008 -0.937635 3.535534 0 3.535534 \n",
       "z\n",
       "\" style=\"stroke: #000000\"/>\n",
       "    </defs>\n",
       "    <g clip-path=\"url(#p031beb9923)\">\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"384.287709\" y=\"215.736672\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"250.836062\" y=\"115.138302\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"268.215886\" y=\"63.657528\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"250.588798\" y=\"58.103823\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"385.156142\" y=\"257.535041\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"319.022014\" y=\"38.460141\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"328.759844\" y=\"32.029001\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"369.4944\" y=\"270.471194\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"79.977449\" y=\"146.401169\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"327.326413\" y=\"21.754494\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"418.526779\" y=\"199.325992\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"192.41827\" y=\"159.726953\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"97.729051\" y=\"143.762398\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"263.009087\" y=\"77.976333\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"226.936023\" y=\"56.275988\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"87.845689\" y=\"144.070199\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"121.395676\" y=\"120.150169\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"337.002186\" y=\"237.287339\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"113.971931\" y=\"141.366045\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"347.622426\" y=\"259.658737\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"257.767899\" y=\"76.084871\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"373.73282\" y=\"211.684001\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"289.49441\" y=\"87.997694\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"284.053118\" y=\"32.392383\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"135.024001\" y=\"148.062597\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"365.732562\" y=\"235.653343\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"368.655719\" y=\"209.306399\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"294.4068\" y=\"104.429662\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"52.87708\" y=\"84.462011\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"311.94727\" y=\"242.067495\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"113.873129\" y=\"106.806603\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"99.020641\" y=\"139.923883\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"143.563455\" y=\"145.428057\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"103.325411\" y=\"122.594489\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"239.085528\" y=\"48.695121\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"325.07873\" y=\"194.352024\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"409.008229\" y=\"209.695718\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"328.417626\" y=\"215.226727\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"258.995664\" y=\"76.161666\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"290.888299\" y=\"50.403414\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"131.095932\" y=\"200.980479\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"108.301808\" y=\"174.565221\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"238.516057\" y=\"44.072371\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"337.491122\" y=\"264.641646\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"354.252653\" y=\"216.224039\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"376.470463\" y=\"247.203312\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"140.511963\" y=\"117.457176\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"352.484847\" y=\"102.457338\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"139.702142\" y=\"145.290769\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"310.001632\" y=\"71.069729\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"334.878399\" y=\"186.974921\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"244.94592\" y=\"42.023518\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"286.120881\" y=\"43.897914\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"335.571648\" y=\"249.526154\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"370.529812\" y=\"221.705865\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"123.287995\" y=\"128.59978\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"262.97197\" y=\"95.86349\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"410.791142\" y=\"195.857555\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"140.456574\" y=\"102.833077\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"302.887182\" y=\"64.316737\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"67.291098\" y=\"119.489523\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"181.215176\" y=\"124.795899\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"108.241505\" y=\"131.846458\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"117.701747\" y=\"125.059466\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"285.127783\" y=\"77.102078\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"190.98224\" y=\"135.210603\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"283.501615\" y=\"57.740402\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"321.162117\" y=\"185.510307\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"197.229156\" y=\"51.679014\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"224.487088\" y=\"20.432364\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"316.096047\" y=\"97.168937\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"309.43733\" y=\"211.181542\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"379.381343\" y=\"235.255008\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"241.650359\" y=\"20.604698\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"315.59228\" y=\"215.102401\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"269.386389\" y=\"66.070808\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"317.363748\" y=\"71.757882\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"92.065532\" y=\"169.516752\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"127.467764\" y=\"119.526512\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"278.508758\" y=\"57.105849\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"327.178651\" y=\"230.754285\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"323.023353\" y=\"248.131304\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"245.66058\" y=\"101.62995\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"293.705031\" y=\"59.386014\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"358.488035\" y=\"199.5703\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"425.60617\" y=\"208.769878\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"378.531684\" y=\"223.843344\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"100.773664\" y=\"124.170639\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"84.600466\" y=\"105.017932\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"369.014731\" y=\"270.112313\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"361.005073\" y=\"258.004028\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"280.889087\" y=\"71.932741\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"300.879881\" y=\"220.93672\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"266.43678\" y=\"37.976799\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"326.527231\" y=\"235.125519\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"340.039141\" y=\"99.889337\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"87.475516\" y=\"142.526894\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"81.029394\" y=\"115.374745\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"306.524823\" y=\"242.63995\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"346.690173\" y=\"190.249064\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"324.101592\" y=\"218.531296\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"379.451163\" y=\"229.412053\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"76.189447\" y=\"94.250694\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"345.717611\" y=\"247.503279\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"313.334289\" y=\"230.264455\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"284.40414\" y=\"58.314357\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"105.390454\" y=\"125.278572\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"327.513994\" y=\"72.365719\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"276.093149\" y=\"57.53302\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"241.907048\" y=\"45.72413\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"131.971964\" y=\"117.886943\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"266.947033\" y=\"74.705791\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"334.222352\" y=\"231.099989\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"135.114982\" y=\"132.062727\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"279.754032\" y=\"24.404687\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"55.916192\" y=\"129.645283\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"281.55736\" y=\"60.137635\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"307.244282\" y=\"85.301443\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"134.144405\" y=\"136.989466\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"99.052898\" y=\"135.45896\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"251.382966\" y=\"81.36741\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"387.132557\" y=\"223.886343\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"250.207904\" y=\"66.17246\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"227.460775\" y=\"72.548359\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"332.854177\" y=\"185.041372\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"340.259363\" y=\"212.191011\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"117.466559\" y=\"151.970797\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"324.911593\" y=\"251.428759\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"68.64773\" y=\"132.668798\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"122.819487\" y=\"141.884853\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"127.261118\" y=\"136.556991\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"147.550514\" y=\"126.551356\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"307.266874\" y=\"225.088164\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"114.445692\" y=\"150.577208\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"114.691107\" y=\"109.618188\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"166.360711\" y=\"184.055665\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"253.238885\" y=\"109.007427\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"97.247081\" y=\"143.610196\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"319.5011\" y=\"204.613714\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"98.030033\" y=\"145.779943\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"303.915638\" y=\"64.492773\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"292.368352\" y=\"83.919845\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"328.167416\" y=\"285.079636\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"367.448873\" y=\"236.538656\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"155.345987\" y=\"108.186222\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"72.405744\" y=\"146.076059\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"95.970933\" y=\"97.183565\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"94.890958\" y=\"167.515946\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"361.635438\" y=\"257.851324\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m8304ef0839\" x=\"316.687707\" y=\"268.732362\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"macf26bd25f\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#macf26bd25f\" x=\"87.78949\" y=\"298.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- −2 -->\n",
       "      <g transform=\"translate(80.418396 312.910437) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \n",
       "L 4684 2272 \n",
       "L 4684 1741 \n",
       "L 678 1741 \n",
       "L 678 2272 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"83.789062\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#macf26bd25f\" x=\"152.174279\" y=\"298.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- −1 -->\n",
       "      <g transform=\"translate(144.803185 312.910437) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"83.789062\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#macf26bd25f\" x=\"216.559068\" y=\"298.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(213.377818 312.910437) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#macf26bd25f\" x=\"280.943857\" y=\"298.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 1 -->\n",
       "      <g transform=\"translate(277.762607 312.910437) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#macf26bd25f\" x=\"345.328646\" y=\"298.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(342.147396 312.910437) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#macf26bd25f\" x=\"409.713435\" y=\"298.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 3 -->\n",
       "      <g transform=\"translate(406.532185 312.910437) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \n",
       "Q 3050 2419 3304 2112 \n",
       "Q 3559 1806 3559 1356 \n",
       "Q 3559 666 3084 287 \n",
       "Q 2609 -91 1734 -91 \n",
       "Q 1441 -91 1130 -33 \n",
       "Q 819 25 488 141 \n",
       "L 488 750 \n",
       "Q 750 597 1062 519 \n",
       "Q 1375 441 1716 441 \n",
       "Q 2309 441 2620 675 \n",
       "Q 2931 909 2931 1356 \n",
       "Q 2931 1769 2642 2001 \n",
       "Q 2353 2234 1838 2234 \n",
       "L 1294 2234 \n",
       "L 1294 2753 \n",
       "L 1863 2753 \n",
       "Q 2328 2753 2575 2939 \n",
       "Q 2822 3125 2822 3475 \n",
       "Q 2822 3834 2567 4026 \n",
       "Q 2313 4219 1838 4219 \n",
       "Q 1578 4219 1281 4162 \n",
       "Q 984 4106 628 3988 \n",
       "L 628 4550 \n",
       "Q 988 4650 1302 4700 \n",
       "Q 1616 4750 1894 4750 \n",
       "Q 2613 4750 3031 4423 \n",
       "Q 3450 4097 3450 3541 \n",
       "Q 3450 3153 3228 2886 \n",
       "Q 3006 2619 2597 2516 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_7\">\n",
       "     <!-- $\\mathbf{x}_1$: Feature 1 -->\n",
       "     <g transform=\"translate(206.241625 326.588562) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-Bold-78\" d=\"M 1422 1791 \n",
       "L 159 3500 \n",
       "L 1344 3500 \n",
       "L 2059 2463 \n",
       "L 2784 3500 \n",
       "L 3969 3500 \n",
       "L 2706 1797 \n",
       "L 4031 0 \n",
       "L 2847 0 \n",
       "L 2059 1106 \n",
       "L 1281 0 \n",
       "L 97 0 \n",
       "L 1422 1791 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-3a\" d=\"M 750 794 \n",
       "L 1409 794 \n",
       "L 1409 0 \n",
       "L 750 0 \n",
       "L 750 794 \n",
       "z\n",
       "M 750 3309 \n",
       "L 1409 3309 \n",
       "L 1409 2516 \n",
       "L 750 2516 \n",
       "L 750 3309 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-46\" d=\"M 628 4666 \n",
       "L 3309 4666 \n",
       "L 3309 4134 \n",
       "L 1259 4134 \n",
       "L 1259 2759 \n",
       "L 3109 2759 \n",
       "L 3109 2228 \n",
       "L 1259 2228 \n",
       "L 1259 0 \n",
       "L 628 0 \n",
       "L 628 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \n",
       "Q 1497 1759 1228 1600 \n",
       "Q 959 1441 959 1056 \n",
       "Q 959 750 1161 570 \n",
       "Q 1363 391 1709 391 \n",
       "Q 2188 391 2477 730 \n",
       "Q 2766 1069 2766 1631 \n",
       "L 2766 1759 \n",
       "L 2194 1759 \n",
       "z\n",
       "M 3341 1997 \n",
       "L 3341 0 \n",
       "L 2766 0 \n",
       "L 2766 531 \n",
       "Q 2569 213 2275 61 \n",
       "Q 1981 -91 1556 -91 \n",
       "Q 1019 -91 701 211 \n",
       "Q 384 513 384 1019 \n",
       "Q 384 1609 779 1909 \n",
       "Q 1175 2209 1959 2209 \n",
       "L 2766 2209 \n",
       "L 2766 2266 \n",
       "Q 2766 2663 2505 2880 \n",
       "Q 2244 3097 1772 3097 \n",
       "Q 1472 3097 1187 3025 \n",
       "Q 903 2953 641 2809 \n",
       "L 641 3341 \n",
       "Q 956 3463 1253 3523 \n",
       "Q 1550 3584 1831 3584 \n",
       "Q 2591 3584 2966 3190 \n",
       "Q 3341 2797 3341 1997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-75\" d=\"M 544 1381 \n",
       "L 544 3500 \n",
       "L 1119 3500 \n",
       "L 1119 1403 \n",
       "Q 1119 906 1312 657 \n",
       "Q 1506 409 1894 409 \n",
       "Q 2359 409 2629 706 \n",
       "Q 2900 1003 2900 1516 \n",
       "L 2900 3500 \n",
       "L 3475 3500 \n",
       "L 3475 0 \n",
       "L 2900 0 \n",
       "L 2900 538 \n",
       "Q 2691 219 2414 64 \n",
       "Q 2138 -91 1772 -91 \n",
       "Q 1169 -91 856 284 \n",
       "Q 544 659 544 1381 \n",
       "z\n",
       "M 1991 3584 \n",
       "L 1991 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
       "Q 2534 3019 2420 3045 \n",
       "Q 2306 3072 2169 3072 \n",
       "Q 1681 3072 1420 2755 \n",
       "Q 1159 2438 1159 1844 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1341 3275 1631 3429 \n",
       "Q 1922 3584 2338 3584 \n",
       "Q 2397 3584 2469 3576 \n",
       "Q 2541 3569 2628 3553 \n",
       "L 2631 2963 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-Bold-78\" transform=\"translate(0 0.09375)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-31\" transform=\"translate(65.458984 -16.3125) scale(0.7)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-3a\" transform=\"translate(112.729492 0.09375)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" transform=\"translate(146.420898 0.09375)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-46\" transform=\"translate(178.208008 0.09375)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" transform=\"translate(235.727539 0.09375)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" transform=\"translate(297.250977 0.09375)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" transform=\"translate(358.530273 0.09375)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-75\" transform=\"translate(397.739258 0.09375)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" transform=\"translate(461.118164 0.09375)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" transform=\"translate(502.231445 0.09375)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" transform=\"translate(563.754883 0.09375)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-31\" transform=\"translate(595.541992 0.09375)\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <defs>\n",
       "       <path id=\"m2c1b7d9251\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m2c1b7d9251\" x=\"34.240625\" y=\"274.767902\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(20.878125 278.567121) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m2c1b7d9251\" x=\"34.240625\" y=\"226.590765\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 1 -->\n",
       "      <g transform=\"translate(20.878125 230.389984) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m2c1b7d9251\" x=\"34.240625\" y=\"178.413628\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(20.878125 182.212847) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m2c1b7d9251\" x=\"34.240625\" y=\"130.236491\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 3 -->\n",
       "      <g transform=\"translate(20.878125 134.03571) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m2c1b7d9251\" x=\"34.240625\" y=\"82.059354\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 4 -->\n",
       "      <g transform=\"translate(20.878125 85.858573) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_12\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m2c1b7d9251\" x=\"34.240625\" y=\"33.882217\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 5 -->\n",
       "      <g transform=\"translate(20.878125 37.681436) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_14\">\n",
       "     <!-- $\\mathbf{x}_2$: Feature 2 -->\n",
       "     <g transform=\"translate(14.798438 185.756) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-Bold-78\" transform=\"translate(0 0.78125)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-32\" transform=\"translate(65.458984 -15.625) scale(0.7)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-3a\" transform=\"translate(112.729492 0.78125)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" transform=\"translate(146.420898 0.78125)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-46\" transform=\"translate(178.208008 0.78125)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" transform=\"translate(235.727539 0.78125)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" transform=\"translate(297.250977 0.78125)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" transform=\"translate(358.530273 0.78125)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-75\" transform=\"translate(397.739258 0.78125)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" transform=\"translate(461.118164 0.78125)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" transform=\"translate(502.231445 0.78125)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" transform=\"translate(563.754883 0.78125)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-32\" transform=\"translate(595.541992 0.78125)\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 34.240625 298.312 \n",
       "L 34.240625 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 444.242625 298.312 \n",
       "L 444.242625 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 34.240625 298.312 \n",
       "L 444.242625 298.312 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 34.240625 7.2 \n",
       "L 444.242625 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p031beb9923\">\n",
       "   <rect x=\"34.240625\" y=\"7.2\" width=\"410.002\" height=\"291.112\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(\n",
    "    n_samples=150,\n",
    "    n_features=2,\n",
    "    centers=3,\n",
    "    cluster_std=0.5,\n",
    "    shuffle=True,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=\"white\", marker=\"o\", edgecolor=\"black\", s=50)\n",
    "plt.xlabel(\"$\\mathbf{x}_1$: Feature 1\")\n",
    "plt.ylabel(\"$\\mathbf{x}_2$: Feature 2\")\n",
    "\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0924dcdb",
   "metadata": {},
   "source": [
    "Visually, we can literally just circle out the 3 clusters. The luxury of such simplicity\n",
    "is because we are working with 2 features, i.e. $\\mathbf{x} \\in \\mathbb{R}^{2}$.\n",
    "In real world, we are working with\n",
    "$D$ features in $\\mathbb{R}^{D}$, where $D$ can be very large. Furthermore, even with\n",
    "such a simple dataset, how do we tell the machine to find the 3 clusters?\n",
    "\n",
    "### The Notion of Similarity and Closeness\n",
    "\n",
    "To define such a metric for unsupervised learning, we can fall back on our intuition.\n",
    "The purpose of clustering is to group similar data points together. So we seek to find\n",
    "a metric that measures the similarity between data points in a dataset.\n",
    "\n",
    "A very simple idea is to use\n",
    "[**intra-cluster variance**](https://stats.stackexchange.com/questions/120509/inter-cluster-variance). For example, within a cluster, the data points are close to each other if\n",
    "the variance is small.\n",
    "\n",
    "Consequently, to make our intuition precise, we need to define a metric rule and an assignment $\\mathcal{A}(\\cdot)$ to assign data points to clusters. We also\n",
    "need to define the notion of closeness and similarity between data points.\n",
    "\n",
    "Lastly, such algorithms require an initial guess of the cluster centers, so that\n",
    "eventually the algorithm can converge to the optimal cluster centers, since we have no way of knowing\n",
    "the optimal cluster centers beforehand, especially in high dimensional space.\n",
    "\n",
    "More formally, the optimization problem requires us to minimize the sum of squared distances between each data point and its cluster center.\n",
    "This is equivalent to minimizing the variance within each cluster.\n",
    "\n",
    "Let's look at some definitions first that will gradually lead us to the formulation of the objective function.\n",
    "\n",
    "## Partition and Voronoi Regions\n",
    "\n",
    "K-Means can be formulated via the lens of [**Voronoi regions**](https://en.wikipedia.org/wiki/Voronoi_diagram)\n",
    "where we define $C_k \\in \\mathcal{C}$ as the **partition** of the data set $\\mathcal{S}$, where each subset is a cluster.\n",
    "We say that $C_k$ is a representative of the cluster $k$ and induces a **Voronoi partition** of $\\mathbb{R}^D$.\n",
    "More formally, we define the Voronoi partition as follows:\n",
    "\n",
    "```{prf:definition} K-Means Voronoi Partition\n",
    ":label: def:kmeans-voronoi-partition\n",
    "\n",
    "Let $\\mathcal{C} = \\{C_1, C_2, \\ldots, C_K\\}$ be a partition of $\\mathcal{S}$, where $C_k \\in C$ is a subset of $\\mathcal{S}$.\n",
    "Then $\\mathcal{C}$ induces a **Voronoi partition** ({prf:ref}`def-voronoi-region`) of $\\mathbb{R}^D$, which decomposes $\\mathbb{R}^D$ into $K$ convex cells,\n",
    "each corresponding to some $C_k \\in \\mathcal{C}$ and containing the region of space whose nearest representative is $C_k$.\n",
    "\n",
    "More concretely, the Voronoi region $C_k$, contains all points $\\mathbf{x} \\in \\mathbb{R}^D$ such that\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\left\\|\\mathbf{x} - \\boldsymbol{v}_k \\right\\|^2 \\leq \\left\\|\\mathbf{x} - \\boldsymbol{v}_j \\right\\|^2 \\text{ for all } j \\neq k\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "which means that the distance between $\\mathbf{x}$ and $\\boldsymbol{v}_k$ is less than or equal to the distance between $\\mathbf{x}$ and any other cluster center $\\boldsymbol{v}_j$.\n",
    "\n",
    "Also,\n",
    "\n",
    "$$\n",
    "\\mathcal{S} = \\bigsqcup_{k=1}^K C_k\n",
    "$$\n",
    "```\n",
    "\n",
    "For a visual representation, see {cite:ps}`pml1Book`'s [figure](https://github.com/probml/pyprobml/blob/master/notebooks/book1/21/kmeans_voronoi.ipynb).\n",
    "\n",
    "## Assignment\n",
    "\n",
    "```{prf:definition} Assignment\n",
    ":label: def:assignment\n",
    "\n",
    "An assignment $\\mathcal{A}(\\cdot)$ is a surjective map,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{A} : \\mathbb{Z}^{+} &\\to \\mathbb{Z}^{+} \\\\\n",
    "\\{1, 2, \\dots, N\\} &\\to \\{1, 2, \\dots, K\\} .\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In this case, $\\mathcal{A}(n) = k$ means that data point $\\mathbf{x}^{(n)}$ is assigned to cluster $k$.\n",
    "\n",
    "One should see that the assignment function $\\mathcal{A}(\\cdot)$ gives rise to the prediction $\\hat{y}^{(n)}$\n",
    "for each data point $\\mathbf{x}^{(n)}$.\n",
    "\n",
    "$$\n",
    "\\hat{y}^{(n)} = \\mathcal{A}(n) \\quad \\text{for } n = 1, 2, \\dots, N.\n",
    "$$ (eq:assignment-prediction)\n",
    "```\n",
    "\n",
    "```{prf:example} Assignment\n",
    ":label: example:assignment\n",
    "\n",
    "For example, if we have 4 data points $\\mathbf{x}^{(1)}$, $\\mathbf{x}^{(2)}$, $\\mathbf{x}^{(3)}$, and $\\mathbf{x}^{(4)}$,\n",
    "and we want to partition them into 3 clusters $\\hat{C}_1$, $\\hat{C}_2$, and $\\hat{C}_3$, we can define an assignment as follows:\n",
    "\n",
    "- Assign $\\mathbf{x}^{(1)}$ to $\\hat{C}_1$, $\\hat{C}_1 = \\left\\{\\mathbf{x}^{(1)}\\right\\}$.\n",
    "- Assign $\\mathbf{x}^{(3)}$ to $\\hat{C}_2$, $\\hat{C}_2 = \\left\\{\\mathbf{x}^{(3)}\\right\\}$.\n",
    "- Assign $\\mathbf{x}^{(2)}$ and $\\mathbf{x}^{(4)}$ to $\\hat{C}_3$, $\\hat{C}_3 = \\left\\{\\mathbf{x}^{(2)}, \\mathbf{x}^{(4)}\\right\\}$.\n",
    "\n",
    "We can make this more precise by defining an assignment function $\\mathcal{A}$ as follows:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{A} : \\mathbb{Z}^{+} &\\to \\mathbb{Z}^{+} \\\\\n",
    "\\{1, 2, 3, 4\\} &\\to \\{1, 2, 3\\}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "- $\\mathcal{A}(1) = 1$,\n",
    "- $\\mathcal{A}(2) = 3$,\n",
    "- $\\mathcal{A}(3) = 2$, and\n",
    "- $\\mathcal{A}(4) = 3$.\n",
    "```\n",
    "\n",
    "We have seen earlier that the assignment function of the K-Means algorithm follows the nearest-neighbour rule, but\n",
    "we did not explicitly define it here just yet. We will derive that the optimal assignment\n",
    "$\\mathcal{A}^{*}(\\cdot)$ is the one that minimizes the cost function:\n",
    "\n",
    "$$\n",
    "\\mathcal{A}^{*}(n) = \\underset{k}{\\operatorname{argmin}} \\left\\|\\mathbf{x}^{(n)} - \\boldsymbol{\\mu}_k \\right\\|^2\n",
    "$$ (eq:assignment-optimal-1)\n",
    "\n",
    "## Centroids (Representatives)\n",
    "\n",
    "```{prf:definition} Centroids\n",
    ":label: def:centroids\n",
    "\n",
    "The centroids $\\boldsymbol{v}_1, \\boldsymbol{v}_2, \\ldots, \\boldsymbol{v}_K$ of a partition $\\hat{\\mathcal{C}}$ are the representatives of each cluster $C_k \\in \\hat{\\mathcal{C}}$:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{v}_k \\text{ represents cluster } C_k \\text{ for } k = 1, 2, \\ldots, K.\n",
    "$$\n",
    "```\n",
    "\n",
    "## Cost Function\n",
    "\n",
    "We make precise the notion of closeness and similarity between data points by defining a cost function\n",
    "utilizing the [**euclidean distance**](https://en.wikipedia.org/wiki/Euclidean_distance). In practice, we can use other distance metrics such as [**manhattan distance**](https://simple.wikipedia.org/wiki/Manhattan_distance)\n",
    "that suits one's needs.\n",
    "\n",
    "```{prf:definition} K-Means Cost Function\n",
    ":label: def:kmeans-cost\n",
    "\n",
    "For any assignment $\\mathcal{A}(\\cdot)$ that maps the set $\\{1, 2, \\ldots, N\\}$ to $\\{1, 2, \\ldots, K\\}$ and\n",
    "any centroids $\\boldsymbol{v}_1, \\boldsymbol{v}_2, \\dots, \\boldsymbol{v}_K \\in \\mathbb{R}^{D}$,\n",
    "we construct the cost function as follows:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\widehat{\\mathcal{J}}_{\\mathcal{S}}(\\mathcal{A}, \\boldsymbol{v}_1, \\boldsymbol{v}_2, \\ldots, \\boldsymbol{v}_K) &:= \\widehat{\\mathcal{J}}\\left(\\left\\{\\hat{y}^{(n)}\\right\\}_{n=1}^N,\\left\\{\\boldsymbol{v}_{k}\\right\\}_{k=1}^k \\middle \\vert \\mathcal{S}\\right) \\\\\n",
    "&\\overset{\\text{(a)}}{=} \\sum_{n=1}^{N} \\left\\|\\mathbf{x}^{(n)} - \\boldsymbol{v}_{\\mathcal{A}(n)} \\right\\|^2 \\\\\n",
    "&\\overset{\\text{(b)}}{=} \\sum_{n=1}^{N} \\sum_{\\mathcal{A}(n) = k} \\left\\|\\mathbf{x}^{(n)} - \\boldsymbol{v}_k \\right\\|^2 \\\\\n",
    "&\\overset{\\text{(c)}}{=} \\sum_{n=1}^{N} \\sum_{k=1}^{K} r^{(n)}_k \\left\\|\\mathbf{x}^{(n)} - \\boldsymbol{v}_k \\right\\|^2 \\\\\n",
    "&\\overset{\\text{(d)}}{=} \\sum_{n=1}^{N} \\sum_{k=1}^{K} \\mathbb{I}\\left\\{\\mathcal{A}(n) = k\\right\\} \\left\\|\\mathbf{x}^{(n)} - \\boldsymbol{v}_k \\right\\|^2 \\\\\n",
    "&\\overset{\\text{(e)}}{=} \\sum_{k=1}^K \\sum_{\\mathbf{x}^{(n)} \\in \\hat{C}_k} \\left\\|\\mathbf{x}^{(n)} - \\boldsymbol{v}_k \\right\\|^2 \\\\\n",
    "\\end{aligned}\n",
    "$$ (eq:kmeans-cost-1)\n",
    "\n",
    "where\n",
    "\n",
    "- $\\mathcal{A}(n) = k$ means that data point $\\mathbf{x}^{(n)}$ is assigned to cluster $k$.\n",
    "- $r^{(n)}_k$ is an indicator function that is equal to 1 if $\\mathcal{A}(n) = k$ and 0 otherwise.\n",
    "\n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "    r^{(n)}_k &= \\begin{cases} 1 & \\text{if } \\mathcal{A}(n) = k \\\\ 0 & \\text{otherwise} \\end{cases}\n",
    "    \\end{aligned}\n",
    "    $$\n",
    "- $\\hat{C}_k$ is the set of data points that are assigned to cluster $k$.\n",
    "- $\\left\\|\\cdot\\right\\|$ is the euclidean norm.\n",
    "\n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "    \\left\\|\\mathbf{x} - \\boldsymbol{v}\\right\\|^2 &= \\left(\\mathbf{x} - \\boldsymbol{v}\\right)^{\\top} \\left(\\mathbf{x} - \\boldsymbol{v}\\right) \\\\\n",
    "    \\end{aligned}\n",
    "    $$\n",
    "- All 5 forms are equivalent[^equivalent-k-means-cost-function].\n",
    "\n",
    "It is worth a reminder that we have not formally defined what the assignment $\\mathcal{A}(\\cdot)$ is, as well\n",
    "as the representative vectors (centroids) $\\boldsymbol{v}_1, \\boldsymbol{v}_2, \\dots, \\boldsymbol{v}_K$. We will show later that\n",
    "$\\boldsymbol{v}_k$ is the mean of the data points in cluster $k$ and that $\\mathcal{A}(n)=\\underset{k}{\\operatorname{argmin}} \\left\\|\\mathbf{x}^{(n)} - \\boldsymbol{\\mu}_k \\right\\|^2$ is the assignment\n",
    "that minimizes the cost function $\\widehat{\\mathcal{J}}_{\\mathcal{S}}$.\n",
    "```\n",
    "\n",
    "```{prf:remark} Cost Function is a Function of Assignment and Centroids\n",
    ":label: remark:cost-function-is-a-function-of-assignment-and-centroids\n",
    "\n",
    "The cost function is a function **both** the assignment $\\mathcal{A}(\\cdot)$ and the cluster centers $\\boldsymbol{v}_1, \\boldsymbol{v}_2, \\dots, \\boldsymbol{v}_K$,\n",
    "which adds up the squared euclidean distance between each data point and its assigned cluster center. The\n",
    "total cost is what we are minimizing. Note that the problem is equivalent to minimizing each\n",
    "cluster's cost individually.\n",
    "\n",
    "We also call the loss sum of squared error (SSE) , which is just the intra-cluster variance, a measure of how spread out the data points are within a cluster.\n",
    "```\n",
    "\n",
    "## Objective Function\n",
    "\n",
    "Finally, we define the objective function as the cost function $\\widehat{\\mathcal{J}}_{\\mathcal{S}}$ that we are minimizing.\n",
    "\n",
    "```{prf:definition} K-Means Objective Function\n",
    ":label: def:kmeans-objective\n",
    "\n",
    "The **objective** function is to **minimize** the above expression in equation {eq}`eq:kmeans-cost-1`:\n",
    "\n",
    "$$\n",
    "\\begin{alignat}{3}\n",
    "\\underset{\\mathcal{A}, \\boldsymbol{v}_k}{\\operatorname{argmin}} &\\quad& \\widehat{\\mathcal{J}}_{\\mathcal{S}}(\\mathcal{A}, \\boldsymbol{v}_1, \\boldsymbol{v}_2, \\ldots, \\boldsymbol{v}_K) &= \\sum_{n=1}^{N} \\sum_{\\mathcal{A}(n) = k} \\left\\|\\mathbf{x}^{(n)} - \\boldsymbol{v}_k \\right\\|^2  \\\\\n",
    "\\text{subject to} &\\quad& \\hat{C}_1 \\sqcup \\hat{C}_2 \\sqcup \\cdots \\sqcup \\hat{C}_K &= \\mathcal{S} \\\\\n",
    "&\\quad& \\boldsymbol{v}_1, \\boldsymbol{v}_2, \\ldots, \\boldsymbol{v}_K &\\in \\mathbb{R}^D \\\\\n",
    "\\end{alignat}\n",
    "$$ (eq:k-means-objective-function-1)\n",
    "\n",
    "This just means, for all possible assignments $\\mathcal{A}(\\cdot)$ and cluster centers $\\boldsymbol{v}_1, \\boldsymbol{v}_2, \\ldots, \\boldsymbol{v}_K$, we want to find the assignment $\\mathcal{A}(\\cdot)$ and cluster centers $\\boldsymbol{v}_1, \\boldsymbol{v}_2, \\ldots, \\boldsymbol{v}_K$ that minimize the cost function $\\widehat{\\mathcal{J}}_{\\mathcal{S}}$.\n",
    "\n",
    "In other words, of all possible sets (there's a lot, as we should see later) $\\hat{\\mathcal{C}} = \\left\\{\\hat{C}_1, \\hat{C}_2, \\ldots, \\hat{C}_K\\right\\}$, we want to find the set $\\hat{\\mathcal{C}}$ that minimizes the cost function $\\widehat{\\mathcal{J}}_{\\mathcal{S}}$.\n",
    "```\n",
    "\n",
    "```{prf:theorem} Minimizing Individual Cluster's Cost is Equivalent to Minimizing the Objective Function\n",
    ":label: thm:minimizing-individual-clusters-cost-is-equivalent-to-minimizing-the-objective-function\n",
    "\n",
    "The objective function is equivalent to minimizing each cluster's cost individually.\n",
    "```\n",
    "\n",
    "Recall we mentioned that optimizing the objective function $\\widehat{\\mathcal{J}}_{\\mathcal{S}}$ means\n",
    "we are finding the optimal assignment $\\mathcal{A}^*(\\cdot)$ and the optimal cluster centers $\\boldsymbol{v}_1^*, \\boldsymbol{v}_2^*, \\ldots, \\boldsymbol{v}_K^*$\n",
    "at the same time. This is challenging as $\\widehat{\\mathcal{J}}_{\\mathcal{S}}$ is a non-convex function of $\\mathcal{A}(\\cdot)$ and $\\boldsymbol{v}_1, \\boldsymbol{v}_2, \\ldots, \\boldsymbol{v}_K$.\n",
    "We will now fall back on heuristics to find the local optimum. In what follows,\n",
    "we will list the *necessary* conditions to minimize the objective function $\\widehat{\\mathcal{J}}_{\\mathcal{S}}$.\n",
    "\n",
    "## The Necessary Conditions to Minimize the Objective Function\n",
    "\n",
    "With all the definitions in place, we can now formally state the necessary conditions to minimize the objective function.\n",
    "\n",
    "Note a necessary condition only guarantees that if a solution is optimal, then the conditions must be satisfied.\n",
    "However, if a solution does satisfy the conditions, it does not necessarily mean that it is optimal. In short,\n",
    "we may land ourselves with a **local** minimum that is not **globally** optimal.\n",
    "\n",
    "### Condition 1: The Optimal Assignment\n",
    "\n",
    "```{prf:criterion} K-Means Optimal Assignment\n",
    ":label: criterion:kmeans-optimal-assignment\n",
    "\n",
    "Fix the cluster centers $\\boldsymbol{v}_1, \\boldsymbol{v}_2, \\ldots, \\boldsymbol{v}_K$, we seek\n",
    "the optimal assignment $\\mathcal{A}^*(\\cdot)$ that minimizes the cost function $\\widehat{\\mathcal{J}}_{\\mathcal{S}}(\\cdot)$.\n",
    "\n",
    "We claim that the optimal assignment $\\mathcal{A}^*(\\cdot)$ follows the *nearest neighbor* rule, which means that,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{A}^*(n) = \\underset{k \\in \\{1, 2, \\ldots, K\\}}{\\operatorname{argmin}} \\left\\|\\mathbf{x}^{(n)} - \\boldsymbol{v}_k \\right\\|^2 .\n",
    "\\end{aligned}\n",
    "$$ (eq:k-means-criterion-1.1)\n",
    "\n",
    "Then the assignment $\\mathcal{A}^*$ is the optimal assignment that minimizes the cost function $\\widehat{\\mathcal{J}}_{\\mathcal{S}}$.\n",
    "\n",
    "This is quite intuitive as we are merely assigning each data point $\\mathbf{x}^{(n)}$ to cluster $k$\n",
    "whose center $\\boldsymbol{v}_k$ is closest to $\\mathbf{x}^{(n)}$.\n",
    "\n",
    "We rephrase the claim by saying that for any assignment $\\mathcal{A}$, we have\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\widehat{\\mathcal{J}}_{\\mathcal{S}}(\\mathcal{A}, \\boldsymbol{v}_1, \\boldsymbol{v}_2, \\ldots, \\boldsymbol{v}_K) &\\geq \\widehat{\\mathcal{J}}_{\\mathcal{S}}(\\mathcal{A}^*, \\boldsymbol{v}_1, \\boldsymbol{v}_2, \\ldots, \\boldsymbol{v}_K) \\\\\n",
    "\\end{aligned}\n",
    "$$ (eq:k-means-criterion-1.2)\n",
    "\n",
    "Let's prove this claim.\n",
    "```\n",
    "\n",
    "```{prf:proof}\n",
    "In equation {eq}`eq:k-means-criterion-1.2`, we have that $\\boldsymbol{v}_1, \\boldsymbol{v}_2, \\ldots, \\boldsymbol{v}_K$ are fixed.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\widehat{\\mathcal{J}}_{\\mathcal{S}}(\\mathcal{A}, \\boldsymbol{v}_1, \\boldsymbol{v}_2, \\ldots, \\boldsymbol{v}_K) &= \\sum_{n=1}^{N} \\sum_{\\mathcal{A}(n) = k} \\left\\|\\mathbf{x}^{(n)} - \\boldsymbol{v}_k \\right\\|^2 \\\\\n",
    "&\\geq \\sum_{n=1}^{N} \\sum_{\\mathcal{A}^*(n) = k} \\left\\|\\mathbf{x}^{(n)} - \\boldsymbol{v}_k \\right\\|^2 \\\\\n",
    "&= \\widehat{\\mathcal{J}}_{\\mathcal{S}}(\\mathcal{A}^*, \\boldsymbol{v}_1, \\boldsymbol{v}_2, \\ldots, \\boldsymbol{v}_K)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This is just a proof by definition of $\\mathcal{A}^*$ since\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\left\\|\\mathbf{x}^{(n)} - \\boldsymbol{v}_{\\mathcal{A}(n)} \\right\\|^2 \\geq \\left\\|\\mathbf{x}^{(n)} - \\boldsymbol{v}_{\\mathcal{A}^*(n)} \\right\\|^2 .\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "If you look at it intuitively, it just means there does not exist any other arrangement/assignment $\\mathcal{A}$\n",
    "that can reduce the cost function $\\widehat{\\mathcal{J}}_{\\mathcal{S}}$ better than the optimal assignment $\\mathcal{A}^*$ (nearest neighbor rule).\n",
    "```\n",
    "\n",
    "### Condition 2: The Optimal Cluster Centers (Centroids)\n",
    "\n",
    "```{prf:criterion} K-Means Optimal Cluster Centers\n",
    ":label: criterion:kmeans-optimal-cluster-centers\n",
    "\n",
    "Fix the assignment $\\mathcal{A}^*(\\cdot)$, we seek the optimal cluster centers $\\boldsymbol{v}_1^*, \\boldsymbol{v}_2^*, \\ldots, \\boldsymbol{v}_K^*$ that minimize the cost function $\\widehat{\\mathcal{J}}_{\\mathcal{S}}$.\n",
    "\n",
    "We claim that the optimal cluster centers is the mean of the data points assigned to each cluster.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\boldsymbol{v}_k^* = \\frac{1}{\\left|\\hat{C}_k^*\\right|} \\sum_{\\mathbf{x}^{(n)} \\in \\hat{C}_k^*} \\mathbf{x}^{(n)}\n",
    "\\end{aligned}\n",
    "$$ (eq:k-means-criterion-2.1)\n",
    "\n",
    "where $\\left|\\hat{C}_k^*\\right|$ is the number of data points assigned to cluster $k$. We can denote it\n",
    "as $N_k$ for convenience.\n",
    "\n",
    "We can also rephrease this claim by saying that for any cluster centers $\\boldsymbol{v}_1, \\boldsymbol{v}_2, \\ldots, \\boldsymbol{v}_K$, fixing the assignment $\\mathcal{A}^*$, we have\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\widehat{\\mathcal{J}}_{\\mathcal{S}}(\\mathcal{A}^*, \\boldsymbol{v}_1, \\boldsymbol{v}_2, \\ldots, \\boldsymbol{v}_K) &\\geq \\widehat{\\mathcal{J}}_{\\mathcal{S}}(\\mathcal{A}^*, \\boldsymbol{v}_1^*, \\boldsymbol{v}_2^*, \\ldots, \\boldsymbol{v}_K^*) \\\\\n",
    "\\end{aligned}\n",
    "$$ (eq:k-means-criterion-2.2)\n",
    "```\n",
    "\n",
    "```{prf:proof}\n",
    "This proof in short just says that the mean minimizes the sum of squared distances.\n",
    "\n",
    "Since we established ({prf:ref}`thm:minimizing-individual-clusters-cost-is-equivalent-to-minimizing-the-objective-function`)\n",
    "that minimizing each individual cluster $\\hat{C}_k$ is equivalent to minimizing the cost function $\\widehat{\\mathcal{J}}_{\\mathcal{S}}$,\n",
    "we can now fix any cluster $\\hat{C}_k$ (i.e. also fixing the assignment $\\mathcal{A}^*$) and seek the optimal cluster center $\\boldsymbol{v}_k^*$ that minimizes the cost function $\\widehat{\\mathcal{J}}_{\\hat{C}_k}$.\n",
    "\n",
    "Note after fixing the assignment $\\mathcal{A}^*(\\cdot)$, $\\widehat{\\mathcal{J}}_{\\hat{C}_k}$ is now just a\n",
    "function of $\\boldsymbol{v}_k$ and is the cost for that cluster.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\widehat{\\mathcal{J}}_{\\hat{C}_k}(\\boldsymbol{v}_k) = \\sum_{\\mathbf{x}^{(n)} \\in \\hat{C}_k} \\left\\|\\mathbf{x}^{(n)} - \\boldsymbol{v}_k \\right\\|^2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We can now take the derivative of $\\widehat{\\mathcal{J}}_{\\hat{C}_k}$ with respect to $\\boldsymbol{v}_k$ and set it to zero to find the optimal cluster center $\\boldsymbol{v}_k^*$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial}{\\partial \\boldsymbol{v}_k} \\widehat{\\mathcal{J}}_{\\hat{C}_k}(\\boldsymbol{v}_k) &= \\frac{\\partial}{\\partial \\boldsymbol{v}_k} \\sum_{\\mathbf{x}^{(n)} \\in \\hat{C}_k} \\left\\|\\mathbf{x}^{(n)} - \\boldsymbol{v}_k \\right\\|^2 \\\\\n",
    "&= 2 \\sum_{\\mathbf{x}^{(n)} \\in \\hat{C}_k} \\left(\\mathbf{x}^{(n)} - \\boldsymbol{v}_k \\right) \\\\\n",
    "&= 2 \\left( \\sum_{\\mathbf{x}^{(n)} \\in \\hat{C}_k} \\mathbf{x}^{(n)} - \\sum_{\\mathbf{x}^{(n)} \\in \\hat{C}_k} \\boldsymbol{v}_k \\right) \\\\\n",
    "&= 2 \\left( \\sum_{\\mathbf{x}^{(n)} \\in \\hat{C}_k} \\mathbf{x}^{(n)} - N_k \\boldsymbol{v}_k \\right) \\\\\n",
    "\\end{aligned}\n",
    "$$ (eq:derivative-of-k-means-cost-function)\n",
    "\n",
    "where $N_k$ is the number of data points assigned to cluster $k$.\n",
    "\n",
    "Now to minimize equation {eq}`eq:derivative-of-k-means-cost-function`, we set it to zero and solve for $\\boldsymbol{v}_k$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\quad\n",
    "\\frac{\\partial}{\\partial \\boldsymbol{v}_k} \\widehat{\\mathcal{J}}_{\\hat{C}_k}(\\boldsymbol{v}_k) = 0 \\\\\n",
    "\\iff &\\quad 2 \\left( \\sum_{\\mathbf{x}^{(n)} \\in \\hat{C}_k} \\mathbf{x}^{(n)} - N_k \\boldsymbol{v}_k \\right) = 0 \\\\\n",
    "\\iff &\\quad \\sum_{\\mathbf{x}^{(n)} \\in \\hat{C}_k} \\mathbf{x}^{(n)} - N_k \\boldsymbol{v}_k = 0 \\\\\n",
    "\\iff &\\quad \\sum_{\\mathbf{x}^{(n)} \\in \\hat{C}_k} \\mathbf{x}^{(n)} = N_k \\boldsymbol{v}_k \\\\\n",
    "\\iff &\\quad \\boldsymbol{v}_k = \\frac{1}{N_k} \\sum_{\\mathbf{x}^{(n)} \\in \\hat{C}_k} \\mathbf{x}^{(n)} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "recovering {prf:ref}`criterion:kmeans-optimal-cluster-centers`.\n",
    "\n",
    "There are other variants of [proof](https://math.stackexchange.com/questions/967138/formal-proof-that-mean-minimize-squared-error-function).\n",
    "```\n",
    "\n",
    "```{prf:remark} Notation\n",
    ":label: prf:remark:kmeans-optimal-cluster-centers-notation\n",
    "\n",
    "We will now denote $\\boldsymbol{v}_k^*$ as $\\boldsymbol{\\mu}_k$ in the following sections.\n",
    "```\n",
    "\n",
    "### Objective Function Re-defined\n",
    "\n",
    "We can now re-define the objective function in equation {eq}`eq:k-means-objective-function-1` in terms of the optimal cluster centers and assignments.\n",
    "\n",
    "$$\n",
    "\\begin{alignat}{4}\n",
    "\\underset{\\mathcal{A}, \\boldsymbol{\\mu}_k}{\\operatorname{argmin}} &\\quad& \\widehat{\\mathcal{J}}_{\\mathcal{S}}(\\mathcal{A}, \\boldsymbol{\\mu}_1, \\boldsymbol{\\mu}_2, \\ldots, \\boldsymbol{\\mu}_K) &= \\sum_{n=1}^{N} \\sum_{\\mathcal{A}(n) = k} \\left\\|\\mathbf{x}^{(n)} - \\boldsymbol{\\mu}_k \\right\\|^2  \\\\\n",
    "\\text{subject to} &\\quad& \\hat{C}_1 \\cup \\hat{C}_2 \\cup \\cdots \\cup \\hat{C}_K &= \\mathcal{S} \\\\\n",
    "&\\quad& \\boldsymbol{\\mu}_1, \\boldsymbol{\\mu}_2, \\ldots, \\boldsymbol{\\mu}_K &\\in \\mathbb{R}^D \\\\\n",
    "&\\quad& \\hat{y}^{(n)} := \\mathcal{A}(n) &= \\underset{k}{\\operatorname{argmin}} \\left\\|\\mathbf{x}^{(n)} - \\boldsymbol{\\mu}_k \\right\\|^2 \\\\\n",
    "\\end{alignat}\n",
    "$$ (eq:k-means-objective-function-2)\n",
    "\n",
    "```{prf:remark} Cost Function is a function of assignments and cluster centers\n",
    ":label: remark:kmeans-cost-function-is-a-function-of-assignments-and-cluster-centers\n",
    "\n",
    "Reminder!\n",
    "\n",
    "The cost function in equation {eq}`eq:k-means-objective-function-2` is a function of **both** the cluster assignments and cluster centers.\n",
    "And therefore we are minimizing the cost function with respect to the cluster assignments and cluster centers. However,\n",
    "jointly optimizing both the cluster assignments and cluster centers is computationally challenging, and therefore\n",
    "we split to two steps, first optimizing the cluster assignments and then optimizing the cluster centers in a greedy manner.\n",
    "```\n",
    "\n",
    "## Algorithm\n",
    "\n",
    "We are now ready to define the full Lloyd's algorithm for K-Means.\n",
    "\n",
    "```{prf:algorithm} Lloyd's Algorithm (K-Means)\n",
    ":label: lloyd-kmeans-algorithm\n",
    "\n",
    "Given a set of data points (samples)\n",
    "\n",
    "$$\n",
    "\\mathcal{S} = \\left\\{\\mathbf{x}^{(1)}, \\mathbf{x}^{(2)}, \\dots, \\mathbf{x}^{(N)}\\right\\}\n",
    "$$\n",
    "\n",
    "the K-Means algorithm aims to group the data points into $K$ clusters\n",
    "\n",
    "$$\n",
    "\\hat{\\mathcal{C}} = \\left\\{ \\hat{C}_1, \\hat{C}_2, \\dots, \\hat{C}_K \\right\\}\n",
    "$$\n",
    "\n",
    "such that the sum of squared distances\n",
    "between each data point and its cluster center is minimized.\n",
    "\n",
    "In code, $\\hat{\\mathcal{C}}$ can be treated as a dictionary/hash map,\n",
    "where the **key** is the cluster number and the **value** is the set of data points assigned to that cluster.\n",
    "\n",
    "\n",
    "1. **Initialization Step**: Initialize $K$ cluster centers $\\boldsymbol{\\mu}_1^{[0]}, \\boldsymbol{\\mu}_2^{[0]}, \\dots, \\boldsymbol{\\mu}_K^{[0]}$ randomly (best to be far apart)\n",
    "where the superscript $[0]$ denotes the iteration number $t=0$.\n",
    "    - In the very first iteration, there are no data points in any cluster $\\hat{C}_k^{[0]} = \\emptyset$. Therefore, the cluster centers are just randomly chosen for simplicity.\n",
    "    - By random, we mean that the cluster centers are randomly chosen from the data points $\\mathcal{S} = \\left\\{\\mathbf{x}^{(1)}, \\mathbf{x}^{(2)}, \\dots, \\mathbf{x}^{(N)}\\right\\}$\n",
    "    and not randomly chosen from the feature space $\\mathbb{R}^D$.\n",
    "    - Subsequent iterations will have data points in the clusters $\\hat{C}_k^{[t]} \\neq \\emptyset$ and thus\n",
    "    $\\boldsymbol{\\mu}_k^{[t]}$ will be the mean of the data points in cluster $k$.\n",
    "    - Each $\\boldsymbol{\\mu}_k^{[0]} = \\begin{bmatrix} \\mu_{1k}^{[0]} & \\mu_{2k}^{[0]} & \\cdots & \\mu_{Dk}^{[0]} \\end{bmatrix}^{\\mathrm{T}}$ is a $D$-dimensional vector, where $D$ is the number of features, and represents the\n",
    "    mean vector of all the data points in cluster $k$.\n",
    "    - Note that $\\mu_{dk}^{[0]}$ is the mean value of the $d$-th feature in cluster $k$.\n",
    "    - We denote $\\boldsymbol{\\mu} = \\begin{bmatrix} \\boldsymbol{\\mu}_1 & \\boldsymbol{\\mu}_2 & \\cdots & \\boldsymbol{\\mu}_K \\end{bmatrix}_{K \\times D}^{\\mathrm{T}}$ to be the collection of all $\\boldsymbol{\\mu}_1, \\boldsymbol{\\mu}_2, \\dots, \\boldsymbol{\\mu}_K$.\n",
    "\n",
    "\n",
    "2. **Assignment Step (E)**: For $t=0, 1, 2, \\dots$, assign each data point $\\mathbf{x}^{(n)}$ to the closest cluster center $\\boldsymbol{\\mu}_k^{[t]}$,\n",
    "\n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "    \\hat{y}^{(n)[t]} := \\mathcal{A}^{*(n)[t]} &= \\underset{k \\in \\{1, 2, \\ldots, K\\}}{\\operatorname{argmin}} \\left\\| \\mathbf{x}^{(n)} - \\boldsymbol{\\mu}_k^{[t]} \\right\\|^2 \\\\\n",
    "    \\end{aligned}\n",
    "    $$ (eq:kmeans-classify)\n",
    "\n",
    "    In other words, $\\hat{y}^{(n)[t]}$ is the output of the optimal assignment rule at the $t$-th iteration\n",
    "    and is the index of the cluster center $\\boldsymbol{\\mu}_k^{[t]}$ that is closest to $\\mathbf{x}^{(n)}$.\n",
    "\n",
    "    For instance, if $K = 3$, and for the first sample point $n=1$,\n",
    "    assume the closest cluster center is $\\boldsymbol{\\mu}_2^{[t]}$, then the assignment $\\mathcal{A}^{*}$ will\n",
    "    assign this point to cluster $k=2$, $\\hat{y}^{(1)} = 2$. Note that $\\hat{y}^{(n)}$ is a scalar and has the same superscript as $\\mathbf{x}^{(n)}$, indicating they belong to the same sample.\n",
    "\n",
    "    For notational convenience, we can also denote $\\hat{C}_k^{[t]}$ as the set of data points that are assigned to cluster $k$:\n",
    "\n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "    \\hat{C}_k^{[t]} &= \\left\\{ \\mathbf{x}^{(n)} \\mid \\hat{y}^{(n)} = k \\right\\}\n",
    "    \\end{aligned}\n",
    "    $$\n",
    "\n",
    "    Mathematically, this means partitioning the data points using [Voronoi Diagram](https://en.wikipedia.org/wiki/Voronoi_diagram),\n",
    "    as mentioned in the previous section {prf:ref}`def:kmeans-voronoi-partition`.\n",
    "\n",
    "3. **Update Step (M)**: Update the cluster centers for the next iteration.\n",
    "\n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "    \\boldsymbol{\\mu}_k^{[t+1]} &= \\frac{1}{|\\hat{C}_k^{[t]}|} \\sum_{\\mathbf{x}^{(n)} \\in \\hat{C}_k^{[t]}} \\mathbf{x}^{(n)} \\\\\n",
    "    \\end{aligned}\n",
    "    $$ (eq:kmeans-recenter)\n",
    "\n",
    "    Notice that the cluster center $\\boldsymbol{\\mu}_k^{[t+1]}$ is the mean of all data points that are assigned to cluster $k$.\n",
    "\n",
    "4. Repeat steps 2 and 3 until the centroids stop changing.\n",
    "\n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "    \\boldsymbol{\\mu}_k^{[t+1]} = \\boldsymbol{\\mu}_k^{[t]}\n",
    "    \\end{aligned}\n",
    "    $$ (eq:kmeans-convergence)\n",
    "\n",
    "    In other words,\n",
    "\n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "    \\widehat{\\mathcal{J}}_{\\mathcal{S}}^{[t+1]}\\left(\\mathcal{A}^{*[t+1]}, \\boldsymbol{\\mu}^{[t+1]} \\right) = \\widehat{\\mathcal{J}}_{\\mathcal{S}}^{[t]}\\left(\\mathcal{A}^{*[t]}, \\boldsymbol{\\mu}^{[t]} \\right)\n",
    "    \\end{aligned}\n",
    "    $$\n",
    "\n",
    "    This is the convergence condition.\n",
    "```\n",
    "\n",
    "```{prf:remark} K-Means is a Greedy Algorithm\n",
    ":label: remark-kmeans-greedy\n",
    "\n",
    "It is important to recognize that the K-Means (Lloyd's) Algorithm optimizes two objectives in an alternating fashion.\n",
    "It alternatively changes both the assignment step $\\mathcal{A}^{*}(\\cdot)$ and the update step $\\boldsymbol{\\mu}_k^{[t+1]}$\n",
    "to greedily minimize the cost function $\\widehat{\\mathcal{J}}(\\mathcal{A}, \\boldsymbol{\\mu})$.\n",
    "```\n",
    "\n",
    "## Convergence\n",
    "\n",
    "In this section, we will prove that the K-Means Algorithm converges to a local minimum of the cost function $\\widehat{\\mathcal{J}}(\\mathcal{A}, \\boldsymbol{\\mu})$.\n",
    "\n",
    "### Lemma 1: Stirling Numbers of the Second Kind\n",
    "\n",
    "```{prf:lemma} Stirling Numbers of the Second Kind\n",
    ":label: stirling-numbers\n",
    "\n",
    "The Stirling Numbers of the Second Kind $S(n, k)$ are defined as the number of ways to partition a set of $n$ elements into $k$ non-empty subsets.\n",
    "\n",
    "There are at most $k^n$ ways to partition a set of $n$ elements into $k$ non-empty subsets.\n",
    "\n",
    "In our case, since there are $N$ data points, and we want to partition them into $K$ clusters, there are at most $K^N$ ways to partition the data points into $K$ clusters.\n",
    "\n",
    "In other words, the assignment step $\\mathcal{A}(\\cdot)$ has at most $K^N$ possible mappings.\n",
    "The same applies to the update step $\\boldsymbol{\\mu}_k$ since $\\boldsymbol{\\mu}_k$ is dependent on the assignment step $\\mathcal{A}(\\cdot)$.\n",
    "```\n",
    "\n",
    "(cost-function-monotically-decreases)=\n",
    "### Lemma 2: Cost Function of K-Means Monotonically Decreases\n",
    "\n",
    "```{prf:lemma} Cost Function of K-Means Monotonically Decreases\n",
    ":label: kmeans-monotonic-decrease\n",
    "\n",
    "The cost function $\\widehat{\\mathcal{J}}$ of K-Means monotonically decreases. This means\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\widehat{\\mathcal{J}}^{[t+1]} \\leq \\widehat{\\mathcal{J}}^{[t]}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "for each iteration $t$.\n",
    "```\n",
    "\n",
    "```{prf:proof}\n",
    "This is a consequence of {prf:ref}`criterion:kmeans-optimal-assignment` and {prf:ref}`criterion:kmeans-optimal-cluster-centers`.\n",
    "\n",
    "In particular, the objective function $\\widehat{\\mathcal{J}}$ is made up of two steps, the assignment step and the update step. We minimize the assignment step by finding the optimal assignment $\\mathcal{A}^{*}(\\cdot)$, and we minimize the update step by finding the optimal cluster centers $\\boldsymbol{\\mu}_k^{*}$ based on the optimal assignment $\\mathcal{A}^{*}(\\cdot)$ at each iteration.\n",
    "\n",
    "Equation {eq}`eq:k-means-criterion-1.2` shows that for each iteration $t$, fixing the cluster\n",
    "centers (mean) $\\boldsymbol{\\mu}_k^{[t]}$, the assignment step $\\mathcal{A}^{*[t]}$ is optimal.\n",
    "\n",
    "This means\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\widehat{\\mathcal{J}}^{[t]} &= \\widehat{\\mathcal{J}}\\left(\\mathcal{A}^{*[t]}, \\boldsymbol{\\mu}_1^{[t]}, \\boldsymbol{\\mu}_2^{[t]}, \\dots, \\boldsymbol{\\mu}_K^{[t]}\\right) \\\\\n",
    "&\\geq \\widehat{\\mathcal{J}}\\left(\\mathcal{A}^{*(t + 1)}, \\boldsymbol{\\mu}_1^{[t]}, \\boldsymbol{\\mu}_2^{[t]}, \\dots, \\boldsymbol{\\mu}_K^{[t]}\\right) \\\\\n",
    "&= \\widehat{\\mathcal{J}}^{t+1}\\left(\\mathcal{A}^{*(t + 1)}, \\boldsymbol{\\mu}_1^{[t]}, \\boldsymbol{\\mu}_2^{[t]}, \\dots, \\boldsymbol{\\mu}_K^{[t]}\\right) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "What this inequality means is that at iteration $t$, at the assignment step (E), the cost function $\\widehat{\\mathcal{J}}$ is at least as large as the cost function $\\widehat{\\mathcal{J}}$ at the next iteration $t + 1$. This implies that the cost function $\\widehat{\\mathcal{J}}$ monotonically decreases\n",
    "at this step.\n",
    "\n",
    "Similarly, at the update step (M), the cost function $\\widehat{\\mathcal{J}}$ is at least as large as the cost function $\\widehat{\\mathcal{J}}$ at the next iteration $t + 1$, fixing the assignment $\\mathcal{A}^{*(t + 1)}$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\widehat{\\mathcal{J}}^{[t]} &= \\widehat{\\mathcal{J}}\\left(\\mathcal{A}^{*[t]}, \\boldsymbol{\\mu}_1^{[t]}, \\boldsymbol{\\mu}_2^{[t]}, \\dots, \\boldsymbol{\\mu}_K^{[t]}\\right) \\\\\n",
    "&\\geq \\widehat{\\mathcal{J}}\\left(\\mathcal{A}^{*[t]}, \\boldsymbol{\\mu}_1^{(t + 1)}, \\boldsymbol{\\mu}_2^{(t + 1)}, \\dots, \\boldsymbol{\\mu}_K^{(t + 1)}\\right) \\\\\n",
    "&= \\widehat{\\mathcal{J}}^{t+1}\\left(\\mathcal{A}^{*[t]}, \\boldsymbol{\\mu}_1^{(t + 1)}, \\boldsymbol{\\mu}_2^{(t + 1)}, \\dots, \\boldsymbol{\\mu}_K^{(t + 1)}\\right) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Now this means that at both steps, the cost function $\\widehat{\\mathcal{J}}$ monotonically decreases. So,\n",
    "the cost function $\\widehat{\\mathcal{J}}$ monotonically decreases at each iteration.\n",
    "```\n",
    "\n",
    "### Lemma 3: Monotone Convergence Theorem\n",
    "\n",
    "```{prf:lemma} Monotone Convergence Theorem\n",
    ":label: monotone-convergence\n",
    "\n",
    "The [Monotone Convergence Theorem](https://en.wikipedia.org/wiki/Monotone_convergence_theorem) states\n",
    "that if a sequence of functions $\\{f_n\\}$ is non-decreasing and bounded, then the sequence $\\{f_n\\}$ converges to a limit.\n",
    "\n",
    "In our case, the sequence of functions $\\{f_n\\}$ is the sequence of cost functions $\\left\\{\\widehat{\\mathcal{J}}^{[t]}\\right\\}$, and the limit is the cost function $\\widehat{\\mathcal{J}}^{*}$.\n",
    "\n",
    "So it is guaranteed that the sequence of cost functions $\\left\\{\\widehat{\\mathcal{J}}^{[t]}\\right\\}$ converges to the cost function $\\widehat{\\mathcal{J}}^{*}$ locally.\n",
    "```\n",
    "\n",
    "### K-Means Converges in Finite Steps\n",
    "\n",
    "We are now left to show that the sequence of cost functions $\\left\\{\\widehat{\\mathcal{J}}^{[t]}\\right\\}$ is finite,\n",
    "so that $\\left\\{\\widehat{\\mathcal{J}}^{[t]}\\right\\}$ converges in finite steps.\n",
    "\n",
    "Since {prf:ref}`stirling-numbers` states that there exists $K^N$ possible assignments\n",
    "$\\mathcal{A}(\\cdot)$, and simiarly exists $K^N$ possible cluster centers $\\boldsymbol{\\mu}_k$,\n",
    "then there exists $K^N$ possible cost functions $\\widehat{\\mathcal{J}}$. Then,\n",
    "\n",
    "- At each iteration $t$, the cost function $\\widehat{\\mathcal{J}}^{[t]}$ decreases monotonically.\n",
    "- This means at $t+1$, if the cost function $\\widehat{\\mathcal{J}}^{(t + 1)}$ decreases,\n",
    "    then this means the assignments $\\mathcal{A}^{*[t + 1]}$ are different from the assignments $\\mathcal{A}^{*[t]}$. Consequently, the partition never repeats if the cost function $\\widehat{\\mathcal{J}}$ decreases.\n",
    "- This means it will loop over each possible assignment $\\mathcal{A}$, and eventually converge to the unique solution $\\mathcal{A}^{*}(\\cdot)$.\n",
    "\n",
    "For that specific initialization, the algorithm has an unique solution, and it is guaranteed to converge to that solution.\n",
    "\n",
    "### Local Minima\n",
    "\n",
    "It is known that K-Means converges in finite steps but does not guarantee convergence\n",
    "to the global minimum. This means that for different initializations, K-Means can converge\n",
    "to different local minima.\n",
    "\n",
    "We can initialize the algorithm with different initializations, and run the algorithm\n",
    "multiple times. Then, we can choose the best solution among the different local minima.\n",
    "\n",
    "## Hypothesis Space\n",
    "\n",
    "For completeness sake, let’s define the hypothesis space $\\mathcal{H}$ for K-Means.\n",
    "\n",
    "Intuitively, the hypothesis space $\\mathcal{H}$ is the set of all possible clusterings of the data.\n",
    "\n",
    "Formally, given a set of $N$ data points $\\left\\{\\mathbf{x}^{(n)}\\right\\}_{n=1}^N$,\n",
    "let $C_k$ be the Voronoi cell of the $k$-th cluster center $\\boldsymbol{\\mu}_k$.\n",
    "\n",
    "Then, we can write the class of functions $\\mathcal{H}$ as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{H} &= \\left\\{\\mathcal{A}: \\mathbb{Z} \\rightarrow \\mathbb{Z} \\mid \\mathcal{A}(n) \\in \\{1, 2, \\dots, K\\} \\text{ for all } n \\in \\{1, 2, \\dots, N\\}\\right\\} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This means the hypothesis space $\\mathcal{H}$ is finite with cardinality $K^N$.\n",
    "\n",
    "For more details, see [here](https://stats.stackexchange.com/posts/502352/) and [here](https://courses.cs.washington.edu/courses/cse446/16sp/clustering_1.pdf).\n",
    "\n",
    "## How to find $K$?\n",
    "\n",
    "Since $K$ is a *priori*, we need to choose $K$ before we run the algorithm. Choosing the\n",
    "wrong $K$ will result in a poor clustering. So, how do we choose the right $K$?\n",
    "\n",
    "### Choose $K$ that Minimizes the Cost Function\n",
    "\n",
    "In normal supervised problem, we usually run the algorithm on the train dataset and\n",
    "choose the model that minimizes the cost function on the train dataset, or one that\n",
    "maximizes the performance.\n",
    "\n",
    "Can we do the same for K-Means? The answer is no, this is because our cost funtion\n",
    "monotonically decreases with increasing $K$.\n",
    "\n",
    "This is because we \"cover\" more input space $\\mathcal{X}$ with increasing $K$, thus\n",
    "decreasing the cost function {cite:ps}`pml1Book`.\n",
    "\n",
    "### Elbow Method\n",
    "\n",
    "While this may not be the best method, it is a simple and widely recognized one to choose $K$.\n",
    "\n",
    "The simple heuristic is described as follows:\n",
    "\n",
    "```{prf:algorithm} Elbow Method\n",
    ":label: elbow-method\n",
    "\n",
    "1. Run K-Means with $K$ from 1 to $K_{\\max}$.\n",
    "2. For each $k=0,1,\\ldots, K_{\\max}$, compute the cost function $\\widehat{\\mathcal{J}}_k$\n",
    "3. Plot the cost function $\\widehat{\\mathcal{J}}_k$ against $k$.\n",
    "4. Find the \"elbow\" of the curve, which is the point where the cost function $\\widehat{\\mathcal{J}}_k$ starts to decrease more slowly.\n",
    "```\n",
    "\n",
    "### Other Methods\n",
    "\n",
    "See {cite}`pml1Book` for more methods.\n",
    "\n",
    "## Time and Space Complexity\n",
    "\n",
    "### Brute Force Search and Global Minimum\n",
    "\n",
    "The hypothesis space $\\mathcal{H}$ is finite, implying that\n",
    "if we do a brute force search over all possible clusterings, we can find the global minimum.\n",
    "\n",
    "Quoting from [CMU 10-315](http://www.cs.cmu.edu/~ninamf/courses/315sp19/homeworks/hw6.pdf),\n",
    "we consider the brute-force search to be the following:\n",
    "\n",
    "```{prf:algorithm} Brute Force Search for K-Means\n",
    ":label: brute-force-search-kmeans\n",
    "\n",
    "For each possible cluster\n",
    "\n",
    "$$\n",
    "\\hat{\\mathcal{C}} = \\left\\{\\hat{C}_1, \\hat{C}_2, \\dots, \\hat{C}_K\\right\\}\n",
    "$$\n",
    "\n",
    "induced by the assignment $\\mathcal{A}$ in $\\mathcal{H}$, compute the\n",
    "optimal centroids\n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol{\\mu}} = \\left\\{\\hat{\\boldsymbol{\\mu}}_1, \\hat{\\boldsymbol{\\mu}}_2, \\dots, \\hat{\\boldsymbol{\\mu}}_K\\right\\}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol{\\mu}}_k = \\frac{1}{|\\hat{C}_k|} \\sum_{\\mathbf{x} \\in \\hat{C}_k} \\mathbf{x}\n",
    "$$\n",
    "\n",
    "is the mean of the points in the $k$-th cluster.\n",
    "\n",
    "Then, compute the cost function $\\widehat{\\mathcal{J}}$ centroids $\\hat{\\boldsymbol{\\mu}}$.\n",
    "\n",
    "Repeat this for all possible clusterings $\\mathcal{A}(\\cdot)$ in $\\mathcal{H}$ and finally\n",
    "return the clustering $\\hat{C}$ that gives the minimum cost function $\\widehat{\\mathcal{J}}$.\n",
    "```\n",
    "\n",
    "Then the time complexity of the brute force search is exponential with respect to the number of inputs since there are $K^N$ possible clusterings and\n",
    "we are looping over each possible clustering to find the global minimum. Indeed, this has time complexity\n",
    "\n",
    "$$\n",
    "\\mathcal{O}\\left(K^N\\right)\n",
    "$$\n",
    "\n",
    "### Lloyd’s Algorithm\n",
    "\n",
    "Let $T$ denote the number of iterations of Lloyd’s algorithm.\n",
    "\n",
    "Then, the average time complexity of Lloyd’s algorithm is\n",
    "\n",
    "$$\n",
    "\\mathcal{O}(T N K D)\n",
    "$$\n",
    "\n",
    "where $N$ is the number of data points, $K$ is the number of clusters, and $D$ is the number of features.\n",
    "\n",
    "This can be easily seen in the python implementation written [here](https://github.com/gao-hongnan/gaohn-probability-stats/blob/machine-learning/src/clustering/kmeans/kmeans.py).\n",
    "We are essentially looping like this:\n",
    "\n",
    "```python\n",
    "for t in range(max_iter):\n",
    "    # E Step: Assign each data point to the closest cluster center\n",
    "    for n in range(n_samples):\n",
    "      # compute argmin distance O(KD) since we are looping over all\n",
    "      # K cluster centers and each cluster center has D features\n",
    "\n",
    "      # do assignment which requires you to loop over all\n",
    "      # K cluster centers: O(N)\n",
    "\n",
    "    # so total O(NKD) here already\n",
    "\n",
    "  # M step: Update the cluster centers\n",
    "  for k in range(n_clusters):\n",
    "    # compute the mean of the points in the k-th cluster: O(KD)\n",
    "    # since we are looping over all K cluster centers and each\n",
    "    # cluster center has D features\n",
    "```\n",
    "\n",
    "where the total time complexity approximately $\\mathcal{O}(T N K D)$.\n",
    "\n",
    "The worst case complexity is given by $\\mathcal{O}\\left(N^{(K+2/D)}\\right)$[^worst-case-time-complexity].\n",
    "\n",
    "\n",
    "```{list-table} Time Complexity of K-Means\n",
    ":header-rows: 1\n",
    ":name: time-complexity-kmeans\n",
    "\n",
    "* - Train\n",
    "  - Inference\n",
    "* - $\\mathcal{O}(NKD)$\n",
    "  - $\\mathcal{O}(KD)$\n",
    "```\n",
    "\n",
    "For space complexity, we need to store the cluster centers $\\boldsymbol{\\mu}_k$ and the cluster assignments $\\mathcal{A}(n)$, where the former is a $K \\times D$ matrix and the latter is a $N$-dimensional vector.\n",
    "We typically do not include the input data $\\left\\{\\mathbf{x}^{(n)}\\right\\}_{n=1}^N$ in the space complexity since it is given. If included that is $\\mathcal{O}(ND)$, totalling $\\mathcal{O}(N + KD + ND)$.\n",
    "\n",
    "Inference wise, even for a single data point, we need to compute the distance to all cluster centers,\n",
    "so you need to invoke the cluster centers $\\boldsymbol{\\mu}_k$, so roughly is $\\mathcal{O}(KD)$.\n",
    "\n",
    "```{list-table} Space Complexity of K-Means\n",
    ":header-rows: 1\n",
    ":name: space-complexity-kmeans\n",
    "\n",
    "* - Train\n",
    "  - Inference\n",
    "* - $\\mathcal{O}(KD + N)$\n",
    "  - $\\mathcal{O}(KD)$\n",
    "```\n",
    "\n",
    "## When to Use K-Means?\n",
    "\n",
    "See [google's article](https://developers.google.com/machine-learning/clustering/algorithm/advantages-disadvantages).\n",
    "\n",
    "- Relatively simple to implement.\n",
    "\n",
    "- Scales to large data sets.\n",
    "\n",
    "- Guarantees local convergence.\n",
    "\n",
    "- Can warm-start the positions of centroids.\n",
    "\n",
    "- Easily adapts to new examples.\n",
    "\n",
    "- Generalizes to clusters of different shapes and sizes, such as elliptical clusters.\n",
    "\n",
    "## When can K-Means Fail?\n",
    "\n",
    "- The number of clusters ($K$) is specified a **priori**, which means we need to specify\n",
    "    the number of clusters before running the algorithm. Choosing $K$ may not be straightforward,\n",
    "    especially in the case of high-dimensional data.\n",
    "- The Lloyd’s algorithm is sensitive to the initial cluster centers. This means that\n",
    "    the algorithm may converge to a local minimum instead of the global minimum. To remedy this,\n",
    "    we can run the algorithm multiple times with different initial cluster centers.\n",
    "- K-Means assumes spherical clusters. This is not obvious.\n",
    "  - K-Means assumes spherical shape because the algorithm uses the euclidean distance metric to measure the similarity between observations and centroids. euclidean distance is a measure of straight-line distance between two points in a euclidean space, and it assumes that the data is isotropic, meaning that the **variance** along all dimensions is equal. Now\n",
    "  imagine a cluster with an elliptical shape. And imagine the principal axis is quite long, then two points at the extreme ends of the cluster will have a large euclidean distance. This means that the cluster may be split into two clusters by K-Means, which is not what we want. On the contrary,\n",
    "  if the cluster is spherical, then the euclidean distance between two points at the extreme ends of the cluster will be equidistant to the centroid.\n",
    "  - Further quoting [the answer here](https://stats.stackexchange.com/questions/133656/how-to-understand-the-drawbacks-of-k-means),\n",
    "    K-means is a special case of Gaussian Mixture Models (GMM). GMM assumes that the data comes from a mixture of $K$ Gaussian distributions. In  other words, there is a certain probability that the data comes from one of $K$ of the Gaussian distributions.\n",
    "\n",
    "    If we make the the probability to be in each of the $K$ Gaussians equal and make the covariance matrices to be $\\sigma^2 \\mathbf{I}$, where $\\sigma^2 $ is the same fixed constant for each of the $K$ Gaussians, and take the limit when $\\sigma^2 \\rightarrow 0$ then we get K-means.\n",
    "\n",
    "    So, what does this tell us about the drawbacks of K-means?\n",
    "\n",
    "    1. K-means leads to clusters that look multivariate Gaussian.\n",
    "    2. Since the variance across the variables is the same, K-means leads to clusters that look spherical.\n",
    "    3. Not only do clusters look spherical, but since the covariance matrix is the same across the $K$ groups, K-means leads to clusters that look like the same sphere.\n",
    "    4. K-means tends towards equal sized groups.\n",
    "\n",
    "    Overall, if we interpret K-Means from the perspective of probabilistic modeling, then we can see that K-Means is a special case of GMM.\n",
    "    And recall that [in the geometry of multivariate gaussian](../../../probability_theory/05_joint_distributions/0507_multivariate_gaussian/geometry_of_multivariate_gaussian.md),\n",
    "    the shape of the multivariate gaussian is determined by the covariance matrix. Since we have deduced that\n",
    "    the covariance matrix is $\\sigma^2 \\mathbf{I}$, a diagonal matrix with equal variance across all features, then the\n",
    "    shape is a sphere since the axis has equal length.\n",
    "- Scaling with number of dimensions. As the number of dimensions increases, a distance-based similarity measure converges to a constant value between any given examples. Reduce dimensionality either by using PCA on the feature data, or by using “spectral clustering” to modify the clustering algorithm.\n",
    "- Best to feature scale if we use euclidean distance as the distance metric. This is because features with larger scale will dominate the distance metric.\n",
    "- For categorical features, we no longer use mean as the cluster center. Instead, we use the mode.\n",
    "\n",
    "## K-Means++\n",
    "\n",
    "We have seen earlier that convergence can be an issue with K-Means, and it is recommended to use different seed\n",
    "initializations to get better results.\n",
    "\n",
    "We state a better initialization method, K-Means++. The intuition behind this approach is that spreading out the $K$ initial cluster centers is a good thing: the first cluster center is chosen uniformly at random from the data points that are being clustered, after which each subsequent cluster center is chosen from the remaining data points with probability proportional to its squared distance from the point's closest existing cluster center.\n",
    "\n",
    "From [Wikipeda: K-Means++](https://en.wikipedia.org/wiki/K-means%2B%2B), we have the following:\n",
    "\n",
    "The exact algorithm is as follows:\n",
    "\n",
    "1. Choose one center uniformly at random among the data points.\n",
    "2. For each data point $\\mathbf{x}$ not chosen yet, compute $\\mathrm{D}(\\mathbf{x})$, the distance between $\\mathbf{x}$ and the nearest center that has already been chosen.\n",
    "3. Choose one new data point at random as a new center, using a weighted probability distribution where a point $\\mathbf{x}$ is chosen with probability proportional to $\\mathrm{D}(\\mathbf{x})^2$.\n",
    "4. Repeat Steps 2 and 3 until $K$ centers have been chosen.\n",
    "5. Now that the initial centers have been chosen, proceed using standard $K$-means clustering.\n",
    "\n",
    "What is more surprising is that this method can be shown to guarantee that the recontruction error is never more than $\\mathcal{O}(\\log K)$ worse than optimal\n",
    "{cite:ps}`pml1Book`.\n",
    "\n",
    "## K-Medoids\n",
    "\n",
    "See section 21.3.5 of Probabilistic Machine Learning: An Introduction by Kevin P. Murphy.\n",
    "\n",
    "## References and Further Readings\n",
    "\n",
    "I also highly recommend Nathaniel Dake's [blog on K-Means](https://www.nathanieldake.com/Machine_Learning/04-Unsupervised_Learning_Cluster_Analysis-02-Cluster-Analysis-K-Means-Clustering.html)\n",
    "here, he does a fantatic job in explaining the intuition behind K-Means and provide visualizations to help you understand the algorithm,\n",
    "especially how K-Means can fail.\n",
    "\n",
    "- Murphy, Kevin P. \"Chapter 21.3. K-Means Clustering.\" In Probabilistic Machine Learning: An Introduction. MIT Press, 2022.\n",
    "- Hal Daumé III. \"Chapter 3.4. K-Means Clustering.\" In A Course in Machine Learning, January 2017.\n",
    "- Hal Daumé III. \"Chapter 15.1. K-Means Clustering.\" In A Course in Machine Learning, January 2017.\n",
    "- Bishop, Christopher M. \"Chapter 9.1. K-Means Clustering.\" In Pattern Recognition and Machine Learning. New York: Springer-Verlag, 2016.\n",
    "- James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. \"Chapter 12.4.1. K-Means Clustering.\" In An Introduction to Statistical Learning: With Applications in R. Boston: Springer, 2022.\n",
    "- Hastie, Trevor, Tibshirani, Robert and Friedman, Jerome. \"Chapter 14.3. Cluster Analysis.\" In The Elements of Statistical Learning. New York, NY, USA: Springer New York Inc., 2001.\n",
    "- Raschka, Sebastian. \"Chapter 10.1. Grouping objects by similarity using k-means.\" In Machine Learning with PyTorch and Scikit-Learn.\n",
    "- Jung, Alexander. \"Chapter 8.1. Hard Clustering with K-Means.\" In Machine Learning: The Basics. Singapore: Springer Nature Singapore, 2023.\n",
    "- Vincent, Tan. \"Lecture 17a.\" In MA4270 Data Modelling and Computation.\n",
    "\n",
    "\n",
    "[^disjoint_union]: Disjoint union indicates that each data point $\\mathbf{x}^{(n)}$\n",
    "can be assigned to one and only one cluster $C_k$.\n",
    "[^collection_of_clusters]: Note $C$ is not the same as $\\mathcal{S}$ even though\n",
    "they both represent all samples. The cardinality of $C$ is $K$, while the cardinality of $\\mathcal{S}$ is $N$.\n",
    "[^jointly_optimizing]: This means that we are jointly optimizing the assignments $\\mathcal{A}$ and the cluster centers $\\boldsymbol{\\mu}_k$.\n",
    "[^y]: There's actually no ground truth target labels in unsupervised learning, this is for education purposes.\n",
    "[^equivalent-k-means-cost-function]: The reason of writing so many equivalent forms is because many textbooks use different notations, so I tried to list a few common ones.\n",
    "[^worst-case-time-complexity]: Refer to “How slow is the k-means method?” D. Arthur and S. Vassilvitskii - SoCG2006. for more details."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.11.5"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "mystnb": {
   "number_source_lines": true
  },
  "source_map": [
   18,
   31,
   235,
   256
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}