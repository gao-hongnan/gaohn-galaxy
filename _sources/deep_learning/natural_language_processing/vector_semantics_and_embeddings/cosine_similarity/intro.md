# Cosine Similarity and Notion of Closeness

To formalize the intuition of closeness between two word (vectors), we will formally introduce
the concept of cosine similarity.

## Motivation

Cosine similarity is a commonly used similarity measure in natural language processing (NLP) tasks such as text classification, information retrieval, and recommendation systems. The motivation behind using cosine similarity in NLP is that it provides a way to compare the similarity of two documents, sentences or words regardless of their size or the frequency of the terms they contain.

In NLP, text data is typically represented as high-dimensional vectors where each dimension represents a term or a word in a corpus. The vector contains the frequency or occurrence of each term or word in the text data. Cosine similarity calculates the angle between two vectors and provides a measure of similarity between them, regardless of their length or frequency. This makes it particularly useful for comparing the similarity of two documents that have different lengths or that use different words to express similar meanings.

## References and Further Readings

- Jurafsky, Dan, and James H. Martin. "Chapter 6.4. Cosine for Measuring Similarity." In Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition. Pearson, 2022.