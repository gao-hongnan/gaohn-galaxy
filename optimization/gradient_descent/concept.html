
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Gradient Descent Concept &#8212; Machine Learning Chronicles</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=796348d33e8b1d947c94" rel="stylesheet">
<link href="../../_static/styles/bootstrap.css?digest=796348d33e8b1d947c94" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=796348d33e8b1d947c94" rel="stylesheet">

  
  <link href="../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=796348d33e8b1d947c94" rel="stylesheet">
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2">
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2">
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=4ec06e9971c5264fbd345897d5258098f11cc577" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=796348d33e8b1d947c94">
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=796348d33e8b1d947c94">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=8bf782fb4ee92b3d3646425e50f299c4e1fd152d"></script>
    <script src="../../_static/tabs.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"tex": {"macros": {"defeq": "\\overset{\\text{def}}{=}", "defa": "\\overset{\\text{(a)}}{=}", "defb": "\\overset{\\text{(b)}}{=}", "defc": "\\overset{\\text{(c)}}{=}", "defd": "\\overset{\\text{(d)}}{=}", "st": "\\mid", "mod": "\\mid", "S": "\\Omega", "s": "\\omega", "e": "\\exp", "P": "\\mathbb{P}", "R": "\\mathbb{R}", "expectation": "\\mathbb{E}", "v": "\\mathbf{v}", "a": "\\mathbf{a}", "b": "\\mathbf{b}", "c": "\\mathbf{c}", "u": "\\mathbf{u}", "w": "\\mathbf{w}", "x": "\\mathbf{x}", "y": "\\mathbf{y}", "z": "\\mathbf{z}", "0": "\\mathbf{0}", "1": "\\mathbf{1}", "A": "\\mathbf{A}", "B": "\\mathbf{B}", "C": "\\mathbf{C}", "E": "\\mathcal{F}", "eventA": "\\mathcal{A}", "lset": "\\left\\{", "rset": "\\right\\}", "lsq": "\\left[", "rsq": "\\right]", "lpar": "\\left(", "rpar": "\\right)", "lcurl": "\\left\\{", "rcurl": "\\right\\}", "pmf": "p_X", "pdf": "f_X", "pdftwo": "f_{X,Y}", "pdfjoint": "f_{\\mathbf{X}}", "pmfjointxy": "p_{X, Y}", "pdfjointxy": "f_{X, Y}", "cdf": "F_X", "pspace": "(\\Omega, \\mathcal{F}, \\mathbb{P})", "var": "\\operatorname{Var}", "std": "\\operatorname{Std}", "bern": "\\operatorname{Bernoulli}", "binomial": "\\operatorname{Binomial}", "geometric": "\\operatorname{Geometric}", "poisson": "\\operatorname{Poisson}", "uniform": "\\operatorname{Uniform}", "normal": "\\operatorname{Normal}", "gaussian": "\\operatorname{Gaussian}", "gaussiansymbol": "\\mathcal{N}", "exponential": "\\operatorname{Exponential}", "iid": "\\textbf{i.i.d.}", "and": "\\text{and}"}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'optimization/gradient_descent/concept';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Gradient Descent Construction" href="implementation.html" />
    <link rel="prev" title="Gradient Descent" href="intro.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="docsearch:language" content="en">
  </head>
  
  
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="180" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>

  
  <input type="checkbox" class="sidebar-toggle" name="__primary" id="__primary">
  <label class="overlay overlay-primary" for="__primary"></label>

  
  <input type="checkbox" class="sidebar-toggle" name="__secondary" id="__secondary">
  <label class="overlay overlay-secondary" for="__secondary"></label>

  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
      
<form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false">
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
    </div>
  </div>

  
  <nav class="bd-header navbar navbar-expand-lg bd-navbar" id="navbar-main"><div class="bd-header__inner bd-page-width">
  <label class="sidebar-toggle primary-toggle" for="__primary">
      <span class="fa-solid fa-bars"></span>
  </label>
  <div id="navbar-start">
    
    
  


<a class="navbar-brand logo" href="../../intro.html">

  
  
  
  
  
  
  

  
    <img src="../../_static/logo.png" class="logo__image only-light" alt="Logo image">
    <img src="../../_static/logo.png" class="logo__image only-dark" alt="Logo image">
  
  
</a>
    
  </div>

  
  <div class="col-lg-9 navbar-header-items">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <nav class="navbar-nav">
    <p class="sidebar-header-items__title" role="heading" aria-level="1" aria-label="Site Navigation">
        Site Navigation
    </p>
    <ul id="navbar-main-elements" class="navbar-nav">
        
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../notations/mathematical_notations.html">
                        Mathematical Notations
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../notations/machine_learning_notations.html">
                        Machine Learning Notations
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../notations/deep_learning_notations.html">
                        Deep Learning Notations
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../probability_theory/01_mathematical_preliminaries/intro.html">
                        Chapter 1. Mathematical Preliminaries
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../probability_theory/02_probability/intro.html">
                        Chapter 2. Probability
                      </a>
                    </li>
                
            <div class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                    More
                </button>
                <div class="dropdown-menu">
                    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../probability_theory/03_discrete_random_variables/intro.html">
                        Chapter 3. Discrete Random Variables
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../probability_theory/04_continuous_random_variables/intro.html">
                        Chapter 4. Continuous Random Variables
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../probability_theory/05_joint_distributions/intro.html">
                        Chapter 5. Joint Distributions
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../probability_theory/06_sample_statistics/intro.html">
                        Chapter 6. Sample Statistics
                      </a>
                    </li>
                

                    <li class="nav-item current active">
                      <a class="nav-link nav-internal" href="intro.html">
                        Gradient Descent
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../machine_learning/fundamentals/intro.html">
                        Fundamentals
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../machine_learning/linear_models/intro.html">
                        Linear Models
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../machine_learning/generative/intro.html">
                        Generative
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../machine_learning/model_selection_and_evaluation/intro.html">
                        Model Selection and Evaluation
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../machine_learning/trees/intro.html">
                        Trees, Forests, Bagging and Boosting
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../machine_learning/decomposition/intro.html">
                        Dimensionality Reduction
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../machine_learning/neighbours/intro.html">
                        Neighbourhood
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../machine_learning/mixtures/intro.html">
                        Mixture Models
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../machine_learning/clustering/intro.html">
                        Clustering
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../deep_learning/natural_language_processing/intro.html">
                        Natural Language Processing (NLP)
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../references_resources_roadmap/bibliography.html">
                        Bibliography
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../references_resources_roadmap/resources.html">
                        Resources
                      </a>
                    </li>
                
                </div>
            </div>
            
    </ul>
</nav>
      </div>
      
    </div>

    <div id="navbar-end">
      
        <div class="navbar-end-item navbar-persistent--container">
          
<button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-toggle="tooltip">
  <i class="fa-solid fa-magnifying-glass"></i>
</button>
        </div>
      
      
      <div class="navbar-end-item">
        <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
</button>
      </div>
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
      </ul>
      </div>
      
    </div>
  </div>


  
  
    <div class="navbar-persistent--mobile">
<button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-toggle="tooltip">
  <i class="fa-solid fa-magnifying-glass"></i>
</button>
    </div>
  

  
  <label class="sidebar-toggle secondary-toggle" for="__secondary">
      <span class="fa-solid fa-outdent"></span>
  </label>
  

</div>
  </nav>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        
  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
      
      <div class="navbar-center-item">
        <nav class="navbar-nav">
    <p class="sidebar-header-items__title" role="heading" aria-level="1" aria-label="Site Navigation">
        Site Navigation
    </p>
    <ul id="navbar-main-elements" class="navbar-nav">
        
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../notations/mathematical_notations.html">
                        Mathematical Notations
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../notations/machine_learning_notations.html">
                        Machine Learning Notations
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../notations/deep_learning_notations.html">
                        Deep Learning Notations
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../probability_theory/01_mathematical_preliminaries/intro.html">
                        Chapter 1. Mathematical Preliminaries
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../probability_theory/02_probability/intro.html">
                        Chapter 2. Probability
                      </a>
                    </li>
                
            <div class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                    More
                </button>
                <div class="dropdown-menu">
                    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../probability_theory/03_discrete_random_variables/intro.html">
                        Chapter 3. Discrete Random Variables
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../probability_theory/04_continuous_random_variables/intro.html">
                        Chapter 4. Continuous Random Variables
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../probability_theory/05_joint_distributions/intro.html">
                        Chapter 5. Joint Distributions
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../probability_theory/06_sample_statistics/intro.html">
                        Chapter 6. Sample Statistics
                      </a>
                    </li>
                

                    <li class="nav-item current active">
                      <a class="nav-link nav-internal" href="intro.html">
                        Gradient Descent
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../machine_learning/fundamentals/intro.html">
                        Fundamentals
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../machine_learning/linear_models/intro.html">
                        Linear Models
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../machine_learning/generative/intro.html">
                        Generative
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../machine_learning/model_selection_and_evaluation/intro.html">
                        Model Selection and Evaluation
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../machine_learning/trees/intro.html">
                        Trees, Forests, Bagging and Boosting
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../machine_learning/decomposition/intro.html">
                        Dimensionality Reduction
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../machine_learning/neighbours/intro.html">
                        Neighbourhood
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../machine_learning/mixtures/intro.html">
                        Mixture Models
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../machine_learning/clustering/intro.html">
                        Clustering
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../deep_learning/natural_language_processing/intro.html">
                        Natural Language Processing (NLP)
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../references_resources_roadmap/bibliography.html">
                        Bibliography
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../references_resources_roadmap/resources.html">
                        Resources
                      </a>
                    </li>
                
                </div>
            </div>
            
    </ul>
</nav>
      </div>
      
      </div>
    

    
    
    <div class="sidebar-header-items__end">
      
      <div class="navbar-end-item">
        <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
</button>
      </div>
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
      </ul>
      </div>
      
    </div>
    
  </div>

  
  <div class="sidebar-start-items sidebar-primary__section">
    <div class="sidebar-start-items__item">
  


<a class="navbar-brand logo" href="../../intro.html">

  
  
  
  
  
  
  

  
    <img src="../../_static/logo.png" class="logo__image only-light" alt="Logo image">
    <img src="../../_static/logo.png" class="logo__image only-dark" alt="Logo image">
  
  
</a>
    </div>
    <div class="sidebar-start-items__item">
<form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false">
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
    </div>
    <div class="sidebar-start-items__item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    Introduction
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Notations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../notations/mathematical_notations.html">Mathematical Notations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notations/machine_learning_notations.html">Machine Learning Notations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notations/deep_learning_notations.html">Deep Learning Notations</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Probability Theory</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../probability_theory/01_mathematical_preliminaries/intro.html">Chapter 1. Mathematical Preliminaries</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/01_mathematical_preliminaries/01_combinatorics.html">Permutations and Combinations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/01_mathematical_preliminaries/02_calculus.html">Calculus</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/01_mathematical_preliminaries/contours.html">Contour Maps</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/01_mathematical_preliminaries/exercises.html">Exercises</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../probability_theory/02_probability/intro.html">Chapter 2. Probability</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/02_probability/0202_probability_space.html">Probability Space</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/02_probability/0203_probability_axioms.html">Probability Axioms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/02_probability/0204_conditional_probability.html">Conditional Probability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/02_probability/0205_independence.html">Independence</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/02_probability/0206_bayes_theorem.html">Baye’s Theorem and the Law of Total Probability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/02_probability/summary.html">Summary</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/intro.html">Chapter 3. Discrete Random Variables</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/0301_random_variables.html">Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/0302_discrete_random_variables.html">Discrete Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/0303_probability_mass_function.html">Probability Mass Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/0304_cumulative_distribution_function.html">Cumulative Distribution Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/0305_expectation.html">Expectation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/0306_moments_and_variance.html">Moments and Variance</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/uniform/intro.html">Discrete Uniform Distribution</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/uniform/0307_discrete_uniform_distribution_concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/uniform/0307_discrete_uniform_distribution_application.html">Application</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/bernoulli/intro.html">Bernoulli Distribution</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/bernoulli/0308_bernoulli_distribution_concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/bernoulli/0308_bernoulli_distribution_application.html">Application</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/iid.html">Independent and Identically Distributed (IID)</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/binomial/intro.html">Binomial Distribution</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/binomial/0309_binomial_distribution_concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/binomial/0309_binomial_distribution_implementation.html">Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/binomial/0309_binomial_distribution_application.html">Real World Examples</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/geometric/intro.html">Geometric Distribution</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/geometric/0310_geometric_distribution_concept.html">Concept</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/poisson/intro.html">Poisson Distribution</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/poisson/0311_poisson_distribution_concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/poisson/0311_poisson_distribution_implementation.html">Implementation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/summary.html">Important</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/exercises.html">Exercises</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/intro.html">Chapter 4. Continuous Random Variables</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/from_discrete_to_continuous.html">From Discrete to Continuous</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0401_continuous_random_variables.html">Continuous Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0402_probability_density_function.html">Probability Density Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0403_expectation.html">Expectation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0404_moments_and_variance.html">Moments and Variance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0405_cumulative_distribution_function.html">Cumulative Distribution Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0406_mean_median_mode.html">Mean, Median and Mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0407_continuous_uniform_distribution.html">Continuous Uniform Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0408_exponential_distribution.html">Exponential Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0409_gaussian_distribution.html">Gaussian Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0410_skewness_and_kurtosis.html">Skewness and Kurtosis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0411_convolve_and_sum_of_random_variables.html">Convolution and Sum of Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0412_functions_of_random_variables.html">Functions of Random Variables</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../probability_theory/05_joint_distributions/intro.html">Chapter 5. Joint Distributions</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/05_joint_distributions/from_single_variable_to_joint_distributions.html">From Single Variable to Joint Distributions</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0501_joint_pmf_pdf/intro.html">Joint PMF and PDF</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0501_joint_pmf_pdf/concept.html">Concept</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0502_joint_expectation_and_correlation/intro.html">Joint Expectation and Correlation</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0502_joint_expectation_and_correlation/concept.html">Concept</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0503_conditional_pmf_pdf/intro.html">Conditional PMF and PDF</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0503_conditional_pmf_pdf/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0503_conditional_pmf_pdf/application.html">Application</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0504_conditional_expectation_variance/intro.html">Conditional Expectation and Variance</a><input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-14"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0504_conditional_expectation_variance/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0504_conditional_expectation_variance/exercises.html">Exercises</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0505_sum_of_random_variables/intro.html">Sum of Random Variables</a><input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-15"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0505_sum_of_random_variables/concept.html">Concept</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0506_random_vectors/intro.html">Random Vectors</a><input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-16"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0506_random_vectors/concept.html">Concept</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0507_multivariate_gaussian/intro.html">Multivariate Gaussian Distribution</a><input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-17"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0507_multivariate_gaussian/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0507_multivariate_gaussian/application_transformation.html">Application: Plots and Transformations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0507_multivariate_gaussian/psd.html">Covariance Matrix is Positive Semi-Definite</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0507_multivariate_gaussian/eigendecomposition.html">Eigendecomposition and Covariance Matrix</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0507_multivariate_gaussian/geometry_of_multivariate_gaussian.html">The Geometry of Multivariate Gaussians</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../probability_theory/06_sample_statistics/intro.html">Chapter 6. Sample Statistics</a><input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-18"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0601_moment_generating_and_characteristic_functions/intro.html">Moment Generating and Characteristic Functions</a><input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-19"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0601_moment_generating_and_characteristic_functions/moment_generating_function.html">Moment Generating Function</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0601_moment_generating_and_characteristic_functions/moment_generating_function_application_sum_of_rv.html">Application: Moment Generating Function and the Sum of Random Variables</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0601_moment_generating_and_characteristic_functions/characteristic_function.html">Characteristic Function</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0602_probability_inequalities/intro.html">Probability Inequalities</a><input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-20"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0602_probability_inequalities/concept.html">Probability Inequalities</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0602_probability_inequalities/application.html">Application: Learning Theory</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0603_law_of_large_numbers/intro.html">Law of Large Numbers</a><input class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-21"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0603_law_of_large_numbers/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0603_law_of_large_numbers/convergence.html">Convergence of Sample Average</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0603_law_of_large_numbers/application.html">Application: Learning Theory</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Optimization</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="intro.html">Gradient Descent</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-22"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Gradient Descent Concept</a></li>
<li class="toctree-l2"><a class="reference internal" href="implementation.html">Gradient Descent Construction</a></li>
<li class="toctree-l2"><a class="reference internal" href="application.html">Application: Gradient Descent</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Machine Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../machine_learning/fundamentals/intro.html">Fundamentals</a><input class="toctree-checkbox" id="toctree-checkbox-23" name="toctree-checkbox-23" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-23"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../machine_learning/fundamentals/criterions/intro.html">Loss</a><input class="toctree-checkbox" id="toctree-checkbox-24" name="toctree-checkbox-24" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-24"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../machine_learning/fundamentals/criterions/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../machine_learning/fundamentals/criterions/cross_entropy_loss.html">Cross Entropy Loss</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../machine_learning/fundamentals/criterions/focal_loss.html">Focal Loss</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../machine_learning/fundamentals/empirical_risk_minimization/intro.html">Empirical Risk Minimization</a><input class="toctree-checkbox" id="toctree-checkbox-25" name="toctree-checkbox-25" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-25"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../machine_learning/fundamentals/empirical_risk_minimization/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../machine_learning/fundamentals/empirical_risk_minimization/bayes_optimal_classifier.html">Bayes Optimal Classifier</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../machine_learning/fundamentals/learning_theory/intro.html">Is the Learning Problem Solvable?</a><input class="toctree-checkbox" id="toctree-checkbox-26" name="toctree-checkbox-26" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-26"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../machine_learning/fundamentals/learning_theory/concept.html">Concept</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../machine_learning/fundamentals/bias_and_variance/intro.html">Bias and Variance Tradeoff</a><input class="toctree-checkbox" id="toctree-checkbox-27" name="toctree-checkbox-27" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-27"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../machine_learning/fundamentals/bias_and_variance/concept.html">Bias-Variance Tradeoff Concept</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../machine_learning/fundamentals/decision_boundary/intro.html">Decision Boundary</a><input class="toctree-checkbox" id="toctree-checkbox-28" name="toctree-checkbox-28" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-28"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../machine_learning/fundamentals/decision_boundary/concept.html">Concept</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../machine_learning/fundamentals/voronoi_region/intro.html">Voronoi Region</a><input class="toctree-checkbox" id="toctree-checkbox-29" name="toctree-checkbox-29" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-29"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../machine_learning/fundamentals/voronoi_region/concept.html">Concept</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../machine_learning/linear_models/intro.html">Linear Models</a><input class="toctree-checkbox" id="toctree-checkbox-30" name="toctree-checkbox-30" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-30"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../machine_learning/linear_models/linear_regression/intro.html">Linear Regression</a><input class="toctree-checkbox" id="toctree-checkbox-31" name="toctree-checkbox-31" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-31"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../machine_learning/linear_models/linear_regression/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../machine_learning/linear_models/linear_regression/implementation.html">Implementation</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../machine_learning/linear_models/logistic_regression/intro.html">Logistic Regression</a><input class="toctree-checkbox" id="toctree-checkbox-32" name="toctree-checkbox-32" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-32"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../machine_learning/linear_models/logistic_regression/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../machine_learning/linear_models/logistic_regression/implementation.html">Implementation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../machine_learning/linear_models/generalized_linear_models/intro.html">Generalized Linear Models</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../machine_learning/generative/intro.html">Generative</a><input class="toctree-checkbox" id="toctree-checkbox-33" name="toctree-checkbox-33" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-33"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../machine_learning/generative/naive_bayes/intro.html">Naive Bayes</a><input class="toctree-checkbox" id="toctree-checkbox-34" name="toctree-checkbox-34" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-34"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../machine_learning/generative/naive_bayes/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../machine_learning/generative/naive_bayes/implementation.html">Naives Bayes Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../machine_learning/generative/naive_bayes/example_penguins.html">Naive Bayes Application: Penguins</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../machine_learning/generative/naive_bayes/application_mnist.html">Naive Bayes Application (MNIST)</a></li>

</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../machine_learning/model_selection_and_evaluation/intro.html">Model Selection and Evaluation</a><input class="toctree-checkbox" id="toctree-checkbox-35" name="toctree-checkbox-35" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-35"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../machine_learning/model_selection_and_evaluation/metrics/intro.html">Metrics and Scoring Rules</a><input class="toctree-checkbox" id="toctree-checkbox-36" name="toctree-checkbox-36" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-36"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../machine_learning/model_selection_and_evaluation/metrics/classification/intro.html">Classification Metrics</a><input class="toctree-checkbox" id="toctree-checkbox-37" name="toctree-checkbox-37" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-37"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../machine_learning/model_selection_and_evaluation/metrics/classification/accuracy.html">Accuracy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../machine_learning/model_selection_and_evaluation/metrics/classification/precision_recall_f1.html">Precision, Recall and F1 Score</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../machine_learning/model_selection_and_evaluation/metrics/classification/brier_score.html">Brier Score</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../machine_learning/model_selection_and_evaluation/metrics/regression/intro.html">Regression Metrics</a><input class="toctree-checkbox" id="toctree-checkbox-38" name="toctree-checkbox-38" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-38"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../machine_learning/model_selection_and_evaluation/metrics/regression/mae.html">Mean Absolute Error</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../machine_learning/model_selection_and_evaluation/metrics/regression/rmse.html">(Root) Mean Squared Error</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../machine_learning/model_selection_and_evaluation/metrics/regression/mape.html">Mean Absolute Percentage Error</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../machine_learning/trees/intro.html">Trees, Forests, Bagging and Boosting</a><input class="toctree-checkbox" id="toctree-checkbox-39" name="toctree-checkbox-39" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-39"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../machine_learning/trees/decision_trees/intro.html">Decision Trees</a><input class="toctree-checkbox" id="toctree-checkbox-40" name="toctree-checkbox-40" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-40"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../machine_learning/trees/decision_trees/concept.html">Braindump</a></li>





</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../machine_learning/trees/ensemble_learning/intro.html">Ensemble Learning</a><input class="toctree-checkbox" id="toctree-checkbox-41" name="toctree-checkbox-41" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-41"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../machine_learning/trees/ensemble_learning/bagging/intro.html">Bagging</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../machine_learning/trees/ensemble_learning/random_forests/intro.html">Random Forests</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../machine_learning/trees/ensemble_learning/boosting/intro.html">Boosting</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../machine_learning/decomposition/intro.html">Dimensionality Reduction</a><input class="toctree-checkbox" id="toctree-checkbox-42" name="toctree-checkbox-42" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-42"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../machine_learning/decomposition/pca/intro.html">Principal Component Analysis</a><input class="toctree-checkbox" id="toctree-checkbox-43" name="toctree-checkbox-43" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-43"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../machine_learning/decomposition/pca/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../machine_learning/decomposition/pca/implementation.html">PCA</a></li>

<li class="toctree-l3"><a class="reference internal" href="../../machine_learning/decomposition/pca/eigenface.html">Eigenface</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../machine_learning/neighbours/intro.html">Neighbourhood</a><input class="toctree-checkbox" id="toctree-checkbox-44" name="toctree-checkbox-44" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-44"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../machine_learning/neighbours/k_nearest_neighbours/intro.html">K-Nearest Neighbours</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../machine_learning/mixtures/intro.html">Mixture Models</a><input class="toctree-checkbox" id="toctree-checkbox-45" name="toctree-checkbox-45" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-45"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../machine_learning/mixtures/gmm/intro.html">Gaussian Mixture Models</a><input class="toctree-checkbox" id="toctree-checkbox-46" name="toctree-checkbox-46" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-46"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../machine_learning/mixtures/gmm/concept.html">Concept</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../machine_learning/clustering/intro.html">Clustering</a><input class="toctree-checkbox" id="toctree-checkbox-47" name="toctree-checkbox-47" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-47"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../machine_learning/clustering/kmeans/intro.html">K-Means</a><input class="toctree-checkbox" id="toctree-checkbox-48" name="toctree-checkbox-48" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-48"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../machine_learning/clustering/kmeans/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../machine_learning/clustering/kmeans/implementation.html">Implementation: K-Means (Lloyd)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../machine_learning/clustering/kmeans/image_segmentation.html">Application: Image Compression and Segmentation</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Deep Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../deep_learning/natural_language_processing/intro.html">Natural Language Processing (NLP)</a><input class="toctree-checkbox" id="toctree-checkbox-49" name="toctree-checkbox-49" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-49"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../deep_learning/natural_language_processing/vector_semantics_and_embeddings/intro.html">Vector Semantics and Embeddings</a><input class="toctree-checkbox" id="toctree-checkbox-50" name="toctree-checkbox-50" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-50"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../deep_learning/natural_language_processing/vector_semantics_and_embeddings/words_and_vectors/intro.html">Words and Vectors</a><input class="toctree-checkbox" id="toctree-checkbox-51" name="toctree-checkbox-51" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-51"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../deep_learning/natural_language_processing/vector_semantics_and_embeddings/words_and_vectors/concept.html">Concept</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../deep_learning/natural_language_processing/vector_semantics_and_embeddings/cosine_similarity/intro.html">Cosine Similarity and Notion of Closeness</a><input class="toctree-checkbox" id="toctree-checkbox-52" name="toctree-checkbox-52" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-52"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../deep_learning/natural_language_processing/vector_semantics_and_embeddings/cosine_similarity/concept.html">Concept</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../deep_learning/natural_language_processing/vector_semantics_and_embeddings/cosine_similarity/implementation.html">Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../deep_learning/natural_language_processing/vector_semantics_and_embeddings/cosine_similarity/word_similarity.html">Application: Word Similarity</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../deep_learning/natural_language_processing/vector_semantics_and_embeddings/tf_idf/intro.html">Term Frequency-Inverse Document Frequency (TF-IDF)</a><input class="toctree-checkbox" id="toctree-checkbox-53" name="toctree-checkbox-53" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-53"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../deep_learning/natural_language_processing/vector_semantics_and_embeddings/tf_idf/concept.html">Concept</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../deep_learning/natural_language_processing/vector_semantics_and_embeddings/tf_idf/implementation.html">Implementation</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">References, Resources and Roadmap</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../references_resources_roadmap/bibliography.html">Bibliography</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../references_resources_roadmap/resources.html">Resources</a></li>
</ul>

    </div>
</nav>
    </div>
  </div>
  

  
  <div class="sidebar-end-items sidebar-primary__section">
    <div class="sidebar-end-items__item">
    </div>
  </div>

  
  <div id="rtd-footer-container"></div>

      </div>
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

        <div class="bd-content">
          <div class="bd-article-container">
            
            <div class="bd-header-article">
                



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        <label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" data-toggle="tooltip" data-placement="right" title="Toggle primary sidebar">
            <span class="fa-solid fa-bars"></span>
        </label>
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="btn btn-sm"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<div class="dropdown dropdown-repository-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      <li><a href="https://github.com/gao-hongnan/gaohn-galaxy" target="_blank"
   class="btn btn-sm dropdown-item"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">repository</span>
</a>
</a>
      
      <li><a href="https://github.com/gao-hongnan/gaohn-galaxy/issues/new?title=Issue%20on%20page%20%2Foptimization/gradient_descent/concept.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm dropdown-item"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">open issue</span>
</a>
</a>
      
  </ul>
</div>



<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      <li><a href="../../_sources/optimization/gradient_descent/concept.md" target="_blank"
   class="btn btn-sm dropdown-item"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</a>
      
      <li>
<button onclick="printPdf(this)"
  class="btn btn-sm dropdown-item"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</a>
      
  </ul>
</div>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary" data-toggle="tooltip" data-placement="left" title="Toggle secondary sidebar">
            <span class="fa-solid fa-list"></span>
        </label>
    </div>
</div>
            </div>
            
            

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Gradient Descent Concept</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-intuition">
   The Intuition
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-idea-of-neighbourhood">
   The idea of neighbourhood
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gradient-vector">
   Gradient Vector
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#directional-derivatives">
   Directional Derivatives
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#intuition-of-directional-derivative">
     Intuition of Directional Derivative
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#theorem-the-direction-derivative">
     Theorem (The Direction Derivative)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#difference-between-gradient-and-directional-derivative">
     Difference between Gradient and Directional Derivative
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gradient-points-to-the-direction-of-steepest-ascent">
   Gradient Points to the Direction of Steepest Ascent
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#further-readings">
   Further Readings
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>

            <article class="bd-article" role="main">
              
  <div class="math notranslate nohighlight">
\[
\newcommand{\F}{\mathbb{F}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\u}{\mathbf{u}}
\newcommand{\v}{\mathbf{v}}
\newcommand{\x}{\mathbf{x}}
\]</div>
<section class="tex2jax_ignore mathjax_ignore" id="gradient-descent-concept">
<h1>Gradient Descent Concept<a class="headerlink" href="#gradient-descent-concept" title="Permalink to this heading">#</a></h1>
<p>In mathematics, <strong>gradient descent</strong> (also often called steepest descent) is a <strong>first-order iterative optimization algorithm</strong> for finding a local minimum of a differentiable function. The idea is to take repeated steps in the opposite direction of the gradient (or approximate gradient) of the function at the current point, because this is the direction of steepest descent. Conversely, stepping in the direction of the gradient will lead to a local maximum of that function; the procedure is then known as gradient ascent.<a class="footnote-reference brackets" href="#gradient-descent-intro" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a></p>
<p>This blog answers two questions:</p>
<ol class="arabic simple">
<li><p>How does the gradient descent algorithm work?</p></li>
<li><p>Why does the gradient points in the direction of the steepest ascent?</p></li>
</ol>
<p>In many articles and courses, gradient descent is often used to find the minimum of a function, by way of the following procedure:
<span class="math notranslate nohighlight">\({\color{red} \textbf{To rewrite this pseudocode}}\)</span></p>
<div class="proof algorithm admonition" id="gd-algo">
<p class="admonition-title"><span class="caption-number">Algorithm 1 </span> (Gradient Descent)</p>
<section class="algorithm-content" id="proof-content">
<ol class="arabic simple">
<li><p>Start at the initial point.</p></li>
<li><p>Take a step in the opposite direction of the gradient.</p></li>
<li><p>Repeat steps 2 and 3 until the function is no longer decreasing.</p></li>
<li><p>The point where the function is no longer decreasing is the minimum.</p></li>
</ol>
</section>
</div><p>Our second question is equivalent to answering the point 2, on why do we take a step in the opposite direction of the gradient.</p>
<section id="the-intuition">
<h2>The Intuition<a class="headerlink" href="#the-intuition" title="Permalink to this heading">#</a></h2>
<p>Quote wikipedia’s example.</p>
</section>
<section id="the-idea-of-neighbourhood">
<span id="gradient-descent-concept-md-the-idea-of-neighbourhood"></span><h2>The idea of neighbourhood<a class="headerlink" href="#the-idea-of-neighbourhood" title="Permalink to this heading">#</a></h2>
<p>Let us start with an one-dimensional example to illustrate the idea of neighbourhood.</p>
<p>Define <span class="math notranslate nohighlight">\(f: \R \to \R\)</span> to be <span class="math notranslate nohighlight">\(f(x) = x^2\)</span>, then the gradient of <span class="math notranslate nohighlight">\(f\)</span> at a point <span class="math notranslate nohighlight">\(x = x_0\)</span> is <span class="math notranslate nohighlight">\(\frac {\partial f(x)} {\partial x} \vert x_0 = 2x_0\)</span>.</p>
<p>A wrong interpretation is to treat the understanding of gradient as if it is a linear function.</p>
<p>Let us fix a point <span class="math notranslate nohighlight">\(x = 2\)</span>, its output <span class="math notranslate nohighlight">\(f(x) = 4\)</span> and the gradient at that point is <span class="math notranslate nohighlight">\(\frac {\partial f(x)} {\partial x} \vert_{x=2} = 4\)</span>.
It is wrong to say that for every <span class="math notranslate nohighlight">\(1\)</span> unit increase of <span class="math notranslate nohighlight">\(x\)</span>, <span class="math notranslate nohighlight">\(f(x)\)</span> will increase by <span class="math notranslate nohighlight">\(4\)</span>, where <span class="math notranslate nohighlight">\(4\)</span> is the gradient at that point.
One can indeed verify that if <span class="math notranslate nohighlight">\(x\)</span> is increased by <span class="math notranslate nohighlight">\(1\)</span> unit from <span class="math notranslate nohighlight">\(x=2\)</span> to <span class="math notranslate nohighlight">\(x = 3\)</span>, then <span class="math notranslate nohighlight">\(f(x) = 3^2 = 9\)</span>, where <span class="math notranslate nohighlight">\(f(x)\)</span> is increased by <span class="math notranslate nohighlight">\(5\)</span> units, and not by <span class="math notranslate nohighlight">\(4\)</span>.</p>
<p>Unlike the case of a linear function, the gradient of <span class="math notranslate nohighlight">\(f\)</span> at a point <span class="math notranslate nohighlight">\(x = x_0\)</span> is not fixed, but depends on the value of <span class="math notranslate nohighlight">\(x_0\)</span>.
Consequently, the meaning of the gradient in such a function is only well defined in a small <span class="math notranslate nohighlight">\(\epsilon\)</span>-neighbourhood<a class="footnote-reference brackets" href="#epsilon" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a> around <span class="math notranslate nohighlight">\(x = 2\)</span>.
Within this small neighbourhood, we can “loosely” visualize the portion of the function to be a “linear function” with a slope of <span class="math notranslate nohighlight">\(2\)</span>, as illustrated in the following figure.
The enclosed green box is the neighborhood of <span class="math notranslate nohighlight">\(x = 2\)</span>, where if we zoom in, the portion of the function inside looks like a linear function with a slope of <span class="math notranslate nohighlight">\(2\)</span>.</p>
<figure class="align-default" id="neighbourhood">
<a class="reference internal image-reference" href="../../_images/neighbourhood.jpg"><img alt="../../_images/neighbourhood.jpg" src="../../_images/neighbourhood.jpg" style="width: 300px; height: 300px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 15 </span><span class="caption-text">Neighbourhood around x = 2.</span><a class="headerlink" href="#neighbourhood" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Indeed, if we set <span class="math notranslate nohighlight">\(\epsilon = 0.001\)</span>, then if <span class="math notranslate nohighlight">\(x = 2\)</span>, we have <span class="math notranslate nohighlight">\(f(x) = 4\)</span>; moving <span class="math notranslate nohighlight">\(x\)</span> by <span class="math notranslate nohighlight">\(\epsilon\)</span> from <span class="math notranslate nohighlight">\(x = 2\)</span> to <span class="math notranslate nohighlight">\(x = 2.001\)</span>
yields us <span class="math notranslate nohighlight">\(f(x) \approxeq 4.004\)</span>. We now see that <span class="math notranslate nohighlight">\(x\)</span> increasing by <span class="math notranslate nohighlight">\(0.001\)</span> unit indeed yields us an increase of <span class="math notranslate nohighlight">\(4.004 - 4 = 0.004\)</span>,
a factor of <span class="math notranslate nohighlight">\(4\)</span> increase to the output <span class="math notranslate nohighlight">\(f(x)\)</span>.</p>
</section>
<section id="gradient-vector">
<h2>Gradient Vector<a class="headerlink" href="#gradient-vector" title="Permalink to this heading">#</a></h2>
<div class="proof definition admonition" id="grad_vec">
<p class="admonition-title"><span class="caption-number">Definition 103 </span> (Gradient Vector)</p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(f: \R^n \to \R\)</span> be a function that maps <span class="math notranslate nohighlight">\(\x\)</span> to <span class="math notranslate nohighlight">\(f(\x)\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
f \colon \R^n &amp;\longrightarrow \R \\
\x &amp;\longmapsto f(\x) \\
\end{align*}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\x = [x_1, x_2, \ldots, x_n]^\top\)</span> is a vector of <span class="math notranslate nohighlight">\(n\)</span> variables.</p>
<p>Then the <em><strong>gradient</strong></em> of <span class="math notranslate nohighlight">\(f\)</span> is the <strong>vector</strong> of first-order partial derivatives of <span class="math notranslate nohighlight">\(f\)</span> with respect to each of the <span class="math notranslate nohighlight">\(n\)</span> variables.</p>
<div class="math notranslate nohighlight" id="equation-grad-vec">
<span class="eqno">(51)<a class="headerlink" href="#equation-grad-vec" title="Permalink to this equation">#</a></span>\[
\begin{equation}
\nabla f(\x) = \bigg[\frac{\partial f(\x)}{\partial x_1}, \frac{\partial f(\x)}{\partial x_2}, \ldots, \frac{\partial f(\x)}{\partial x_n} \bigg]^{\top}
\end{equation}
\]</div>
<p>We can also replace the notation <span class="math notranslate nohighlight">\(\frac{\partial f(\x)}{\partial x_i}\)</span> in equation <a class="reference internal" href="#equation-grad-vec">(51)</a> with <span class="math notranslate nohighlight">\(f_{x_i}\)</span>.</p>
</section>
</div></section>
<section id="directional-derivatives">
<h2>Directional Derivatives<a class="headerlink" href="#directional-derivatives" title="Permalink to this heading">#</a></h2>
<div class="proof definition admonition" id="directional_der">
<p class="admonition-title"><span class="caption-number">Definition 104 </span> (Directional Derivative)</p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(f: \R^n \to \R\)</span> be a function that maps <span class="math notranslate nohighlight">\(\x\)</span> to <span class="math notranslate nohighlight">\(f(\x)\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
f \colon \R^n &amp;\longrightarrow \R \\
\x &amp;\longmapsto f(\x) \\
\end{align*}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\x = [x_1, x_2, \ldots, x_n]^\top\)</span> is a vector of <span class="math notranslate nohighlight">\(n\)</span> variables.</p>
<p>Then the <em><strong>directional derivative</strong></em> of <span class="math notranslate nohighlight">\(f\)</span> <strong>at a point</strong> <span class="math notranslate nohighlight">\(\x\)</span> along a <strong>direction vector</strong></p>
<div class="math notranslate nohighlight">
\[
\v = [v_1, v_2, \ldots, v_n]^\top
\]</div>
<p>is the function <span class="math notranslate nohighlight">\(D_{\v}(f)\)</span> defined by the limit:</p>
<div class="math notranslate nohighlight" id="equation-directional-derivative-def">
<span class="eqno">(52)<a class="headerlink" href="#equation-directional-derivative-def" title="Permalink to this equation">#</a></span>\[
\begin{equation}
D_{\v}(f) = \lim_{h \to 0} \frac{f(\x + h\v) - f(\x)}{h}
\end{equation}
\]</div>
</section>
</div><p>To avoid notation confusion and also understand the definition better, we use a simple example in 2-dimensional to illustrate the definition of the directional derivative.</p>
<div class="proof example admonition" id="directional_derivative_example">
<p class="admonition-title"><span class="caption-number">Example 20 </span> (Directional Derivative)</p>
<section class="example-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(f: \R^2 \to \R\)</span> be defined as</p>
<div class="math notranslate nohighlight">
\[
f(\x) = f(x_1, x_2) = x_1^2 + x_2^2
\]</div>
<p>where <span class="math notranslate nohighlight">\(\x\)</span> is a vector of scalar values <span class="math notranslate nohighlight">\(x_1\)</span> and <span class="math notranslate nohighlight">\(x_2\)</span>, both corresponds to the x- and y-coordinates of a point in the plane respectively.</p>
<p>Following closely the definition, we need to define a <strong>direction vector</strong> <span class="math notranslate nohighlight">\(\v\)</span>.
Since the definition did not say what <span class="math notranslate nohighlight">\(\v\)</span> was, we can define <span class="math notranslate nohighlight">\(\v\)</span> to be the unit vector in the south-east direction.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\v = \begin{bmatrix} 1 \\ -1 \end{bmatrix}
\end{split}\]</div>
<p>We also note that we want to compute the <strong>directional derivative</strong> of <span class="math notranslate nohighlight">\(f\)</span> at a point <span class="math notranslate nohighlight">\((x_1, x_2)\)</span>, where we arbitrarily choose <span class="math notranslate nohighlight">\((x_1, x_2) = \begin{bmatrix} 1 \\ 1 \end{bmatrix}\)</span>.</p>
<p>Then, we can define the directional derivative of <span class="math notranslate nohighlight">\(f\)</span> at the point <span class="math notranslate nohighlight">\(\x = \begin{bmatrix} 1 \\ 1 \end{bmatrix}\)</span> along <span class="math notranslate nohighlight">\(\v = \begin{bmatrix} 1 \\ -1 \end{bmatrix}\)</span> as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{equation*}
\begin{split}
D_{\v}(f(1, 1)) &amp;= \lim_{h \to 0} \frac{f(\x + h\v) - f(\x)}{h} \\
          &amp;= \lim_{h \to 0} \frac{f(x_1 + hv_1, x_2 + hv_2) - f(x_1, x_2)}{h} \\
          &amp;= \lim_{h \to 0} \frac{f(1 + h, 1 - h) - f(1, 1)}{h}
\end{split}
\end{equation*}
\end{split}\]</div>
<p>which evaluates to how much <span class="math notranslate nohighlight">\(f\)</span> changes when it moves a small unit distance <span class="math notranslate nohighlight">\(h\)</span> from <span class="math notranslate nohighlight">\(\x\)</span> to <span class="math notranslate nohighlight">\(\x + h\)</span> along the direction <span class="math notranslate nohighlight">\(\v\)</span>.</p>
</section>
</div><section id="intuition-of-directional-derivative">
<span id="gradient-descent-concept-md-intuition-of-directional-derivative"></span><h3>Intuition of Directional Derivative<a class="headerlink" href="#intuition-of-directional-derivative" title="Permalink to this heading">#</a></h3>
<p>This section builds up to an important derivation of the idea of the directional derivative.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>In this section, we should constantly recall the idea of a <a class="reference internal" href="#gradient-descent-concept-md-the-idea-of-neighbourhood"><span class="std std-ref">neighbourhood</span></a> whenever we talk about per unit change, we should visualize that this unit is very small.</p>
</div>
<p>Let us restrict our focus to 2-variable mutlivariate function <span class="math notranslate nohighlight">\(f(x, y)\)</span>, bearing in mind that it can be scaled up to <span class="math notranslate nohighlight">\(n\)</span> variables.
The components of <span class="math notranslate nohighlight">\(\nabla f\)</span> are the partial derivatives of <span class="math notranslate nohighlight">\(f\)</span> with respect to <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>.</p>
<p>More concretely, given the partial derivative</p>
<div class="math notranslate nohighlight" id="equation-partial-x">
<span class="eqno">(53)<a class="headerlink" href="#equation-partial-x" title="Permalink to this equation">#</a></span>\[
\begin{equation}
\frac{\partial f(x, y)}{\partial x}
\end{equation}
\]</div>
<p>equation <a class="reference internal" href="#equation-partial-x">(53)</a> answers the question:
how much does the value of <span class="math notranslate nohighlight">\(f\)</span> change when we hold <span class="math notranslate nohighlight">\(y\)</span> constant and nudge <span class="math notranslate nohighlight">\(x\)</span> by a small amount in the <span class="math notranslate nohighlight">\(x\)</span> direction (i.e. in the direction pointed to by the vector <span class="math notranslate nohighlight">\(\begin{bmatrix} 1 &amp; 0 \end{bmatrix}^\top\)</span>) <span id="id3">[<a class="reference internal" href="../../references_resources_roadmap/bibliography.html#id13" title="Sootla Sten. URL: https://sootlasten.github.io/2017/gradient-steepest-ascent/.">Sten, n.d.</a>]</span>.</p>
<p>In a similar vein, given the partial derivative</p>
<div class="math notranslate nohighlight" id="equation-partial-y">
<span class="eqno">(54)<a class="headerlink" href="#equation-partial-y" title="Permalink to this equation">#</a></span>\[
\begin{equation}
\frac{\partial f(x, y)}{\partial y}
\end{equation}
\]</div>
<p>equation <a class="reference internal" href="#equation-partial-y">(54)</a> answers the question:
how much does the value of <span class="math notranslate nohighlight">\(f\)</span> change when we hold <span class="math notranslate nohighlight">\(x\)</span> constant and nudge <span class="math notranslate nohighlight">\(y\)</span> by a small amount in the <span class="math notranslate nohighlight">\(y\)</span> direction (i.e. in the direction pointed to by the vector <span class="math notranslate nohighlight">\(\begin{bmatrix} 0 &amp; 1 \end{bmatrix}^\top\)</span>) <span id="id4">[<a class="reference internal" href="../../references_resources_roadmap/bibliography.html#id13" title="Sootla Sten. URL: https://sootlasten.github.io/2017/gradient-steepest-ascent/.">Sten, n.d.</a>]</span>.</p>
<p>Finally, it is often useful to note that <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> can move in tandem (i.e. we do not hold any of them constant), how do we then calculate how much <span class="math notranslate nohighlight">\(f\)</span> changes when we nudge both <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> by a small amount.
Note that moving <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> both by <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span> respectively is synonymous with moving the point <span class="math notranslate nohighlight">\((x, y)\)</span> by the vector <span class="math notranslate nohighlight">\(\begin{bmatrix} a &amp; b \end{bmatrix}^\top\)</span>. With vector in the playing field, we now attach the notion of a “direction”.</p>
<p>We can use equation <a class="reference internal" href="#equation-directional-derivative-def">(52)</a> to compute how much <span class="math notranslate nohighlight">\(f\)</span> changes when we nudge the points <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> by a small amount in the <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> directions respectively.
However, the calculation is cumbersome if we use the definition of the directional derivative.
We will now use an example to derive an <em>alternate</em> formula by relating directional derivative to their partial derivatives composition.</p>
<div class="proof example admonition" id="example-4">
<p class="admonition-title"><span class="caption-number">Example 21 </span> (Directional Derivative)</p>
<section class="example-content" id="proof-content">
<p>If we want to move <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> by <span class="math notranslate nohighlight">\(1\)</span> and <span class="math notranslate nohighlight">\(-1\)</span> respectively, then it simply means that <span class="math notranslate nohighlight">\(x\)</span> is moved 1 unit along the <span class="math notranslate nohighlight">\(x\)</span> axis
and <span class="math notranslate nohighlight">\(y\)</span> is moved -1 unit along the <span class="math notranslate nohighlight">\(y\)</span> axis.</p>
<p>In vector terms, this means we moved <span class="math notranslate nohighlight">\((x, y)\)</span> in the direction <span class="math notranslate nohighlight">\(\begin{bmatrix} 1 &amp; -1 \end{bmatrix}^\top\)</span>.</p>
<p>More generically, let <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> move <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span> units respectively, then we moved <span class="math notranslate nohighlight">\((x, y)\)</span> in the direction <span class="math notranslate nohighlight">\(\begin{bmatrix} a &amp; b \end{bmatrix}^\top\)</span>.
It turns out that the amount that <span class="math notranslate nohighlight">\(f\)</span> changes when we move in the <span class="math notranslate nohighlight">\(\v = \begin{bmatrix} a &amp; b \end{bmatrix}^\top\)</span> direction is exactly how much
<span class="math notranslate nohighlight">\(f\)</span> changes when we move <span class="math notranslate nohighlight">\(a\)</span> units along the <span class="math notranslate nohighlight">\(x\)</span> axis and <span class="math notranslate nohighlight">\(b\)</span> units along the <span class="math notranslate nohighlight">\(y\)</span> axis <span id="id5">[<a class="reference internal" href="../../references_resources_roadmap/bibliography.html#id13" title="Sootla Sten. URL: https://sootlasten.github.io/2017/gradient-steepest-ascent/.">Sten, n.d.</a>]</span>.</p>
<p>Recall that we <strong>know</strong> how much <span class="math notranslate nohighlight">\(f\)</span> changes when we move <span class="math notranslate nohighlight">\(x\)</span> by 1 unit:</p>
<ul class="simple">
<li><p>The partial derivative of <span class="math notranslate nohighlight">\(f\)</span> with respect to <span class="math notranslate nohighlight">\(x\)</span>: <span class="math notranslate nohighlight">\(\frac{\partial f(x, y)}{\partial x}\)</span></p></li>
</ul>
<p>Recall that we <strong>know</strong> how much <span class="math notranslate nohighlight">\(f\)</span> changes when we move <span class="math notranslate nohighlight">\(y\)</span> by 1 unit:</p>
<ul class="simple">
<li><p>The partial derivative of <span class="math notranslate nohighlight">\(f\)</span> with respect to <span class="math notranslate nohighlight">\(y\)</span>: <span class="math notranslate nohighlight">\(\frac{\partial f(x, y)}{\partial y}\)</span></p></li>
</ul>
<p>It follows that if we move <span class="math notranslate nohighlight">\(x\)</span> by <span class="math notranslate nohighlight">\(a\)</span> units, then <span class="math notranslate nohighlight">\(f\)</span> changes by</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(a \cdot \frac{\partial f(x, y)}{\partial x}\)</span></p></li>
</ul>
<p>and if we move <span class="math notranslate nohighlight">\(y\)</span> by <span class="math notranslate nohighlight">\(b\)</span> units, then <span class="math notranslate nohighlight">\(f\)</span> changes by</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(b \cdot \frac{\partial f(x, y)}{\partial y}\)</span></p></li>
</ul>
<p>Therefore, the amount that <span class="math notranslate nohighlight">\(f\)</span> changes when we move <span class="math notranslate nohighlight">\(x\)</span> by <span class="math notranslate nohighlight">\(a\)</span> units and <span class="math notranslate nohighlight">\(y\)</span> by <span class="math notranslate nohighlight">\(b\)</span> units is</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(a \cdot \frac{\partial f(x, y)}{\partial x} + b \cdot \frac{\partial f(x, y)}{\partial y}\)</span></p></li>
</ul>
<p>One might notice that for multivariate <span class="math notranslate nohighlight">\(f:\R^n \to \R\)</span>, the derivative <span class="math notranslate nohighlight">\(D_{\v}(f)\)</span> at the point <span class="math notranslate nohighlight">\(x \in \R^n\)</span> is defined to be a linear map <span class="math notranslate nohighlight">\(T: \R^n \to \R\)</span>.
Therefore, the linearity rule applies.</p>
<p>If the above is still not obvious, then we can approach it geometrically.
Recall that we are mapping from <span class="math notranslate nohighlight">\(\R^2\)</span> to <span class="math notranslate nohighlight">\(\R\)</span>, the below diagram illustrates the mapping.</p>
<figure class="align-default" id="directive-fig">
<a class="reference internal image-reference" href="../../_images/geometric_r2_r1.jpg"><img alt="../../_images/geometric_r2_r1.jpg" src="../../_images/geometric_r2_r1.jpg" style="width: 300px; height: 300px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 16 </span><span class="caption-text">Geometric Intuition</span><a class="headerlink" href="#directive-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
</div></section>
<section id="theorem-the-direction-derivative">
<span id="gradient-descent-concept-md-theorem-the-direction-derivative"></span><h3>Theorem (The Direction Derivative)<a class="headerlink" href="#theorem-the-direction-derivative" title="Permalink to this heading">#</a></h3>
<p>The intuition <a class="reference internal" href="#gradient-descent-concept-md-intuition-of-directional-derivative"><span class="std std-ref">developed in the previous section</span></a> allows us to
derive a new formula to calculate the directional derivative of <span class="math notranslate nohighlight">\(f\)</span> at a point <span class="math notranslate nohighlight">\(\x\)</span> when moved in the direction of <span class="math notranslate nohighlight">\(\v\)</span>.</p>
<p>Although the example used is in 2-dimensions, we can generalize to <span class="math notranslate nohighlight">\(n\)</span> variables and we state it formally.</p>
<div class="proof theorem admonition" id="dir-deriv-theorem">
<p class="admonition-title"><span class="caption-number">Theorem 57 </span> (Directional Derivative)</p>
<section class="theorem-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(f: \R^n \to \R\)</span> be a function <span class="math notranslate nohighlight">\(f(\x)\)</span> where <span class="math notranslate nohighlight">\(\x = [x_1, x_2, \ldots, x_n]^\top\)</span> is a vector of <span class="math notranslate nohighlight">\(n\)</span> variables.</p>
<p>If <span class="math notranslate nohighlight">\(f\)</span> is <strong>differentiable</strong> at <span class="math notranslate nohighlight">\(\x\)</span>, then the directional derivative <span class="math notranslate nohighlight">\(D_{\v}(f)\)</span> at <span class="math notranslate nohighlight">\(\x\)</span> is defined as follows:</p>
<div class="math notranslate nohighlight" id="equation-directional-derivative-theorem">
<span class="eqno">(55)<a class="headerlink" href="#equation-directional-derivative-theorem" title="Permalink to this equation">#</a></span>\[
\begin{equation}
D_{\v}(f(\x)) = \nabla f(\x) \cdot \v
\end{equation}
\]</div>
<p>In most textbooks, we will <strong>normalize</strong> the direction vector <span class="math notranslate nohighlight">\(\v\)</span> to the standard unit vector <span class="math notranslate nohighlight">\(\u\)</span>.
However, the usage of unit vector simplifies the derivation and other applications<a class="footnote-reference brackets" href="#unit-vector-1" id="id6" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a><a class="footnote-reference brackets" href="#unit-vector-2" id="id7" role="doc-noteref"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></a> , it is not an universal rule.</p>
</section>
</div><div class="info admonition">
<p class="admonition-title">Important Note.</p>
<p>It is extremely important to remember that the <strong>gradient vector</strong> of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(\x\)</span> is already defined as <span class="math notranslate nohighlight">\(\nabla f(\x)\)</span>, to be the vector of partial derivatives of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(\x\)</span>.</p>
<p>This has two consequences:</p>
<ol class="arabic simple">
<li><p>Given the gradient vector of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(\x\)</span> and any direction <span class="math notranslate nohighlight">\(\v\)</span>, we can recover the directional derivative <span class="math notranslate nohighlight">\(D_{\v}(f)\)</span> at <span class="math notranslate nohighlight">\(\x\)</span> by simply multiplying it by <span class="math notranslate nohighlight">\(\v\)</span>.</p></li>
<li><p>Given the gradient vector of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(\x\)</span> and a scalar value directional derivative <span class="math notranslate nohighlight">\(D_{\v}(f)\)</span>, we can recover the direction <span class="math notranslate nohighlight">\(\v\)</span> by division.</p></li>
</ol>
</div>
</section>
<section id="difference-between-gradient-and-directional-derivative">
<h3>Difference between Gradient and Directional Derivative<a class="footnote-reference brackets" href="#gradient-directional-derivative-difference" id="id8" role="doc-noteref"><span class="fn-bracket">[</span>5<span class="fn-bracket">]</span></a><a class="headerlink" href="#difference-between-gradient-and-directional-derivative" title="Permalink to this heading">#</a></h3>
<p>When I was learning multivariate calculus, I have had my fair share of trouble with the difference
between the gradient and the directional derivative.
It is easy to confuse the two terms.</p>
<p>Let us go back to the definitions:</p>
<ul class="simple">
<li><p>The <strong>gradient</strong> of a mutlivariate function <span class="math notranslate nohighlight">\(f\)</span> at a point <span class="math notranslate nohighlight">\(\x\)</span> is the <em><strong>vector</strong></em> of partial derivatives of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(\x\)</span>, as defined in equation <a class="reference internal" href="#equation-grad-vec">(51)</a>.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\nabla f(\x) = \bigg[\frac{\partial f(\x)}{\partial x_1}, \frac{\partial f(\x)}{\partial x_2}, \ldots, \frac{\partial f(\x)}{\partial x_n} \bigg]^\top
\]</div>
<ul class="simple">
<li><p>The <strong>directional derivative</strong> of a mutlivariate function <span class="math notranslate nohighlight">\(f\)</span> at a point <span class="math notranslate nohighlight">\(\x\)</span> is the <em><strong>scalar</strong></em> value of the partial derivative of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(\x\)</span> in the direction of <span class="math notranslate nohighlight">\(\v\)</span>,
as defined in equation <a class="reference internal" href="#equation-directional-derivative-def">(52)</a> and <a class="reference internal" href="#equation-directional-derivative-theorem">(55)</a>.</p></li>
</ul>
<p>There are infinite directional derivatives around a point <span class="math notranslate nohighlight">\(\x\)</span> since there are infinitely many directions <span class="math notranslate nohighlight">\(\v\)</span>.
However, there is only one gradient vector around a point <span class="math notranslate nohighlight">\(\x\)</span> and this is defined to be the vector that is the steepest ascent, which we will prove in the next section.</p>
<p>Consider the following example:</p>
<ul class="simple">
<li><p>We have a point <span class="math notranslate nohighlight">\((x, y)\)</span> in <span class="math notranslate nohighlight">\(f\)</span>, where <span class="math notranslate nohighlight">\(f\)</span> is a function of two variables.
Since this function can be graphed in 3-dimensions, we can visualize that there are <em><strong>infinite</strong></em> number of directions around the point <span class="math notranslate nohighlight">\((x, y)\)</span>.</p></li>
<li><p>We can categorize the directions around the point as vectors <span class="math notranslate nohighlight">\(\v\)</span>. For simplicity, we restrict <span class="math notranslate nohighlight">\(\v\)</span> to be unit vectors.</p></li>
<li><p>Recall the definition of the directional derivative of <span class="math notranslate nohighlight">\(f\)</span> at a point <span class="math notranslate nohighlight">\(\x\)</span> to be a <strong>scalar-valued</strong> function <span class="math notranslate nohighlight">\(D_{\v}(f)\)</span> parametrized by the direction vector <span class="math notranslate nohighlight">\(\v\)</span>.</p>
<ul>
<li><p>We also note that <span class="math notranslate nohighlight">\(D_{\v}(f)\)</span> is the instantaneous rate of change of <span class="math notranslate nohighlight">\(f\)</span> when we move in the direction <span class="math notranslate nohighlight">\(\v\)</span>.</p></li>
</ul>
</li>
<li><p>There exists <strong>infinite</strong> number of such directional derivatives of <span class="math notranslate nohighlight">\(f\)</span> at a point <span class="math notranslate nohighlight">\(\x\)</span> since there are infinite number of directions <span class="math notranslate nohighlight">\(\v\)</span>. This can be seen by the orange arrows.</p></li>
<li><p>However, there exists an <strong>unique</strong> direction <span class="math notranslate nohighlight">\(\v\)</span> which gives rise to the fastest instantaneous rate of change of <span class="math notranslate nohighlight">\(f\)</span> when moved in that direction.</p></li>
<li><p>This unique direction points in the same direction as the gradient vector <span class="math notranslate nohighlight">\(\v\)</span>.</p></li>
</ul>
<figure class="align-default" id="infinite-directions">
<a class="reference internal image-reference" href="../../_images/infinite_directions.jpg"><img alt="../../_images/infinite_directions.jpg" src="../../_images/infinite_directions.jpg" style="width: 300px; height: 300px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 17 </span><span class="caption-text">Infinite directions around a point in cross-sectional plane of 3d-figure.</span><a class="headerlink" href="#infinite-directions" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
</section>
<section id="gradient-points-to-the-direction-of-steepest-ascent">
<h2>Gradient Points to the Direction of Steepest Ascent<a class="headerlink" href="#gradient-points-to-the-direction-of-steepest-ascent" title="Permalink to this heading">#</a></h2>
<p>We finally have the necessary definitions and theorems to prove the following statement:</p>
<blockquote>
<div><p>The <strong>direction</strong> of steepest ascent of a function <span class="math notranslate nohighlight">\(f: \R^n \to \R\)</span> is given by the <strong>gradient vector</strong> of <span class="math notranslate nohighlight">\(f\)</span>
at a point <span class="math notranslate nohighlight">\(\x = \begin{bmatrix} x_1 &amp; x_2 &amp; \ldots &amp; x_n \end{bmatrix}^\top\)</span>.
In other words, <span class="math notranslate nohighlight">\(f\)</span> increases the fastest when we move in the direction of the <strong>gradient vector</strong>.</p>
</div></blockquote>
<p>Before we prove this, it is best for us to rephrase the question to:</p>
<blockquote>
<div><p>Given a function <span class="math notranslate nohighlight">\(f\)</span> and a point <span class="math notranslate nohighlight">\(\x\)</span>, which direction vector (unit vector) <span class="math notranslate nohighlight">\(\v\)</span> gives the fastest rate of change of <span class="math notranslate nohighlight">\(f\)</span> when moved in that direction?</p>
</div></blockquote>
<p>This can then be found easily by following the theorem/definition in the <a class="reference internal" href="#gradient-descent-concept-md-theorem-the-direction-derivative"><span class="std std-ref">section on the alternative definition of the directional derivative</span></a>.</p>
<p>The logic is that the <strong>directional derivative</strong> of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(\x\)</span> is the <strong>rate of change</strong>, then it suffices to find the <strong>direction vector</strong> <span class="math notranslate nohighlight">\(\v\)</span> that gives the <strong>fastest rate of change</strong>.</p>
<p>The theorem <a class="reference internal" href="#equation-directional-derivative-theorem">(55)</a> states that <span class="math notranslate nohighlight">\(D_{\v}(f(\x)) = \nabla f(\x) \cdot \v\)</span>.
We then seek to solve the optimization problem:</p>
<div class="math notranslate nohighlight" id="equation-optimal-direction-1">
<span class="eqno">(56)<a class="headerlink" href="#equation-optimal-direction-1" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{align}
\v_{\max} &amp;\overset{(a)}{=} \underset{\v, \lVert \v \rVert = 1}{\operatorname{argmax}}D_{\v}(f(\x)) \\
          &amp;\overset{(b)}{=} \underset{\v, \lVert \v \rVert = 1}{\operatorname{argmax}}\nabla f(\x) \cdot \v \\
          &amp;\overset{(c)}{=} \underset{\v, \lVert \v \rVert = 1}{\operatorname{argmax}} \lVert \nabla f(\x) \rVert \cdot \lVert \v \rVert \cos(\theta)\\
\end{align}
\end{split}\]</div>
<p>where we want to find the <strong>unique</strong> <span class="math notranslate nohighlight">\(\v_{\max}\)</span> such that <span class="math notranslate nohighlight">\(D_{\v}(f(\x))\)</span> is the <strong>maximum</strong>.</p>
<p>In <a class="reference internal" href="#equation-optimal-direction-1">(56)</a>, equation (c), we invoked the <a class="reference external" href="https://en.wikipedia.org/wiki/Dot_product#Geometric_definition">geometric definition of the dot product</a>
to represent the dot product as the <strong>cosine of the angle between the two vectors</strong>.</p>
<p>Since we are optimizing over <span class="math notranslate nohighlight">\(\v\)</span> and the length of <span class="math notranslate nohighlight">\(\v\)</span> is 1, we can simplify the expression to:</p>
<div class="math notranslate nohighlight" id="equation-optimal-direction-2">
<span class="eqno">(57)<a class="headerlink" href="#equation-optimal-direction-2" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{align}
\v_{\max} &amp;\overset{(d)}{=}  \underset{\v, \lVert \v \rVert = 1}{\operatorname{argmax}} \lVert \nabla f(\x) \rVert \cdot \lVert \v \rVert \cos(\theta) \\
          &amp;\overset{(e)}{=}  \underset{\v}{\operatorname{argmax}} \lVert \nabla f(\x) \rVert \cos(\theta) \\
          &amp;\overset{(f)}{=}  \underset{\v}{\operatorname{argmax}} \cos(\theta) \\
\end{align}
\end{split}\]</div>
<p>where</p>
<ul class="simple">
<li><p>In <a class="reference internal" href="#equation-optimal-direction-2">(57)</a> equation (e), <span class="math notranslate nohighlight">\(\lVert \v \rVert\)</span> was dropped since it is 1;</p></li>
<li><p>In <a class="reference internal" href="#equation-optimal-direction-2">(57)</a> equation (f), <span class="math notranslate nohighlight">\(\lVert \nabla f(\x) \rVert\)</span> was dropped since it is not a function of <span class="math notranslate nohighlight">\(\v\)</span> and hence it is irrelevant.</p></li>
<li><p><span class="math notranslate nohighlight">\(\theta\)</span>, however, being the angle between <span class="math notranslate nohighlight">\(\v\)</span> and <span class="math notranslate nohighlight">\(\nabla f(\x)\)</span>, is dependent on <span class="math notranslate nohighlight">\(\v\)</span>.</p></li>
<li><p>Therefore, <span class="math notranslate nohighlight">\(\cos(\theta)\)</span> is maximal when <span class="math notranslate nohighlight">\(\cos(\theta) = 1 \implies \theta = 0\)</span>.</p></li>
<li><p>Consequently, <span class="math notranslate nohighlight">\(\theta = 0\)</span> implies <span class="math notranslate nohighlight">\(\nabla f(\x)\)</span> and <span class="math notranslate nohighlight">\(\v\)</span> are parallel.</p></li>
</ul>
<div class="important admonition">
<p class="admonition-title">Success</p>
<p>At this point, we have answered the question posed:</p>
<p>Given a function <span class="math notranslate nohighlight">\(f\)</span> and a point <span class="math notranslate nohighlight">\(\x\)</span>, which direction vector (unit vector) <span class="math notranslate nohighlight">\(\v\)</span> gives the fastest rate of change of <span class="math notranslate nohighlight">\(f\)</span> when moved in that direction?</p>
<p>It turns out this direction/unit vector we are finding is the gradient vector <span class="math notranslate nohighlight">\(\nabla f(\x)\)</span> itself, but reduced to its unit vector since we proved that <span class="math notranslate nohighlight">\(\v \parallel \nabla f(\x)\)</span>.</p>
<p>As a result, we can then say that <span class="math notranslate nohighlight">\(f\)</span> increases the fastest when we move in the direction of the gradient vector <span class="math notranslate nohighlight">\(\nabla f(\x)\)</span>.</p>
<p>It is worth noting that the rate of change of <span class="math notranslate nohighlight">\(f\)</span> at a point <span class="math notranslate nohighlight">\(\x\)</span> in the direction of the gradient vector <span class="math notranslate nohighlight">\(\nabla f(\x)\)</span>
is given by the magnitude of the gradient vector itself <span id="id9">[<a class="reference internal" href="../../references_resources_roadmap/bibliography.html#id13" title="Sootla Sten. URL: https://sootlasten.github.io/2017/gradient-steepest-ascent/.">Sten, n.d.</a>]</span>.</p>
</div>
<p>We finally convinced ourselves that subtracting the gradient vector indeed (local) minimizes the objective/loss function provided it’s differentiable.</p>
</section>
<section id="further-readings">
<h2>Further Readings<a class="headerlink" href="#further-readings" title="Permalink to this heading">#</a></h2>
<p>Gradient Descent is a big topic as it is the heart of many machine learning algorithms.</p>
<p>Being a subset of the broader topic of optimization, there are many resources that
you can read to learn more about gradient descent.</p>
<ul class="simple">
<li><p>Zhang, Aston, Zachary C. Lipton, Mu Li, and Alexander J. Smola. “Chapter 12.3 Gradient Descent.” In Dive into Deep Learning. Berkeley: 2021</p></li>
<li><p><a class="reference external" href="https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/partial-derivative-and-gradient-articles/a/the-gradient">Khan’s Academy: Gradient</a></p></li>
</ul>
<hr class="footnotes docutils" />
<aside class="footnote brackets" id="gradient-descent-intro" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p><strong>“Gradient Descent,” Wikipedia (Wikimedia Foundation, June 28, 2022), <a class="reference external" href="https://en.wikipedia.org/wiki/Gradient_descent">https://en.wikipedia.org/wiki/Gradient_descent</a>.</strong></p>
</aside>
<aside class="footnote brackets" id="epsilon" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">2</a><span class="fn-bracket">]</span></span>
<p><span class="math notranslate nohighlight">\(\epsilon\)</span> is often denoted <span class="math notranslate nohighlight">\(h\)</span> in the limit definition of derivatives.</p>
</aside>
<aside class="footnote brackets" id="unit-vector-1" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">3</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://math.stackexchange.com/questions/1486767/why-in-a-directional-derivative-it-has-to-be-a-unit-vector#:~:text=If%20you%20don't%20use,the%20magnitude%20of%20the%20vector.&amp;text=That%20is%20a%20way%20to,to%20use%20a%20unit%20vector">Why in a directional derivative it has to be a unit vector</a></p>
</aside>
<aside class="footnote brackets" id="unit-vector-2" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id7">4</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://math.stackexchange.com/questions/809376/why-normalize-and-the-definition-of-directional-derivative">why normalize and the definition of directional derivative</a></p>
</aside>
<aside class="footnote brackets" id="gradient-directional-derivative-difference" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id8">5</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://math.stackexchange.com/questions/661195/what-is-the-difference-between-the-gradient-and-the-directional-derivative">What is the difference between the gradient and the directional derivative?</a></p>
</aside>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./optimization/gradient_descent"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

            </article>
            

            
            
            <footer class="bd-footer-article">
                <!-- Previous / next buttons -->
<div class='prev-next-area'>
  <a class='left-prev' id="prev-link" href="intro.html" title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
          <p class="prev-next-subtitle">previous</p>
          <p class="prev-next-title">Gradient Descent</p>
      </div>
  </a>
  <a class='right-next' id="next-link" href="implementation.html" title="next page">
  <div class="prev-next-info">
      <p class="prev-next-subtitle">next</p>
      <p class="prev-next-title">Gradient Descent Construction</p>
  </div>
  <i class="fa-solid fa-angle-right"></i>
  </a>
</div>
            </footer>
            
          </div>
          
          
          
            <div class="bd-sidebar-secondary bd-toc">
              
<div class="toc-item">
  
<div class="tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
</div>
<nav id="bd-toc-nav" class="page-toc">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-intuition">
   The Intuition
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-idea-of-neighbourhood">
   The idea of neighbourhood
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gradient-vector">
   Gradient Vector
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#directional-derivatives">
   Directional Derivatives
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#intuition-of-directional-derivative">
     Intuition of Directional Derivative
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#theorem-the-direction-derivative">
     Theorem (The Direction Derivative)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#difference-between-gradient-and-directional-derivative">
     Difference between Gradient and Directional Derivative
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gradient-points-to-the-direction-of-steepest-ascent">
   Gradient Points to the Direction of Steepest Ascent
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#further-readings">
   Further Readings
  </a>
 </li>
</ul>

</nav>
</div>

            </div>
          
          
        </div>
        <footer class="bd-footer-content">
          <div class="bd-footer-content__inner">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Gao Hongnan
</p>

  </div>
  
  <div class="footer-item">
    
<p class="copyright">

    &copy; Copyright 2023.<br>

</p>

  </div>
  
  <div class="footer-item">
    <p class="last-updated">
Last updated on None.<br>
</p>
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </div>
        </footer>
        

      </main>
    </div>
  </div>

  
    
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=796348d33e8b1d947c94"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=796348d33e8b1d947c94"></script>

  </body>
</html>